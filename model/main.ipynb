{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Personality type prediction\n",
    "The Myers Briggs Type Indicator (or MBTI for short) is a personality type system that divides everyone into 16 distinct personality types across 4 axis:\n",
    "\n",
    "1) Introversion (I) – Extroversion (E) : a measure of how much an individual prefers their outer or inner world.\n",
    "\n",
    "2) Intuition (N) – Sensing (S) : a measure of how much an individual processes information through the five senses versus impressions through patterns\n",
    "\n",
    "3) Thinking (T) – Feeling (F) : a measure of preference for objective principles and facts versus weighing the emotional perspectives of others.\n",
    "\n",
    "4) Judging (J) – Perceiving (P) : a measure of how much an individual prefers a planned and ordered life versus a flexible and spontaneous life.\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data prep and cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Please provide your Kaggle credentials to download this dataset. Learn more: http://bit.ly/kaggle-creds\n",
      "Your Kaggle username:Your Kaggle Key:Downloading meta-kaggle.zip to .\\meta-kaggle\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 6.09G/6.09G [04:59<00:00, 21.9MB/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import opendatasets as od\n",
    "\n",
    "od.download('https://www.kaggle.com/kaggle/meta-kaggle')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "from time import time\n",
    "import string\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from bs4 import BeautifulSoup\n",
    "from nltk.stem.snowball import SnowballStemmer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = pd.read_csv('./mbti_1.csv')\n",
    "forum_data = pd.read_csv('meta-kaggle/ForumMessages.csv')\n",
    "mbti = {'I':'Introversion', 'E':'Extroversion', 'N':'Intuition', \n",
    "        'S':'Sensing', 'T':'Thinking', 'F': 'Feeling', \n",
    "        'J':'Judging', 'P': 'Perceiving'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(8675, 2)"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 864x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "type_count = train_data['type'].value_counts()\n",
    "\n",
    "plt.figure(figsize=(12,4))\n",
    "sns.barplot(x=type_count.index, y=type_count.values, alpha=0.8)\n",
    "plt.ylabel('Number of Occurrences', fontsize=12)\n",
    "plt.xlabel('Types', fontsize=12)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Forum Missing Values:\n",
      "Id                             0\n",
      "ForumTopicId                   0\n",
      "PostUserId                     0\n",
      "PostDate                       0\n",
      "ReplyToForumMessageId     991908\n",
      "Message                     7169\n",
      "Medal                    1069997\n",
      "MedalAwardDate           1061138\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "print('Forum Missing Values:')\n",
    "print(forum_data.isnull().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Missing Values:\n",
      "type     0\n",
      "posts    0\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "print('Training Missing Values:')\n",
    "print(train_data.isnull().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Forum Missing Values:\n",
      "Id                             0\n",
      "ForumTopicId                   0\n",
      "PostUserId                     0\n",
      "PostDate                       0\n",
      "ReplyToForumMessageId     991908\n",
      "Message                        0\n",
      "Medal                    1069997\n",
      "MedalAwardDate           1061138\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "forum_data['Message'] = forum_data['Message'].fillna('')\n",
    "\n",
    "print('Forum Missing Values:')\n",
    "print(forum_data.isnull().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3012786     19818\n",
      "75976        9794\n",
      "1723677      8404\n",
      "10654180     7175\n",
      "113660       6176\n",
      "            ...  \n",
      "5417935         1\n",
      "3267530         1\n",
      "5367543         1\n",
      "2110846         1\n",
      "8597061         1\n",
      "Name: PostUserId, Length: 307789, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "print(forum_data['PostUserId'].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "forum_data_agg = forum_data.groupby('PostUserId')['Message'].agg(lambda col: ' '.join(col)).reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "62          1\n",
      "5895914     1\n",
      "5895889     1\n",
      "5895886     1\n",
      "5895832     1\n",
      "           ..\n",
      "2215049     1\n",
      "2215034     1\n",
      "2214981     1\n",
      "2214962     1\n",
      "14369380    1\n",
      "Name: PostUserId, Length: 307789, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "print(forum_data_agg['PostUserId'].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "#function to clean data\n",
    "def clean_text(text):\n",
    "    #get rid of html and seperators\n",
    "    text = BeautifulSoup(text, \"lxml\").text\n",
    "    text = re.sub(r'\\|\\|\\|', r'  ', text) \n",
    "    text = re.sub(r'http\\S+', r'  ', text)\n",
    "    #get rid of punctuation\n",
    "    text = text.replace('.', '  ')\n",
    "    translator = str.maketrans('', '', string.punctuation)\n",
    "    text = text.translate(translator)\n",
    "    #get rid of numbers\n",
    "    text = ''.join(i for i in text if not i.isdigit())\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\eleed\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.9_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python39\\site-packages\\bs4\\__init__.py:435: MarkupResemblesLocatorWarning: The input looks more like a filename than markup. You may want to open this file and pass the filehandle into Beautiful Soup.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "train_data['clean_posts'] = train_data['posts'].apply(clean_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Im finding the lack of me in these posts very alarming    Sex can be boring if its in the same position often   For example me and my girlfriend are currently in an environment where we have to creatively use cowgirl and missionary   There isnt enough        Giving new meaning to Game theory    Hello ENTP Grin  Thats all it takes   Than we converse and they do most of the flirting while I acknowledge their presence and return their words with smooth wordplay and more cheeky grins    This  Lack of Balance and Hand Eye Coordination    Real IQ test I score    Internet IQ tests are funny   I score s or higher    Now like the former responses of this thread I will mention that I dont believe in the IQ test   Before you banish        You know youre an ENTP when you vanish from a site for a year and a half return and find people are still commenting on your posts and liking your ideasthoughts   You know youre an ENTP when you                I over think things sometimes   I go by the old Sherlock Holmes quote    Perhaps when a man has special knowledge and special powers like my  own it rather encourages him to seek a complex        cheshirewolf  tumblr  com  So is I D    post  Not really Ive never thought of EI or JP as real functions    I judge myself on what I use   I use Ne and Ti as my dominates   Fe for emotions and rarely Si   I also use Ni due to me strength        You know though   That was ingenious   After saying it I really want to try it and see what happens with me playing a first person shooter in the back while we drive around   I want to see the look on        out of all of them the rock paper one is the best   It makes me lol    You guys are lucky D Im really high up on the tumblr system    So did you hear about that new first person shooter game Ive been rocking the hell out of the soundtrack on my auto sound equipment that will shake the heavens   We managed to put a couple PSs in        No The way he connected things was very Ne   Ne dominates are just as aware of their environments as Se dominates    Example Shawn Spencer or Patrick Jane Both ENTPs    Well charlie I will be the first to admit I do get jealous like you do   I chalk it up to my w heart mixed with my dominate w   s and s both like to be noticed   s like to be known not the same        D Ill upload the same clip with the mic away from my mouth   Than you wont hear anything    Ninja Assassin style but with splatter    Tik Tok is a really great song   As long as you can mental block out the singer   I love the beat it makes me bounce    drop  io vswck  D Mic really close to my mouth and smokin aces assassins ball playing in the background    Sociable  extrovert Im an extrovert and Im not sociable     Sherlock in the movie was an ENTP   Normally hes played as a EXTJ   In the books hes an ESTJ    As I said   The movie looked good except for it being called sherlock holmes        Oh I never had fear of kissing a guy   I will kiss an animal too   So there was nothing to vanish   Just personal taste and me not liking it    The guy I kissed didnt know me   It was one of those        Sounds pretty much like my area and what Im going through right now trying to figure out which way I want to take my life   I want to do so many things   The biggest problem is that I know if I dont        D I was operating under the impression that you were female   I never looked at your boxy   Okay I help out my gay friends all the time and one of them has developed a little crush on me   I get red        TT You just described me  and Im living the worst nightmare   Im trapped in one place with one one around   Only dull woods   If I was a serial killer this would be the perfect place but sadly Im        TBH and biased sounds like a shadowed INFP   I think maybe he was hurt and turned ESTJ   I can tell because he has some of the typical INFP traits left over    Checks list Im sorry   It seems that you have came at a bad time   Weve already reached our quota of INFJs   However being youre female and I like females I will make you a deal   I will kick one        Im ANTP Leaning toward E   Im easy for both ENTPs and INTPs to identify with     I also imagine ENTPs interrogations would go a little bit like Jacks from  except more mechanical   Rigging up shock treatment equipment in an abandoned building out of an old car batty jumper        It was a compliment  Trust me   Im just as psychopathic D except I have emoticons   Theyre just weird ones   Like laughing when I get hurt or at people running themselves over with their lawn mower               No   Its like a theme for where I live and that is why I know it by heart         and I usual dont leave until the thing ends   But in the mean time   In between times   You work your thing   Ill work mine D  D Im the MBP Pleasure to meet you    Damn need to trust my instincts more I would have been closer I was going to say INFP    EXFP Leaning toward S with the way she responded    D My friends even my gay and lesbian ones always come to me for advice    I bow to my entp masters ENTPs are so great   If it wasnt for ENTPs I wouldnt have been able to build what Im building  Duck Duck  Duck  Shotgun  What Me I never do that    '"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data['clean_posts'][1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "forum_data_agg['clean_messages'] = forum_data_agg['Message'].apply(clean_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Hi Tanya Kaggle will maintain a rating system   If you win but youre ineligible for prize money you will still get a strong rating  Anthony Here are some papers that analyze Eurovision voting patterns   You might find some of them helpful   Gatherer Comparison of Eurovision Song Contest Simulation with Actual Results Reveals Shifting Patterns of Collusive Voting Alliances      Eurovision Song Contest Is Voting Political or Cultural Ginburgh and Noury    Suleman Efstathiou and Johnson    Eurovision Song Contest as a ‘Friendship’ Network\\r\\nDekker   \\n More research       enjoyLove thy Neighbor Love thy Kin Voting Biases in the Eurovision Song Contest   culture and religion Explaining the bias in Eurovision song contest voting   Hybrid System Approach to Determine the Ranking of a Debutant Country in Eurovision   Eurovision   Judgment Versus Public Opinion – Evidence from the Eurovision Song Contest   GiovanniThanks for your feedback   Using the forum to give feedback is a good idea   It allows others to see and comment on suggestions   We might set up a proper feedback forum but for the moment this topic will have to suffice   I also agree that the forum is a bit clunky   However we have a large list of feature requests and only limited resources for the moment  it might take us some time to address this   Apologies   I dont think the prize money in this competition is that relevant the prize is relatively small   Correct me if Im wrong but I think contestants are driven by intrinsic factors  A karma system that rewards forum posts is a good idea   Again apologies for any delay in implementing this there are lots of features on our to do list  Anthony Manish thanks for the feedback   The site is hosted on an Amazon EC server on the east coast of America  Its a fast server but the site has been more popular than we expected  Were currently working on speeding up the site by reducing the number database queries   We may have to implement auto scaling if the site keeps growing so rapidly   Anthony Just made a change which should speed things up   Let me know if it has made a difference for you   Jonathan thanks for your feedback x   Were currently working on caching database queries   There are a lot of good suggestions here that well try before autoscaling  \\xa0 Please use this topic to give us feedback   If youd rather do so in private email me at anthony  goldbloomkaggle  com   Use this topic to discuss any competitions you would like us to run   If you would rather contact me privately email anthony  goldbloomkaggle  com   I accidentally deleted the following post made by another user   Im reposting it on their behalfAre there categorical andor binary variables in the data set Other than the target variable For instance VariableOpen in test data seems to have categories          If there are categorical variables do we get to know what the categories meanThank You Sali Mali has pointed out that there is an error in the AUC\\r\\ncalculation for entries with tied scores that is when two or more scores have\\r\\nprecisely the same value   We will look at the problem over the next  hours and will rescore all entries   ApologiesAnthony The AUC calculation glitch has been fixed and all entries have been rescored   Sali Mali thanks again for pointing this out   Thanks for the feedback   \\r\\n\\r\\n What sort of features do you have in mind Or can you point to a forum that you we should emulate I just added a quick reply box to make the forum less clunky  \\r\\n\\r\\n Great suggestion   I have put this on our extensive features to add list   Colin the choice of scoring system was quite deliberate   Will the competition host considered using Area Under the ROC Curve where participants submit probabilities but  said that he deals with physicians who just want to know the proportion of predictions that are correct  Rajstennaj and Colin were really pleased that you believe that theres value to the project   Let me know if there is anyway that we can help to facilitate a community   We did set up the general Kaggle forum under CommunityForum\\r\\n  with such a community in mind  does it provide sufficient infrastructure You are free to start any new threads on that forum  RegardsAnthony    out of   thats pretty impressive Pity they didnt enter the Kaggle comp   I think youre right  some competitions exhibit more regularities than others   Soccer may be a difficult sport to model   Hi PG   Not sure that I fully understand the question   Are you referring to the situation where a classifier returns only  or  rather than a score or probability Perhaps you can use an example to illustrate the question Regards Anthony Hi PG   You should give the score for all timestamps  a higher score means the instance is more likely to be a member of the positive    AUC measures your classifiers ability to split the classes  so you dont need to decide which scores predict positive instances  and which predict negative instances    Have I addressed your concern The public leaderboard is only indicative because competitors can use information on their score to get information on a portion of the test dataset   The final results are a quite different and b better reflect actual performance   Thanks for participating in this competition   Ive attached the solution file to this post  \\nUPDATE\\xa0The solution is no longer attached but youre welcome to make submissions to this competition   Hi MattThanks for the nice words and the suggestion   Ive posted the solution file   Hi DirkThe Elo Benchmark is based on the training dataset only  \\xa0Having had many email conversations with Jeff I can tell you that the seed ratings matter a lot   Youll notice that Jeff made two submissions for the Elo benchmark  thats because hes refining his seeding method   I believe he plans to make a few more refinements  Jeff uses an iterative process to seed the rating system   For example he might start by giving everybody  and then letting Elo run for  months   He then seeds Elo with the  month ratings and runs Elo again   He does several iterations of this  Does this helpAnthony\\xa0 Just to clarify  the results page will show the leaderboard for all competitors regardless of whether they used future information or not   We will make an honourable mention to the leading competitor who doesnt use future information however their entry will be audited   Kaggle is currently developing a league table that ranks competitors   When it comes to this competition your position on the leaderboard which is indifferent to the use of future information will be what counts towards your Kaggle ranking    Hi JohnHave just confirmed with Jeff methodologies will be shared publicly   RegardsAnthony The competition has been designed to make cheating really difficult   At the end of the competition the winners methodologies will be replicated to help ensure everything is above board   Hi MattThe reason we prevent participants from submitting an unlimited number of times is because otherwisea our servers may not be able to handle all the traffic anda it would be easier to decode the portion of the test dataset thats used to calculate the public leaderboard   The technique you describe often referred to as cross validation is very sensible and we encourage others to use it   Anthony Good suggestion   Were open to ideas on how we can facilitate this   My thinking is the best thing to do is to implement a more functional forum which were doing   We can then encourage those who are still working on the problem to continue to use the competition forum as a way to collaborate   Hi DirkWeve updated the data description  thanks for the pointer  \\xa0\\r\\nThe competition does require participants to forecast the next four observations  \\xa0Weve updated the format of tourismdata  csv so that there is always a value in the last row  \\xa0Regards\\xa0Anthony INFORMS can offer an awardhonourable mention to those who dont use future data   However the Kaggle leaderboard will not seperate those who use future information from those who dont   Hi GregApologies there was a bug that cut off the last  characters   The problem has been fixed but unfortunately the fix will only apply future submissions   Thanks for pointing this out and sorry for the inconvinience   Anthony Hi MattI believe that Will the competition host is preparing a blog post that discusses some of the methods that people applied to this competition  based on the feedback we received   Is this the sort of thing you had in mindAnthony Uri you raise an interesting point   However is five months long enough for somebodys rating to move enough for you to notice this David this is a great suggestion   The HIV competition shows that Kagglers can do great things  \\xa0 My initial concern with any public dataset is that people can look up the answers   We would need researchers to withhold a small portion of the dataset for evaluation   I think the first step is to get in touch with those who set up the Alzheimers project   It also makes sense to contact the Michael J Fox foundation   If anybody has any connection to either of these projects please let me know  Otherwise Ill keep you posted on any progress  Anthony Jase the score on the full dataset is calculated onthefly  so we actually know who is winning based on the full test dataset  Ron the submission that is performing best on the public leaderboard may be different from the submission that is performing best on the full test dataset   We dont link the best submission on the public leaderboard to the best overall submission so that participants dont become confusedconcerned if their scoreposition on the public leaderboard worsens  Leigh my thinking as well   In a tradeoff between having a veracious public leaderboard and a veracious end result  the end result is most important  Jeff good suggestion    Ive put together an Excel sheet that might be helpful for cross validation   You paste your predictions for months  into column G and it aggregates by player by month and then calculates the RMSE   Hope its helpful  Anthony BenThe evaluation method was chosen because Jeff has found that scoring based individual games with RMSE unduly favours systems that predict a draw   Mark Glickman raised another issue  RMSE is better suited to normally distributed rather than binary  outcomes   So in order to use RMSE aggregation is preferable   Of course we could have evaluated on a game by game basis using a different metric  My biggest problem with the current evaluation method is that counting a draw as half a win seems a little arbitrary   However in order to benchmark Elo such an assumption is necessary   Mark and Jeff argue that a draw is generally worth half a win  so this assumption isnt too problematic   Anyway hope this gives you some insight into our thinking   RegardsAnthony Jeff please correct me if Im mistaken but I believe systems that predict draws are favoured because a high proportion of games are draws at the top level  per cent in the training dataset   Of course you can do better  but a system that predicts    for every game will perform better than it should   Has anybody tried Trueskill yet   probably a better starting point than Elo   This blog post does a nice job of stepping through Trueskill   Matt am interested in your thinking on this   Why MAE over MSE or RMSE Is it just that the metric is more intuitive or something subtler Hi DavidI have written to the Alzheimers Disease Neuroimaging Initiative ADNI and the Michael J Fox Foundation   I am scheduling meetings with both for September   Will keep you posted on any progress   Can you put up a link to the datapaper you foundThanks again for the suggestion   Its great if we can use the power of this platform to tackle meaningful problems   RegardsAnthony Uri the correlation between the public leaderboard score and overall score is significantly higher now   Here is the solution file for anybody interested    Uri Im reluctant to release confidence interval information because I want to minimize the advantage to early submitters   Early submitters already have the small advantage of having seen their submissions on two different public leaderboards   By releasing confidence interval information Im giving early submitters access to information that isnt available to later entrants   Jase aside from changing the size of the public leaderboard portion of the test dataset we also selected it more sensibly  so it better represents the overall test dataset    JPL a competition using internet chess data is a good suggestion   For interest the reason we are running the competition using top players is because Elo ratings matter most for top players since it is used to determine who can play in which tournaments   Out of interest has anybody entered this competition using Glicko Glicko or Chessmetrics Are either of you happy to send me your unmodified Glicko submission It would be good to add a Glicko Benchmark team to the leaderboard   My email address is anthony  goldbloomkaggle  com  Would like to do the same for Glicko and Chessmetrics if anybody has tried those   I have also contacted Ron about using his Trueskill submission as a Trueskill Benchmark   Jase I posted a link to your Glicko code on the hints page   Its very good of you to share it   Im really surprised that Glicko is performing worse than the Elo benchmark   Do you think this is because Jeff put lots of work into optimally seeding the Elo benchmark Or is Glicko just not as good Vateesh thanks for sending the files   The files that you sent are actually different   I also had a look at your submissions and you have a few files with the same name but different numbers   Also I was not able to replicate the problem as you describe it   Perhaps you can try again and let me know if youre still experiencing the error Was just chatting to Jeff   Time permitting he is going to benchmark some of these other systems   This way they will all be benchmarked on a consistent basis using the same seeding procedure and the same degree of tuning    Hi Vess   This should not be a problem given the way submissions are stored   Hi Edward   You will appear on the leaderboard as soon as you make your first submission   Hi Edward   Try using examplesubmission  csv available at    and replacing the score column with your predicted scores   If youre still having trouble email the file to me anthony  goldbloomkaggle  com and Ill have a look   Given the way the competition has been setup theres no way to prevent people from using future data   Even if the winner presents a model that doesnt include future data they may have overfitted to replicate the predictions of a model that does include future data   Uri thanks for pointing out the problem   Were currently working on a big upgrade to the website the new site should be launched by the end of this month   The upgrade will involve a more functional forum   In the meantime I will try and fix this problem   Anthony Uri Im not able to replicate the error either on the live site or on the development version   Can you let me know if you experience it again Hi Hans which post Still cant replicate the bug       intermittent problems are really annoying As mentioned were doing a massive site upgrade at the moment  so thats taking up the majority of our development time   How serious is the problem Can we live with it for the next few weeks until we deploy Kaggle    In theory yes   The problem is that theres no way to be certain that the winner didnt use future information   Even when we check the winning model its possible they have used a model with future information to probe the test dataset   Eric thanks for the feedback   Theres not really any reason to insist on a particular file extension   Were currently doing a big site upgrade so Ill add this to our list of feature requests   Seyhan the leaderboard portion of the test dataset is selected randomly   It is somewhat representative of the overall standings   I would really like to be more active in the forums  looks like theres some lively discussion happening Ive been flat out working on the site upgrade which is only a few weeks away from launch  Anyway Id like to share a few thoughts on this discussion  First off there is quite a strong correlation between the public leaderboard and the overall standings   Secondly the lack of relationship between the  scores and the  scores might indicate overfitting   This may be the case if youre experiencing a larger improvement on the  dataset than the  dataset  \\xa0 On a related point I notice that youre all performing very well   It could be that youve reached a local maximum i  e   the best possible score given the techniques youre using    Just to reemphasis Jeffs point you should pay more attention to your cross validation than to the leaderboard   The leaderboard is calculated on a very small amount of data so it is only indicative   PhillippSorry for the delay in doing this I havent had computer access over the last few days   The Spearman correlation between public scores and overall scores is     I also calculated the correlation for different submission quintiles to make sure the relationship holds at the top it doesTop    \\xa0              Its also worth mentioning that the trouble participants are having  reflects realworld difficulties in formulating a chess rating system   This competition is not just a game but a genuine attempt to explore new approaches to rating chess players  Anthony Out of interest why arent people rerunning old approaches that had previously been scored on the new cross validation dataset Greg thanks for pointing this out   Im currently traveling but will look into this over the weekend   Wil if you can get historical data from freechess  org possibly by agreeing to share the winning method with them wed be happy to host a comp here   This way you could specify that the winning method must be an instant gratification system   It would also result in a system thats tuned to lower ranked players   Thanks for pointing out the error   It has now been fixed   Apologies for any confusion   Cole sorry for the slow response   In this competition all your submissions count   In future we will ask participants to nominate  submissions   Phil that is correct   You must remember that Kaggle hopes to do more than just host fun competitions we want to help solve real problems   This is why were reluctant to force participants to choose just one model they may make a poor choice and the compettion host may end up with a suboptimal model   Our compromise position is to allow partipants to nominate five entries a feature which well roll out for future competitions    Phil number  is correct   Luck will play a part but I suspect the test dataset is large enough to limit its impact    I agree in a competition like this one   But as mentioned above we want to host competitions that are useful as well as fun   An upcoming competition will require participants to predict who has prostate cancer based on  variables   In a competition like that it would be a shame to miss out on the best model   Requiring participants to nominate five submissions seems like a good compromise    Greg this problem has now been fixed   Thanks again for pointing it out   Hi ColeApologies for the ambiguity   The time is as it appears on the competition summary page   adjusts according to the timezone on your computer clock so itll be Saturday or Sunday depending on your timezone  You can also see a countdown on the Kaggle home page  Anthony\\xa0 Dirk I just changed the file posted on the Data page to a unix format   Hope this solves the problem   \\tDurai apologies for the slow response   All up  countries were represented   Here is the list in order of most participants to fewest United States United Kingdom Australia Canada Thailand India Germany Spain China Netherlands France Italy New Zealand South Africa Sweden Argentina Croatia Ecuador Greece Indonesia Iran Ireland Mexico Poland Portugal Russia Singapore Turkey and Ukraine \\tRicardo you are correct  I gave the country list for the wrong competition    countries were represented United States Colombia India Australia United Kingdom France Thailand Canada Germany Argentina Japan Afghanistan Albania Austria Belgium Chile China Croatia Ecuador Finland Greece Hong Kong Iran Poland Portugal Slovak Republic Venezuela Uri makes a very good point   One way we could run a competition without knowing future matchups is to have participants rate every  player   Once we know the matchups we can infer predictions based on players ratings  The only downsides to this approach are   It doesnt allow for probabilistic predictions since there are many ways to map ratings into probabilities     We couldnt show a live leaderboard  which helps to motivate participants   Interested in others thoughts on this particularly the importance of a live leaderboard    Ron this is fantastic Looks like a sizable proportion of the black dots are sitting in a vertical line   Though Im sure the Elo Benchmark would look much worse      Out of interest what software did you use to generate the viz ps   Im guessing the anomalies that this viz highlights e  g   that white is a smaller advantage for lower rated players could inform future versions of your rating system   Philipp I dont fully understand your suggestion   Do you mind trying to explain it again Possibly by reference to an exampleAs a general principle tne problem with attempting to prevent people from using neural networks and the like is that participants use them anyway and then overfit other systems to replicate the neural networks results   I actually think that having neural networks et al in the competition is valuable   Even if they wont be implemented as rating systems they may have some benchmarking value   Assuming they predict most accurately they give a sense for what level of predictive accuracy is possible from any given dataset   As an aside if we require participants to submit ratings and dont \\r\\ngive them access to the matchups that theyll be scored on this should\\r\\n force participants to create a rating system       shouldnt it\\n BTW Jeff re   I have been and continue to be amazed by the level of participation so far  \\xa0 I had no idea so many people would participate   Congratulations on organising such a popular competition \\tPEW what criteria would you use to evaluate such systemsBTW I think youd be surprised at the proportion of the top  who are building rating systems    Philipp thanks for pointing out this bug   The error was only aesthetic  had been accidently hardcoded into the new theme   The platform was still only permitting two submissions   Anyway the error has been fixed    Philipp thanks for your nice words Hopefully having a more professional look and feel will help us attract interesting competitions with bigger prize pools    Hi allWondering why the benchmark is still leading when it is publically available    Have people had trouble replicating the authors methodology Or is everybody trying their own approaches Anthony Hi JesseYou are correct this is instruction is wrong   The monthly columns  mm should be  lines long including the header and the quarterly columns qq should be  lines long  The examplesubmission  csv file available on the data page gives an example   Im at a conference today but will correct the instruction as soon as I get the opportunity   Something was amiss   There was an error in the data uploaded on Kaggle Kaggles fault not the authors  The changes are not particularly big so models that performed well on the previous dataset should continue to perform well   To give you the opportunity to rerun your models and make new entries we have extended the competition deadline by two weeks and lifted the daily submission limit to three per day   And I believe George intends to release the code used to create the benchmark   Apologies for the error Dont hesitate to ask if you have any questions   Unfortunately the movie isnt out in Australia yet weve still got another week to wait   Sorry for the slow response  Ive been flat out with the new site launch   Below is the list of rows used to calculate the public leaderboard Dirk Ive changed the line break format   Let me know if this doesnt fix the problem    Jason theres a bug that prevents users seeing previous scores when they have longish technique descriptions   We are aware of the problem and will fix it as soon as we can  Diogo thanks for pointing out this error   We will setup pagination on the submission page shortly    Hi allJust to let you know that we have extended the deadline for this competition by just over a week   Both Jeff and I will be travellng around mid November so wouldnt be able to deal with the competitions conclusion  Anhony Apologies I hadnt antipicated that this might be an unpopular move   I should have canvassed opinion first   If others also disapprove I will changed back the deadline Kaggle is not a dictatorship  The downside of changing back the deadline is that it limits our ability to generate publicity   This bothers me becausea   top performers deserve recognitionb   publicity for the competition is publicity for Kaggle and more publicity  more members  more competitions andc   it lessens the chances of getting FIDEs attention  A compromise might be to extend the previous deadline by three days to Wednesday November  when Jeff is offline but I am available   Thoughts Hi PhilippThe Chessbase articles were written by Jeff he has a relationship with the editor   Jeff being away when the competition finishes means that its unlikely that Chessbase will report on the end of the competition a real pity if we hope to grab FIDEs attention   It is unfortunate that were both away when the competition ends Obviously not foreseen when it launched otherwise we would have set a different deadline   Anthony Jeff we must have posted simultaneously   You raise a good point   If Philipp and others are OK with the th then we should go with the compromise date   This would mean that Ill be available to report preliminary results and should mean were ready to report the final results by the time you return   Preliminary will be unconfirmed results from the raw leaderboard   Final results after the top ten have all agreed to share their methodology   Ive changed the deadline to the th   As for Uri breathing down your neck remember that the public leaderboard is only indicative and that the final standings may be different    Tim Kaggle is currently in the process of putting together a  league table which ranks participants based on competition performances  If you perform well in this competition it will count towards your  ranking    This first chart how the leading score has changed on a daybyday basis   The red line shows the Elo benchmark and the blue line shows the leading score   The Elo benchmark was outperformed within  hours which is why its always above the best entry   Interesting to see some recent progress after a period of stagnation well done Philipp   My guess is that any major improvement from this point on will be the result of somebody trying something quite different  This chart shows the number of daily entries   Higher early but seems to have stabilised at around  per day   Happy to put up other charts if people have requests    Phil I made an error in the ten per cent listed above try scoring with the following rows Philipp theres certainly a largish gap between the top five   Of course this is purely indicative   What really matters is the score difference on the final leaderboard        Philipp great suggesion Weve got a stack of features we want to implement but Ill put this in our long term wish list    I tried puting up a general forum for such discussions but found that it was very lightly used   Features in the pipeline include   fixing bugs or incomplete features on the new site   upgrades to Kaggle infrastructure to allow us to score very large entries   Kaggle ranking system  an Elo for Kagglers based on Microsoft Trueskill   extended social networking features including live chat recent activity feeds          Philipps\\xa0 competition analytics suggetion and possibly some other data viz toolsCompetitions in the pipeline include predicting social network connections predicting the likely success of grant applications for a large Australian University forecasting travel times for freeways in Melbourne Australia predicting prostate cancer from a high dimensional dataset subject to ethics approval diagnosing breast cancer from mammographics density images also subject to ethics approvalAny other suggestions Any thoughts on what our priorities ought to be Steffen you can enter using a model coded in any language   JohnDrew I presume those who enter using software other than R are still eligible for prizes\\xa0 Diogo thanks for pointing out this bug   Few minor teething problems with the new site  we should have them sorted out before long    \\tJasonLT are you thinking along the lines of karma points for participating in forum discussions Or would you like the forums to be more of a QA with Stackoverflow style ratingsI like the idea of guest blog posts and community tutorials   After the chess competition ends some might be interested in posting details of their workflowmethodcode  LT the general forum has been taken down for the moment   When I get a little time I will attempt to revive it and start encouraging people to use it    \\tPhilipp  it may not matter that people only compete in a handful of competitions because each competition contains quite a lot of information   Unlike a single chess game participants are competing against many players   Regardless well do plenty of testing with Trueskill before implementation   As for the points system points seem a little abitrary   I like the idea of ratings that account for the strength of a competitions participants   I tend to agree with your point on forum participation points   The Stackoverflow approach seem like a nice way around the problem  There are lots of directions we could take Kaggle   But for the moment were focused on competiitons    YuchunApologies for this error   The public leaderboard is portion of the test dataset is actually the first  per cent because we hadnt implemented the code to select a random portion of the leaderboard yet   For info the reason we kept getting different  per cents is because the random seed in the database was set to zero which told our code to choose a random random seed  Anthony Hi ArtemFor the intuiton behind AUC have a read of the evaluation page   Kaggle implementation of AUC works roughly as follows   Sort submissions from highest to lowest   Goes down the sorted list and for each prediction plot a \\r\\npoint on a graph that represents the cummulative percentage of class A predictions against the \\r\\ncummulative percentage of class B predictions      Join up all the points to form a \\r\\ncurve   The AUC is the area under this curve  \\r\\nHT Phil Brierley for this explanation  William no thresholding is required which is part of the beauty of AUC   In fact given that the algorithm works by sorting participants make submissions containing any real number  higher means more confidence that the observation is of the positive class  Hope this response doesnt serve to confuse people  Anthony Hi JonIts a fixed  per cent chosen randomly  Anthony Hi TamasAs your results suggest the order does matter and the IDs dont   Anthony Artem Ive gone through the steps using your example data   Let me know if Ive made any errors  The Kaggle algorithm basically works as followsFirst order the data predicted            real     Then calculate the totals for each class in the totals  totals  Initialise the cumulative percentagespercentslast  percentslast  Iterate for each solutionsubmission pair counts  counts  counts  counts  percents  countstotalspercents\\xa0  countstotalsrectangle  percentspercentslastpercentslasttriangle  percentspercentslastpercentspercentslast area  area  rectangle  trianglepercentslast  percentspercentslast  percentsSo in your exampleFirst submissionsolution paircounts  counts  percents    percents  triangle  rectangle  Cumulative area  percentslast    percentslast  counts    counts  percents    percents  triangle  rectangle    Cumulative area    percentslast    percentslast  counts    counts  percents    percents  triangle  rectangle  Cumulative area    percentslast    percentslast  counts    counts  percents  percents  triangle  rectangle  Cumulative area    AUC     Also heres Kaggles PHP code to calculate AUCprivate function AUCsubmission solution \\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0 arraymultisortsubmission SORTNUMERIC SORTDESC solution\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0 total  arrayA B\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0 foreach solution as s \\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0 if s  \\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0 totalA\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0 elseif s  \\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0 totalB\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0 \\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0 nextissame   \\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0 thispercentA     \\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0 thispercentB     \\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0 area     \\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0 countA  \\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0 countB  \\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0 index   \\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0 foreach submission as k \\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0 index  \\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0 if nextissame  \\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0 lastpercentA  thispercentA\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0 lastpercentB  thispercentB\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0 \\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0 ifsolutionindex   \\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0 countA   \\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0  else \\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0 countB   \\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0 \\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0 nextissame  \\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0 ifindex  countsolution   \\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0 ifsubmissionindex \\xa0 submissionindex\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0 nextissame   \\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0 mycount  \\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0 \\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0 \\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0 if nextissame   \\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0 thispercentA  countA  totalA \\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0 thispercentB  countB  totalB \\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0 triangle  thispercentB  lastpercentB  thispercentA  lastpercentA     \\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0 rectangle  thispercentB  lastpercentB  lastpercentA \\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0 A  rectangle  triangle \\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0 area  A \\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0 \\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0 \\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0 AUC  area \\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0 return AUC JC I agree that those who enter early have an advantage   However the main source of advantage comes from the fact that they have had the opportunity to spend longer on the problem and try more things   Philipp the current leader has made  entries   If this competition took ternary scores loss win draw this would amount to  possible combinations  making Phillips  entries\\xa0 a drop in the ocean   In fact the test dataset is richer because participants predict the probability of victory  Nonetheless for future competitions we will ask participants to nominate five entries that count towards the final standings    \\tPEW we are not requiring participants to guess but rather encouraging them to rely on their cross validation when determining which models to choose   The problem with allowing people to enter many times and try many parameter tweaks is that they are more likely to accidentally overfit on the test dataset   By this I mean they are more likely to find a parameter tweak that works well on the test dataset but doesnt work as well for future chess games  On your second point you are correct to say that I am worried about statistical guessing   The requirement that participants submit code does not obviate this concern because models can be overfitted once the answers are known   In the extreme case somebody could fit a decision tree that classifies every game perfectly if they know the answers   Showing the standings but not the scores makes statistical guessing only slightly more difficult because participants are close enough that the leaderboard ordering gives meaningful feedback on which guesses are better and which are worse  As an aside it seems that I have failed to convey the message that the public leaderboard is purely indicative and that cross validation is  important   I would even go so far as to say that it may be problematic if the public leaderboard bears too close a resemblance to the overall standings    I like Uris suggestion   It gets around the problem that LT mentons while potentially encouraging people to try things beyond parameter tweaks   Couple of potential problems   A participant exhausts the submission limit and another entrant makes and shares a breakthrough eg the use of Chessmetrics in this competition  Anybody who has exhausted the submission limit wont have the opportunity to build on the breakthrough   This seems less than ideal given that we want to get the best results possible     It might encourage people to make all their entries at the end so that they dont reveal the strength of their hand   What do others think Philipp Kaggle has been experiencing a massive lift in site visits and\\r\\n signups since the new site launched from  unique visitors to    This accounts for the increase in entries   Thanks everyone for making this an amazing competitionBig congratulations to the winner Outis   Also to the runner up Jeremy Howard who only joined the competition late in the piece and to Martin Reichert who finished third  Hopefully well get some of the top ten to tell us about their methods on the blog   In the meantime I encourage you all to tell us a little about what you tried on the forums   Also for interest heres a chart that shows how the best score evolved over time   Rapid improvements initially but after a month progress stalled as participants approached the fronteir of what is possible from this dataset   Apologies Uri and LT  seems that any reply is redundant now   Also big thanks to all those who participated in forum discussions   You helped make this a far more interesting competition    I think I can help with this I dont give names just score combinationsscore publicscore                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                        Jeff can I post the test labels on the forum I only seem to have the aggregate solution on hand attached   Jeff do you have the game by game labelsEdit looks like you posted a minute before me Hi NickYoure welcome to bring additional data as long as its publicly available  Anthony Attached is some sample code that can be used to constuct an entry that generates a forecast based on the average travel time on a given route on a given day of the week at a given time   Attached is some sample Python code that generates forecasts based on the last known travel time   Im new to Python so happy to hear any feedback on the code   Mmm       file didnt attached   Heres the codephprh  fopenRTAData  csv r File to read fromwh  fopensampleHistorical  csv w Write the entry to this filedatedefaulttimezonesetGMT Purely to prevent the interpreter from raising a warningtimeStamp  array                               an Array with the humping off pointsforecastHorizon  array forecast horizon in lots of  minutes e  g      minutes  minutes   hour   This is used for calculating the forecast time stampsforeach timeStamp as ts  \\xa0\\xa0\\xa0 foreach forecastHorizon as f  \\xa0\\xa0\\xa0 \\xa0\\xa0\\xa0 forecastTimeStamp  dateN H istrtotimetsf find day of week hour and minute that corresponds to each of the timestamps \\xa0\\xa0\\xa0 row  while data  fgetcsvrh    FALSE  loop through the datafile\\xa0\\xa0\\xa0 if row    Write the header \\xa0\\xa0\\xa0 \\xa0\\xa0\\xa0 colCount  countdata \\xa0\\xa0\\xa0 \\xa0\\xa0\\xa0 for c c  colCount c \\xa0\\xa0\\xa0 \\xa0\\xa0\\xa0 \\xa0\\xa0\\xa0 fwritewh   datac\\xa0\\xa0\\xa0 \\xa0\\xa0\\xa0 fwritewhn\\xa0\\xa0\\xa0 \\xa0\\xa0\\xa0 if  inarray dateN H i strtotimedataforecastTimeStamp \\xa0  if the day of week hour and minute that corresponds to a forecast timestamp is found then save to an array called tsArray \\xa0\\xa0\\xa0 \\xa0\\xa0\\xa0 for c c  countdata c \\xa0\\xa0\\xa0 \\xa0\\xa0\\xa0 \\xa0\\xa0\\xa0 if  emptydatac  datac  x  \\xa0\\xa0\\xa0 \\xa0\\xa0\\xa0 \\xa0\\xa0\\xa0 \\xa0\\xa0\\xa0 tsArraydateN H i strtotimedatac  datac\\xa0\\xa0\\xa0 \\xa0\\xa0\\xa0 \\xa0\\xa0\\xa0 rowforeach timeStamp as ts  \\xa0\\xa0\\xa0 foreach forecastHorizon as f  \\xa0\\xa0\\xa0 \\xa0\\xa0\\xa0 fwritewh dateYmd Histrtotimetsf\\xa0\\xa0\\xa0 \\xa0\\xa0\\xa0 for c c  colCount   c \\xa0\\xa0\\xa0 \\xa0\\xa0\\xa0 \\xa0\\xa0\\xa0 fwritewh    arraysumtsArraydateN H istrtotimetsfccounttsArraydateN H istrtotimetsfc writes the average for a given day of the week hour and minute to the submission file \\xa0\\xa0\\xa0 \\xa0\\xa0\\xa0 fwritewhn\\xa0\\xa0\\xa0 fcloserhfclosewh File didnt attached   Heres the codeimport csvimport datetimerhopenRTAData  csvr read in the data whopensampleNaivePython  csvw create a file where the entry will be savedrhCSV  csv  readerrhtimeStamp                                 an Array with the cutoff pointsforecastHorizon   forecast horizon in lots of  minutes e  g      minutes  minutes   hour   This is used for calculating the forecast time stampsrow   inialise the row variablefor data in rhCSV loop through the data\\xa0\\xa0\\xa0 if row   if the first row then write the header\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0 for j in rangelendata\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0 wh  write  dataj\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0 wh  writen\\xa0\\xa0\\xa0 if data in timeStamp if the row is a cutoff point\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0 for i in forecastHorizon for each forecast horizon write the cutoff travel time as the forecast the definition of Naive \\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0 dateStr  strdatetime  datetimeintdataintdataintdataintdataintdata  datetime  timedeltai calculte the time stamp given the forecast horizin \\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0 wh  writedateStr write the timestamp to the first column of the CSV\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0 for j in rangelendata\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0 wh  write  dataj write the cutoff travel time to the subsequent columns \\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0\\xa0 wh  writen\\xa0\\xa0\\xa0 row  rh  closewh  close Dirk thanks for pointing this out   Ive written to the RTA about this and they responded sayingIndeed\\xa0 our control room have confirmed significant increase in traffic volumes following the removal of the tolls   This has had an impact on the overall travel times across the M  Something to be aware of when using the older data    Hi Dennis The  per cent doesnt count towards the final standings and is selected at random across the  timestamps and  routes   As for the SMTP error its been fixed   The problem was the result of a flood of signups which caused Google to shut off our mail server   Were now using our own mail server   Anthony Apologies for the error its deciseconds not centiseconds  so  is    seconds   Ive fixed the description    This is something that should be dealt with on a case by case basis   If you find a dataset youd like to use ask on the forum and Ill run it by the RTA   For information Im trying to get hold of some incident data   Will keep you posted on this    Lee this is great Dirk did the same thing with some Python sample code I wrote for the social networking competition   If you guys keep showing me how things can be done better I may become a half decent coder   Toppy thanks for the pointer   A higher priority at the moment is to get forum attachments working again   Hi Peter Ill follow up in this   At the very least we should be able to provide information on the length of different routes   Anthony Hi Carlos Unfortunately not   Clause   c in Kaggles Terms and Conditions saysc            employees or agents of the Competition Host are not \\r\\neligible to participate in any Competition posted by the Competition \\r\\nHost\\r\\n\\tTo answer the second question we would more information about the nature of the business and what your friend does  Anthony Frank this is great I particularly like the heatmap  is it possible to zoomAlso itd be neat to see some animation on the M map  showing how travel times evolve over the course of a dayweek dots getting bigger and smaller   Though I suspect this might be a lot of work  Anthony  David I believe that when loops the measuring device fail travel times are estimated   Im working towards putting together data on when travel time readings are suspect    C does seem to be an expressive language   Im a Linux user though so not inclined to pick it up    Armin I agree   Makes more sense for me compile this information once for everybody   Will try and get it done this week   Daniel Dennis is correct in saying that averaging the values leads to floating point numbers   The answers are integers but the RMSE is calculated using floating point arithmetic    Thanks B Yang   The benefit of publishing code is that you get sensible suggestions in return    Andrew good discovery   Ill pass the question onto the RTA  Edit Wouldnt it be obvious if they werent making the adjustment since peak traffic times would change  This code does generate a sample entry   To use it a download the PHP interpreterb create a file name xxx  php copy the code above and download the data files to the same directoryc run the command PHP xxx  php Youre correct  the future is used to predict the present   However I dont think the temporal leakage invalidates the algorithms developed in this competition    Daniel and Dennis are correct   Keep in mind that the  per cent is a random selection of the  that doesnt count towards the final standings which are calculated based on the other  per cent   The cutoff times are all between am and pm   They were selected using a simple formula that favoured high volatility cutoff times over low volatility cutoff times   So youll see more peak hour morning and afternoon cutoff times   The rationale behind this is that its more important to predict accurately during high volatility times so we want to favour models that do best at these times   That explains why the RMSE is higher than for randomly chosen cutoff points   Aidan have asked the RTA about this   This was the response   The cutoff is due to free flow conditions imposed by the system during data unavailability  Ive written again asking for a little more detail   Will post the response when it comes    Paresh thanks for the thought provoking question   I agree with Dennis I am more interested in the time delay than the percentage delay   On a related matter we think it is more important to predict correctly when travel times are volatile e  g   before and after work   To favour models that predict more accurately during high volatility times we selected more high volatility cutoff points so youll notice more cutoff points during the morning and afternoon   Phil thanks for sharing this   Just got to find a Windows machine to run it on        Hi Markus I can help out on the second part of your query Ive posted some PHP AUC code on another forum post   software packages like R have easy to use packages that calculate AUC   Anthony You can email me the file if you like anthony  goldbloomkaggle  com   Id be happy to take a look at it    Rasmus apologies I deleted the wrong post   Anyway you asked how travel times are measured   There are regularly spaced loops along the M   These loops measure each cars speed and the number of cars that travel across the loop every three minutes  travel times are then calculated using a formula   The formula has been tested and calibrated using test cars that travel along the freeway and record their travel times    Thomas I selected specific cutoff times randomly but chose timeday combinations that are volatile across the dataset   Vitalie the volumes data is used to calculate travel time  see this post for more info   Our priority at the moment is to get the incident loop error and route length data together   However I can find out if this data can be made available if you think it might be useful   As Dennis says itll be highly correlated with travel time and we obviously wouldnt release it for the blanked out times    Jeremy I wasnt aware that public documents with traffic details were available   To the extent that any information is available for blanked out times this would most definitely be considered cheating  As for question  I am aware of this in fact the issue came up in another post   The rules state that the winning model must be implementable by the RTA in order to be eligible for the prize   The averaging model passes this test   As an aside I dont believe the temporal leakage invalidates the algorithms developed in this competition    Benjamin once we get the incident data I will put in a request for this data    I have some information on suspect loop readings that Im working to release   This has information on when loop readings may be unreliable for various reasons   I dont yet know whether or not this will help with the free flow issue   Anyway I will upload them as soon as I can get it into a useful format  I suspect the reason the free flow times are different is because route lengths are different   Rob on your point about missing data it might be helpful if I explain how I put the files together   I received data in the following formatroute IDtimestamptravel time xxx xxxI transposed them into in the hope that theyd be more manageable   When timestamps were missing I just filled in a blank row   Hassan the most important file is RTAData  csv   You can create a sample entry by   downloading RTAData  csv and createHistorical  php attached to the same directory   navigating to that directory in the terminalcommand prompt   running php createHistorical  php This will create an entry based on a historical average for that timeday and is a good starting point   BJB very generous of you to upload a Java code   Ive now enabled   java file uploads so you should be able to upload the file    burak the times in sampleEntry  csv are the times you need to generate forecasts for   Theres more info on how the  cutoff points were selected in this forum post   Aaron you raise a good point   According to the route definitions I have route  extends from loop A to loop A while route  extends from A to A so  should encompass all of    Denniss observation that sometimes  has longer throughput times than  is strange   Ill double check the definitions with the RTA    Aaron another good question   Have also passed this on to the RTA    The number directly to the left of the team name is the teams position and the number to the right of the team name is the teams score or Root Mean Squared Error RMSE    Alexander to me this means that the algorithm can take a timestamp as an input and can generate forecasts for the next mins mins etc   \\xa0Lee thanks for pointing this out  This post   post   Alexander there is no truncation of floats  \\xa0 Mmm       my message seems to have disappeared from the board   Anyway heres a repeat  Aaron the units are deciseconds  Nick actually its a hybrid approach   You can nominate five entries that count towards the final standings   You do this from the submissions page  the last five are chosen by default   At the end of the competition the best of your five nominated entries counts towards your final position   And Nick on your new question the one of the five you nominate that scores best on the  per cent counts   The  per cent is meaningless as far as the final standings are concerned  \\xa0 Jose do you want me to ask if its permissible to use NOAA data If so are you asking about the data that Brad mentions above William thanks for the question  \\xa0   Teams are allowed to merge     One individual cannot be part of several teams our systems ensure this anyway        as long as somebody doesnt have multiple accounts  Agree that we should make this more explicit in the future  \\xa0As for finding people who submit from multiple accounts we are actually in the process of implementing rules that alert us when it looks like this is happening   In the future for large prize money competitions we may look at verifying identities  \\xa0 Really nice feedback  very thought provoking  \\xa0The API suggestion is nice   It does seem that it would prevent people from using the future to predict the present   However the testtraining split is still necessary to prevent overfitting and we could still only give partial leaderboard feedback the API doesnt secure against overfitted parameter tweaks   Also the API approach would add new problems\\xa0   models will take longer to run because of the delay in receiving data points   as you say it would add a huge load on Kaggles servers\\xa0As for the problems you list here are my responsesPredictions can’t use all available prior data since the test data doesn’t provide resultsThis is necessary to ensure against overfitting   If all the data is used to calibrate a model its impossible to know if the model will fit future datasets as well   \\xa0Limited training and test data creates too much variance between the public score and actual scoreThe mistake made in the first competition was with the size of the public leaderboard portion of the test dataset my fault not Jeffs   It was too small which lead to the low correlation between public and overall scores   For the RTA competition we raised the proportion to ensure a stronger correlation   This proportion was calibrated after some testing of the correlation between the two parts of the test dataset   We intend to continue this practice going forward  \\xa0Model parameters can’t be tuned because actual scores aren’t provided\\xa0If we allowed parameter feedback on the whole test dataset this would almost definitely lead to overfilling parameter tweaks that work on the test dataset but wont work for for future datasets  \\xa0Number of submissions is severely limited because they are so large this will become a bigger problem as larger test datasets are created\\xa0I dont think more daily submissions are necessary because the majority of model building should be done with reference to a cross validation dataset  \\xa0Leaderboard doesn’t reflect actual leadersAgain this was my mistake   I made the public leaderboard portion of the test dataset too small   This is not a flaw with the general approach  \\xa0Future data can be used to predict the pastJeff suggested a really nice solution to this test set includes some spurious games so that people can’t mine the test set for useful data about the future   These spurious games wouldnt be used in final evaluation  \\xa0The API also provides a really nice answer to this problem   Attached is some R code to create a GLM entry for this competition   As always happy to hear feedback from others about how this could have been done more elegantly   Anthony Nathaniel is right  the data is correct its just a problem with heading formatting   Will fix this shortly and reupload the data    Rob thanks for jumping in  \\xa0 Eleni just uploaded RouteLengthApprox  csv which has approximate route length data    Konstantin just uploaded RTAError  csv the is valid data   Its available on the data page   Finally  fixed the headings   Just to reiterate all the data are correct  its just the capitalization in the headings that caused trouble  As for the inconsistent numbers of delimiters also fixed  my software package stopped printing delimiters when there were no more values or NAs in a row   Jack the country  of  birth issue is now fixed   Please download the latest version of the data    P  V  Kiran it means that if your solution is implemented using a software package that is not available to the University of Melbourne it must be possible to translate your solution into a different packagelanguage    No   Towards the end of this competition you will be asked to nominate five entries that count towards the final result    Dielson good pick up   Dennis is correct  the date format doesnt matter   What is important is that you put the correct data in the correct cells   \\tMooma I appreciate your frustration but sensor malfunctions  are part and parcel of dealing with realworld data   If we had the data ready at the outset we might have excluded failed sensors and downweighted the impact of partially failed sensors when evaluating predictions    \\tKonstantin Dennis is correct it is not safe to assume that there is no errors in the control data    Nathaniel thanks for pointing this out   Definitely worth investigating   The Number of Successful Grants and Number of Unsuccessful Grants  fields dont change in the test dataset for obvious reasons   The journal citations also remain constant in the test dataset to prevent participants using the future to predict the past   Jose and Joseph just spoke to the RTA about this   The answer is no because it might allow future weather conditions to be used to predict the present   Ahmed just got an answer from the RTA on this   Heres the responseThe answer is  maybe   RTA would request that anyone wishing to use the data for further research purposes write to the RTA and make their case  describing what they wish to do ie the purpose of the research and how they would use the data   The RTA will consider each application on its merits  \\xa0Let me know if youd like me to pass on the relevant email address   Im reluctant to do it in the forum but will offer an introduction to anybody who asks   Just elaborate a little the types of solutions that cant be implemented are those that are encumbered by patents or other intellectual property restrictions   Michelangelo truth is that you can submit any real number we suggest a number between  and  because of the convenient interpretation   AUC ranks your scores  the higher the score the more confident you are that the instance is a member of the positive class   Nathaniel I have looked at the problem in some detail and have spoken to the University of Melbourne   They are looking into it and hope to have an answer for us tomorrow before they break for Christmas    Many thanks to everyone for all your great activity on this fascinating problem  insightful questions and comments on the forum good early results on the leaderboard and interesting discussions\\xa0There have been a lot of questions about exactly what constitutes an acceptable model for the RTA   So far my guidance on this matter has possibly been too fuzzy and I hear a lot of you looking for more definite rules   Therefore we have come up with the following specific rule regarding the allowed model inputs\\xa0Your model can be of any form you like as long as it takes its input only from the following parameters Time of prediction Day of week Is holiday Month of year Route number to be predicted The time taken for route r for datetime t where\\xa0 r is any route and t is any time less than the datetime being predicted for as  many routes and datetimes as you wish The sensor accuracy measurements for any routes r and datestimes t defined as above The estimated route distances as provided by KaggleTo clarify the following are not permitted The use of any data other than those provided by Kaggle for this competition and the list of NSW holidays   The time taken for any routes in the future compared to the prediction being made  your model can still be trained using all data as long as the resultant model only uses the inputs listed above  Furthermore the algorithm must not be encumbered by patent or other IP issues and must be fully documented such that the RTA can completely replicate it without relying on any black box libraries or systems   The university has spent the last two days on the problem   They suspect its an internal inconsistency in their database the figures are drawn from different parts of their database   Well have to wait until the end of the Christmas break to get a final verdict    Hi Alexander   No   Using full timestamp makes it possible for a model to implicitly incorporate external data and future data     You may also use holiday data extracted from the PDF file that you linked to in order to get holiday information for previous years   However we will not be providing a file of this information directly     This is correct  Anthony Deepak thanks for pointing this out   We will ask  the university about this as well   Unfortunately we cant expect an answer until early next year    personIDs refers to all the columns that have investigator IDs e  g   column  has investigator  column  has investigator    Ignore the comment numerical values that should be     As Jeremy Howard pointed out earlier in this thread the key point that answers most of these questions is that the limitation is only on the functional form of the final model   More specifically\\xa0Xiaoshi Lu You can build your model  filtering aggregating etc  using all the datetime information you like   The final functional form that you end up with however should only use the predictors listed above  \\xa0Mooma The inputs listed include this The time taken for route r for datetime t where\\xa0 r is any route and t is any time less than the datetime being predicted for as many routes and datetimes as you wish   So what you ask is specifically allowed   Of course for you to create your input file which includes for example the time taken one hour earlier you will need to use the full datetime   However the resultant model will not directly use this  instead it will only use the time taken on that route as allowed by the rules  \\xa0Alexander Groznetsky Imagine using a very flexible model neural net for instance which trains with all datetime info included in the input parameters   It might implicitly end up using the route times later in the day to predict those earlier This is an example of how a model could be useless in practice even although it appears highly predictive on the competition data  \\xa0 JoseI notice that you are now on the leaderboard   It can take a few minutes before you show up   Anthony Matthew   Using GPL code is fine  \\xa0   The isholiday variable can be a direct input rather than a variable that is derived by reference to a timestamp  \\xa0   You contact me directly at anthony  goldbloomkaggle  com  \\xa0Dennis you can use isspringbreak rather than isholiday  \\xa0 David its really neat  For info it works in Safari but the page videos are aligned a little strangely   Martin Dane is correct the information in\\xa0RouteLengthApprox  csv is in metres   So\\xa0route  is approximately   km  \\xa0 The In the Money indicator is based on the public leaderboard only   It doesnt reveal anything about the final standings   Martin when I open the file it shows  and    What application are you usingAnthony I believe it refers to grants made when the researcher was at another university  \\xa0 Nicholas a Matlab solution is fine as long you dont include libraries that use patented or undocumentedsecret algorithms  \\xa0 Apologies Will   I was on a plane and only just got your message   Will make the adjustment this afternoon  Id also like to congratulate the top teams and congratulate Dirk for running an excellent competition  \\xa0 Thanks to everybody who participated and a big thanks to Dirk for putting together a really nicely designed competition   The test labels are attached to this post   Rafael  and  are fine    is also fine as long as the data is derived entirely from the time series as you say   \\xa0 B Yang first off congratulations again on a fantastic performanceYour frustration is understandable but we cannot enforce rules that dont exist  what is common sense to some is not common sense to others   As Jeremy points out in the RTA competition the rules say The winning entry has to be a general algorithm that can be implemented by the RTA   An algorithm that involved looking up future answers could not be implemented by the RTA    Reginald please email your submission to anthony  goldbloomkaggle  com and Ill have a look   Edith thanks for the feedback   We agree with your comments and we are working on making the terms more competitor friendly    The university has done an investigation and has found that the issue arises from an inconsistency in their database   Wu Wei a route is made up of several loops   A figure of    means that  per cent of the loops in the given route are giving suspect readings    Apologies I didnt clarify this with Mahmoud before the launch but we have discussed this offline  This competition requires you to choose five entries that count towards the final result   To choose five entries visit your submissions page and click the star next to the relevant entry to select it   If you do not choose any entries your last five entries will be chosen by default   Michelangelo the  per cent comes from the test dataset   Eu Jin Lok the sampling is done randomly    For anybody interest heres the actual solution   Hi GregThe answers will be made available on the forum  \\xa0I can ask whether the data can be used for publishing research if you likeKind RegardsAnthony\\xa0 Hi Greg and SuhendarThe university doesnt want the data to be used for any purpose other than for this competition  Anthony Hi GregIt would be nice if the dataset could be used for other work   However if we dont allow competition hosts to place restrictions on the use of their data then we wouldnt get access to it in the first place  \\xa0Will post the solution file now  RegardsAnthony The solution file is attached to this post  Thanks all for participatingAnthony  Entries made before we fixed the leaderboard were scored incorrectly   I have now rescored the relevant entries   The error was the fault of Kaggle and not the competition organizers  \\xa0ApologiesAnthony Hi CerinApologies for the errors   They all stemmed from the fact that the servers hard drive filled up   Ive cleared some space  \\xa0For information were currently rewriting the entire site for the Heritage Health Prize   You can expect the next version to be faster and include many more features  Thanks for your patienceAnthony Hi CerinAli is right your entries will count towards the final standings  \\xa0Anthony Hi allSubmitting from multiple accounts is most definitely against the rules  \\xa0We have done some analysis and found that it happens very rarely   However we are working to put the systems in place to identify and block those who attempt to do it  Kind RegardsAnthony The solution is attached  Thanks all for participatingAnthony HarriThanks for the thoughtful post   The IJCNN people agree with you and have decided not to disqualify Shen  \\xa0As mentioned above Kaggle will soon have the systems in place to detect multiple accounts in real time so that such issues dont arise  Anthony I have sympathy for peoples frustrations   In this case the competition host decided that the results should stand  so we are facilitating their decision  Chris makes a good point about the rules being scattered throughout the site   We will be sure to address this in future competitions   We will also ensure that they are tightly enforced   For information a lot of effort has gone into framing the Heritage Health Prize rules  Finally thanks for the feedback   Its discussions like this that will help us improve Kaggle   Kaggle has received legal advice after the controversy surrounding this competition  \\xa0We have been advised that it sets a dangerous precedent for us to ignore our own terms and conditions notably clause    preventing multiple signups   We have therefore acted in accordance with this clause\\xa0disqualifying those who clearly submitted from multiple accounts  Thank you all for your patience on this issue and rest assured that we are working to ensure that it is not a feature of future competitions   Entrants are welcome to use other data to develop and test their algorithms and entries until  UTC on April   if the data are i freely available to all other Entrants’ and i published or a link provided to the data in the “External Data” on this Forum topic within one\\xa0 week of an entry submission using the other data  \\xa0 Entrants may not use any data other than the Data Sets after  UTC on April   without prior approval   Also covered by Slate and Forbes\\n  \\n  \\nand the Wall Street Journal a couple of weeks ago\\n   And Smarter Planet\\n   The criteria was that somebody had to\\r\\n   make at least one claim in Y \\r\\n   be eligible to make a claim in Y\\r\\n\\r\\nOutliers have been removed from the dataset as well as those suffering from stigmatized diseases   Just to clarify when Jeremy says we cleaned it as much as we can we didnt do much to the claims data on purpose   We figure it makes more sense for you to make your own cleaning assumptions rather than have us impose them on you   Not only are patients who died in Y not in the dataset but patients who died in Y are also not in the dataset because they didnt remain eligible to claim for the whole of Y   Apologies this was an error   Thanks for drawing our attention to it  \\r\\n\\r\\nThe missing values are for those people who have been in hospital for more than two weeks   They should be replaced with a    You can either do this yourself or download the updated dataset   \\r\\n\\r\\nFor information members who have in hospital for more than two weeks have been grouped for privacy reasons they are rare so may otherwise be identifiable   The implication of this grouping is that if you expect somebody to be in hospital for more than two weeks you should predict  days   \\r\\n\\r\\nThis grouping should not have a big impact because\\r\\na   members who are in hospital for more than two weeks are rare about one per cent of members\\r\\nb   the evaluation metric favors algorithms that accurately predict fewer days in hospital on the assumption that these are more preventable   Dorofino\\r\\n\\r\\nGreat idea Forming a team is a really good way to learn   \\r\\n\\r\\nAre you affiliated with the New York R Users Group For info Ive heard rumblings about them setting up a team   \\r\\n  \\r\\n\\r\\nGood luck with this\\r\\n\\r\\nAnthony Hi Rich\\r\\n\\r\\nJust spoke to HPN about this   For the moment they dont want to provide general guidance and ask that you make a request through the contact us form   Your request should detail the topic of your proposed research   Definitely worth making it clear that youre just looking to publish the method that you use to enter the competition   \\r\\n\\r\\nAnthony Y Y Y etc refer to different years   We havent revealed which years to help keep the data private   The years are sequential   We are not revealing what years Yn refer to nor whether or not they refer to calendar years for data privacy reasons   Apologies for the missing values it was an error   You can either replace the missing values with  or download the updated data set   \\r\\n\\r\\nIf youre interested in the reason for the missing data see\\r\\n   Hi bacg\\r\\n\\r\\n   DaysInHospital refers to Y the second year while the claims refer to Y the first year  \\r\\n\\r\\n   Not everything that has a length of stay counts as a hospitalization   In fact you dont have enough detail in the Claims table to calculate DaysInHospital   The detail has been suppressed for privacy reasons  \\r\\n\\r\\nAnthony Hi mbenjam\\r\\n\\r\\nWe would have loved to release more detailed data but have to be mindful of data privacy   \\r\\n\\r\\nAnthony Have received advice from the HPN lawyers   Im really sad to say that the answer is no on all accounts   Eu Jin youve obviously not seen this\\r\\n   Wgn the intention is not to rule out the publication of research   Ive passed on your message to HPN and a clarification will be forthcoming   The lawyers are taking a conservative stance on this issue   Apologies its really disappointing to have people ruled for this reason   \\tflsdcom I have a meeting with them in  minutes   I will be sure to raise this point   In response to ashashos original question I have sought a reexamination of the issue   The HPN lawyers explained that the reason for the hard line is that they have no way to verify that residency permits comply with US legislation   Im really sorry to say that theres not more I can do   The accuracy threshold will be announced when we release the full claims dataset on May    I want to reassure everyone that HPN is working hard behind the scenes to clarify the IP issue   It is not their intention to prevent people from using standard tools nor to discourage anyone from applying their innovative ideas to this problem  \\xa0\\nFor background at Mondays launch event Dr Richard Merkin the man behind the prize spoke of the long tradition of innovation that has resulted from past prizes   He spoke of\\n\\nthe Longitude Prize     apparently Newton and Galileo had attempted to solve this problem but the winner was a self educated clockmaker from Yorkshire\\nNapoleons food preservation prize  won by a confectioner and resulted in the invention of canned food\\nthe Orteig Prize to fly nonstop from New York to Paris     won by the unlikely Charles Lindbergh  \\xa0\\n\\nIt is his hope that this prize will spur similar innovation to solve one of Americas most vexing problems  \\nWe appreciate your patience while we await clarification  \\nKind Regards\\nAnthony This is a sample of the final dataset but the final dataset is not in the Terabyte range   To the best of my knowledge this dataset is on the larger side for medical datasets which tend to be quite small  \\r\\n\\r\\nThis algorithm will not need to operate in a realtime environment and so there is no restriction on execution time   \\trudychev received an answer from HPN on this   A patient who visits a clinic outside the network should be captured in this dataset   Of course as Jeremy keeps reiterating there is always a disconnect between reality and the contents of a database   The decision to predict days in hospital was made to make the test dataset richer  so we can better sort out good algorithms from bad   The logarithm in the evaluation metric was chosen to favor models that predict short stays more accurately as these are assumed to be more readily preventable   \\r\\n\\r\\nAs for the question of nefarious intentions I can tell you what I know about Dr Richard Merkin the man behind the prize   He is a big philanthropist who devotes time and resources to funding scientific projects schools and the arts  \\r\\n\\r\\nIn my opinion HPN did not need to put up  million to get an amazing algorithm   Kaggle has found in its own competitions that with prizes as small as  or a chess DVD participants approach the limit of whats possible on a dataset   In our communications with HPN we have been told that the  million prize is an attempt to draw mass attention to this prize and the issue in general   Dr Merkin wants to promote the potential for medical data mining in lowering healthcare costs   The prize also serves to introduce a large number of talented data scientists to medical data  \\r\\n\\r\\nFinally rest assured that HPN are working hard behind the scenes to clarify the IP issue   \\tmgomari one issue we have to keep in mind are the tradeoffs in releasing data   For data privacy reasons HPN have a granularity threshold which theyre not willing to breach   The data anonymization team represented by keleman in the forums are trying to release CPTCodes probably at an aggregated level   Apparently its pretty lineball and releasng DaysInHospitalY might put this in jeopardy   I describe the data privacy considerations like a waterbed you push down on one part of the bed and it creates a bulge somewhere else  \\r\\n\\r\\nAfter May  youll be able to use DaysInHospitalY and DaysInHospitalY to predict DaysInHospitalY  \\r\\n\\r\\nogenex even if we release DaysInHospitalY you wont be able to do a consistency check   Not all length of stays count as hospitalizations as calculated for this competition and you dont have enough detail in this dataset to work out which count and which dont   For those who dont know jphoward was Kaggles most successful competitor before joining the team   His tutorial gives really clear explanations of the tools and techniques that made him such a successful competitor   Hi Jim\\r\\n\\r\\nThat is correct   For information the reason for the misnomer is that it was days when we sent it to the anonymization team but they had to group the days to ensure the required level of data privacy  \\r\\n\\r\\nAnthony \\tsciolist yes teams are required to publish publicly   \\tashojaee the clarifications havent been made yet   \\tmkarbowski as jphoward keeps pointing out theres often a massive disconnect between reality and the contents of a transactional database   See ejloks humorous post for even odder records\\r\\n   Agree   See the updated evaluation page\\r\\n   We intentionally decided against cleaning the data so as not to impose our assumptions on participants   \\tDomcastro one of Kaggles first suggestions was to remove the registration fee   \\r\\n\\r\\nFor info the registration fee wasnt ever to raise money but to try and deter people who werent serious from downloading this sensitive data   Kaggle pointed out that anybody with malevolent intentions would probably still pay the modest registration fee so its effect would be to deter people who didnt think they had a chance of winning   Kaggle went on to argue that these people may also come from interesting backgrounds and may be the ones most likely to apply creative thinking to the problem   DIH includes inpatient admissions and emergency room visits   As mentioned previously you dont have enough detail to calculate it from the claims table   We want the forum to be tightly integrated into the site e  g   to be able to link to forum posts from profiles and vice versa   YAF is the best   NET forum software out there and integrating it into Kaggle is more trouble than its worth   \\r\\n\\r\\nAlso moserware is a brilliant programmer so its the type of thing he could put together in less than a week   Realworld data is messy \\r\\n\\r\\nWell put up a data dictionary soon   quotedaveime\\nSeriously I understand the need for randomizing and anoymizing the data but unless they have some way to unrandomize it afterwards any algorithms we create will serve no real world application  \\nquote\\ndaveime the data is messy not because its been peturbed but because its realworld data   Anonymization focused on generalizing again not peturbing     The the nineyear old pregnant males actually exist in the raw data  \\nFor info Im told that this is one of the cleaner medical claims datasets around   \\tmgomari the difference between  and  is counted as two days  \\r\\n\\r\\nOverlaps were accounted for so were not double counted   \\tfjn Pi does not have to be an integer   \\tblonchar youre correct HPN are limited in what it can release by the need to protect patient privacy   \\tfrankthedefalcos  com or the women who have been treated for erectile dysfunction  \\tmgomari the answer to both questions is yes   \\tjesensky you will be able to use DaysInHospitalY and DaysInHospitalY as an input to DaysInHospitalY  \\r\\n\\r\\nI like your thinking on the USE OF OTHER DATA loophole if the answer had been no   Creative thinking \\tcybaea many thanks for a great discovery After doing some digging weve discovered that the oddeven observation is an artifact of the cleaning procedure   \\r\\n\\r\\nWe have worked out a remedy and it will be applied to the dataset that will be released on May    In the meantime it shouldnt make a huge different to models that are currently being developed   \\tboegel yes   On May  we will be issuing significantly more data   DayInHospitalY  csv will be changed then   \\tliveflow I may be misunderstanding the question but the competition requires participants to use data from Y Y and Y to predict Y   No   Some Y patients are no longer eligible in Y   We still provide Y patients who arent eligible in Y because theyre useful to train on   No   Again for privacy reasons   \\tInformation Man that is not the intent of the rule   The HPN lawyers are working on clarifying this at the moment   \\tDougieD every member listed in DaysInHospitalY is eligible to claim in Y  so if they have  DIH  they are  above   The same will apply for the members listed in DaysInHospitalY and DaysInHospitalY when we release those files   DaysInHospital is calculated based on the LengthOfStay variable   However you dont have enough detail to calculate DaysInHospital from LengthOfStay   \\tirwint good pickup  thanks Now fixed   \\tgschmidt not sure if this answers your question but the geographic spread is limited to the area in which HPN operates southern California I believe   \\r\\n\\r\\nAs to whether patients change doctors on May  youll have a few years worth of data so will be able to work this out   alexx the HPN lawyers are working on a clarification   This will be released by the time entries can be made on May    \\tmetaxab the competition was designed this way to replicate how the model might be used in real life   In a real life situation you wouldnt be able to predict hospitalization with contemporaneous claims   DaysInHospitalY is derived from the claims table where a hospital stay includes an inpatient stay or an emergency visit   Note you dont have enough information to calculate DaysInHospitalY from the claims table   Hi Drew\\r\\n\\r\\nIt will be in place by May  when entries are accepted   Anybody who accepted the existing rules will receive the notification via email   \\r\\n\\r\\nAnthony In this dataset missing PayDelay either means unknown or greater than    In the May  release the anonymization team will topcode PayDelay so there will be fewer missing values and  will mean    You will get some procedure code information in the May  release   I understand the frustration but data privacy is a priority for HPN   For generating features I recommend SQLLite  though MySQL does the same thing   I know Jeremy and Jeff like Cs Linq   For building models I use R   \\trks we will post a sample entry with the rest of the data on May    \\tRalphH DaysInHospital counts days not nights   So if DaysInHospital is  then they have not been to the hospital at all   If they were in and out of the ER then DaysInHospital would be    \\ttrezza and RHM Y contains data for a  period   trezza unfortunately not   The anonymization team have identified this as a data privacy risk   Hi Allan\\nThats because some members have had claims suppressed   In release  coming soon well make it clear which members this applies to  \\nAnthony\\xa0 Hi Domcastro  \\n   Can I use R\\nYes\\n   Can I use Weka\\nYes\\n   Can I use Excel\\nYes\\n   If I organise the data in a novel way and just use a standard processing algorithm such as Naive Bayes is this OK\\nYes You must preserve the order in Target  csv   Release  zip does supersede Release  zip   \\xa0 Unfortunately not   Apologies for any inconvenience   Darragh its a list of all members in the dataset   No   Chris just heard back from the data anonymization team   Members have been renumbered   \\tcacross HPN had a granularity threshold that they wanted to remain below   Some LOSs had to be suppressed to achieve this target   If there is a blank LOS and SupLOS is  then this is how it was when it came out of the HPN dataset   If there is a blank\\r\\n LOS and SupLOS is  then the LOS has been suppressed   Hope that helps   \\tmkwan you fill in the team wizard when you make your first entry   Team mergers will be granted at the organizers discretion   Yes   \\tChrisR nice to see you competing in this   Sampling is random   We cant give you an HPN benchmark because theyve not tackled this problem before  \\xa0 boegel DaysInHospitalY contains members who made a claim in Y and were eligible to make a claim in Y   DaysInHospitalY contains members who made a claim in Y and were eligible to make a claim in Y   Similarly target  csv contains members who made\\r\\n a claim in Y and were eligible to make a claim in Y   To be eligible means to be an HPN member regardless of whether or not a claim was made  \\nTherefore the  members in DaysInHospitalY are not missing from target  csv but rather didnt make a claim in Y or werent eligible to make a claim in Y   Therefore all members in target  csv were eligible to make a claim in Y  so we have an answer\\r\\n for each of these members  \\nJESENSKY by my calculation  members appear in DaysInHospitalY and DaysInHospitalY but not DaysInHospitalY perhaps you can confirm this figure   These members are missing from DaysInHospitalY because they didnt make a claim in Y despite\\r\\n being eligible  \\nApologies if we didnt communicate this effectively in the description pages   \\tDanB youre right about the selection bias   But because HPN are releasing almost no information on the members themselves theres nothing to model on for patients without claims   \\tProTester theres nothing in the raw data that distinguishes a death from a patient that leaves HPN for another provider   \\tSSRC mapping LOS to DIH is impossible   Not every LOS entry corresponds with a DIH e  g  hospice stay One reason somebody may have DIH in y but no claims is if they werent eligible to claim in Y in which case their Y claims wouldve been removed   \\tGeorge there are  members in the dataset but you are only tested on  members   Thats because the extra  members arent eligible to claim in Y or didnt claim in Y   They have only been provided to help you train your model   Tom SF Haines Jeremy is not the author of the rules   He is merely trying his best to point people to the section that makes the rules as competitor friendly as possible given HPNs requirements  \\nAlso if you would like to publish your algorithm I strongly encourage you to put in a research request using the Contact Us form    is the maximum   Ive said this before but I think \\r\\nJeremys tutorial is really excellent although it is not focussed on HHP   He is hoping to get the opportunity to do an HHP tutorial in the next few months   \\xa0 Further to Wills point those who followed the Netflix Prize will remember the jump from the Simon Funk discovery  \\xa0 She will be added in the next release  \\xa0 Darragh I passed your question onto HPN   Heres the reply\\nIs there a delay between the scheduling of the surgery and when it takes place \\xa0Yes   \\xa0But that is just a matter of scheduling not something forced by the government   \\xa0It would also of course depend on how urgent the surgery is   The intent of that provision is to prevent the data being shared with those who have not agreed to the competition rules   Jeff was just referring to the measures he would take to ensure the data isnt accessible to others   Jose thanks for your diligence on this   Its difficult for us to give specific guidelines   Again HPN is just trying to prevent the data from being accessible to those who havent accepted the rules   Jim its being assessed against Y hospitalizations   Bernhard your interpretation sounds about right to me  \\xa0 Thanks Dave   The data description has been fixed   Hi guys\\xa0\\nGlad you like   This dataset reminds me of the RTA data which was really popular  \\nOn the IP question when no rules are explicitly stated the Kaggle \\r\\nterms and conditions prevail   Specifically clause   \\nBy accepting an Award you agree to grant a license to the Competition Host to use any Model used or consulted by You in generating Your Entry in any way the Competition Host thinks fit   This license will be nonexclusive unless otherwise specified  \\nAnthony Hi Bobby\\nCan you clarify what you mean by this Are you asking if they are obliged to share their model if they finish in first place\\nAnthony Hi Willem\\n\\nto what extend the results have to be identical for example small differences in the random number generator may give different results although they should be similar\\r\\n\\nThey do need to be identical   You can give your random number generator a seed to make sure the resultls are the same each time  \\n\\nin how much time should the results be reproduceable my current best result is a mix of many models each may take minutes to hours to generate\\r\\n\\nThere is no rule about execution time     \\n\\nthe algorithm should produce similar results on a new dataset this doesnt sound very realistic I dont think there is any way to win this competition without optimizing for this specific dataset   Results on other datasets may be very bad with the\\r\\n given optimizations   Probably very good results can be produced by the same algorithm after some tuning but this is a process that requires a lot of knowledge about the used algorithms and a lot of time and patience  \\r\\n\\nNot sure I follow why this is an issue   Remember the Milestone prize is judged in a portion of the test dataset that participants have not been given any feedback on   Perhaps Im misunderstanding the concern  \\nHTH\\nAntthony Regarding the requirement that solutions be identical\\nWillem it would be better to have participants spend time on innovation rather than reproducibility however its important to have strict rules so that the competition remains as fair as possible  \\nB Yang with regards to the compiler issue we can address it if the issue arises   For example we might start by ensuring that the same compiler is used for verification  \\nSali Mali it is exceptable to describe the algorithm and not how it is derived   We are seeking clarification from HPN on the inconsistency that you describe   Apologies for the delay  \\nRegarding the requirement that the algorithm perform similarly on a separate dataset\\nThis is best answered by explaining the rationale behind the rule   It is there to catch any cheating or blatant overfitting   If youre not blatantly overfitting then youre likely to be on safe ground  \\xa0 Hi all\\nNot ignoring this thread   Just seeking clarification from HPN on one issue   \\nAnthony John\\nOnly the lowest of the five entries count   Note for the milestone prize only one can be selected  \\nAnthony I have checked with HPN and a milestone prize winner can choose not to disclose their method but will not be eligible for the milestone prizes  \\r\\n Sorry for the delay on this was just clarifying some issues with HPN  \\n\\nIs it inconsistent as Sali Mali pointed out in another thread to require documentation of the winning algorithms be publicly disclosed to all competitors given Rule  Entrant Representations \\xa0It seems that this disclosure will encourage other competitors\\r\\n to use aspects of the winning Prediction Algorithm which cause violation directly or otherwise of i  iii and possibly iv of that Rule  \\r\\n\\nRule  does not apply to the extent that it prevents a competitors other than a milestone prizewinner from using code published by a milestone prizewinner in accordance with competition rules and b a milestone prizewinner from competing subsequently\\r\\n in the competition using code for which it was awarded the milestone prize  \\n\\nCan you clarify that code libraries and software specifications\\xa0are not required\\xa0to be publicly disclosed to competitors \\xa0These materials and intellectual property appear to be referenced separately from Prediction Algorithm and documentation  \\r\\n\\n\\nChris correctly points to Jeremys response in an earlier forum post\\n“Only the paper describing the algorithm will be posted publicly   The paper must fully describe the algorithm   If other competitors find that its missing key information or doesnt behave as advertised then they can appeal   The idea of course is that\\r\\n progress prize winners will fully share the results theyve used to that point so that all competitors can benefit for the remainder of the comp and so that the overall outcome for health care is improved  ”\\n\\n\\n\\nWill Kaggle or Heritage have a moderation or appeals process for handling competitor complaints \\xa0From the winning entrants pointofview they would not want to be forced through the review process to allow backdoor answers to code and libraries which\\r\\n accelerate a competitors integration of the winning solution   \\n\\nKaggle and the HHP judging panel will moderate the appeals process  \\n\\n\\nCan you comment on the spirit and fairness of the public disclosure of the Prediction Algorithm documentation and its impact on competitiveness \\xa0In particular if the documentation truly does meet the requirement of enabling a skilled computer science\\r\\n practitioner to reproduce the winning result then this places the winning team at an unfair disadavantage all competitors will have access to their algorithms and research in addition to the winning algorithm  \\r\\n\\n\\nThis rule is in place to promote collaboration   Those who would prefer not to share can opt out of the prize  \\n\\n\\nCan you provide more detailed clarification on the level of documentation required by conditional milestone winners \\xa0The guideline provided by the rules would cover a range of details and description spanning from lecture notes to detailed tutorial\\r\\n to whitepaper to conference paper etc   \\n\\nHopefully this was adequately dealt with in Jeremys response requoted above   Let me know if further clarification is needed  \\n\\n\\nCan you comment on the reproducibility requirement \\xa0For example it is possible to construct algorithms with stochastic elements that may not be precisely reproducible even using the same random seed is it sufficient for these algorithms to reproduce\\r\\n the submission approximately \\xa0What if they dont reproduce exactly or reproduce at a prediction accuracy that is worse than the submission score possibly worse than other competitor submissions \\xa0\\r\\n\\n\\nExactly reproducibility is required  \\xa0    Correct If you were  per cent sure that somebody would spend  days in hospital in Y and  per cent sure they would spend  day in hospital than you might predict that they spend would    days in hospital   pham you do not have enough detail in the claims data to reproduce the DIH properly   Youve likely reproduced DIH from claims data as accurately as is possible  \\xa0 SirGuessalot thanks for the pointer   Its been added to our issue tracker   I must admit we have higher priority issues to tackle but well get there eventually  \\r\\n Just to keep you all in the loop the plan is to announce the milestone prize winners at OReillys Strataconf    Will let you know the exact date as soon as\\r\\n were told  \\xa0\\xa0 Full milestone prize rankings will be released after the announcement is made   The rules do not prohibit Oracle Data Miner    Hi all  \\nHPN are currently looking for data scientists\\nHeritage Provider Network  the sponsor of the Heritage Health Prize  is looking to hire data scientists to take its data and analytics department to the next level  \\xa0 If you are interested in healthcare join the largest physicians group in California\\r\\n and one of the largest in the United States and use your data mining skills to make a difference in the provision of health care to individuals throughout Southern California  \\nIf interested please send an email indicating your interest to \\r\\ndatascientistheritagemed  com  \\nAnthony Provisional milestone prize winners will receive an email over the weekend   An announcement will be made at Strataconf on September \\n   Jason the anonymization guys have withheld this information intentionally to make the data set more secure   Sorry Correct   libraryrandomForestsetwdCUsersantgoldbloomDropboxKaggleCompetitionsCredit Scoringtraining  read  csvcstraining  csvRF  randomForesttrainingctrainingSeriousDlqinyrs                   sampsizecdo  traceTRUEimportanceTRUEntreeforestTRUEtest  read  csvcstest  csvpred  data  framepredictRFtestcnamespred  SeriousDlqinyrswrite  csvpredfilesampleEntry  csv Alec setting the random seed is a good idea  \\nDomcastro your hypothesis is correct  \\xa0 Youre correct   Shouldnt include headers    Congratulations team Market Makers and Willem Great coverage in the Wall Street Journal here\\r\\n   For those interested heres the footage from the award ceremony\\r\\n   \\t Does being a member of HPN mean you usually referred to an innetwork provider of say lab testing unless obviosuly it is some specialty unavailable Yes  Can you be a member of HPN and have govt sponsored insurance eg Medicare MediCal Yes for\\r\\n Medicare   I can follow up on MediCal if you like Have passed these questions onto HPN   Will respond as soon as I get an answer  \\xa0 On October  the judges in their sole discretion decide whether or not the documentation is sufficient taking account of the comments made on this forum  \\xa0If they decide the documentation is not sufficient they can impel the winners to address their\\r\\n concerns in the seven days following October    If the winners are asked to resubmit participants have another  days from November  to raise any additional complaints  \\xa0\\nThe judging panel\\xa0are experienced academic reviewers\\n  \\n\\xa0\\n\\xa0\\n\\xa0\\n\\xa0\\n\\xa0\\n\\xa0\\n\\xa0\\n\\xa0 Hi all\\nWe are in the process of liaising with the judges   Well report their decision as soon as we have everybodys feedback   We have made a slight change to the\\xa0Terms and Conditions\\xa0adding   \\n\\nNo individual or entity may share solutions or code for any competition or collaborate in any way with any other individual or entity that is participating as a separate individual or entity for the same competition   The foregoing shall not apply to any\\r\\n public communications such as forum participation or blog posts  \\n\\r\\nWe are also aware that the rules havent been as clear as we might have liked   From now on before you download the data for any new competition you will be reminded that\\n\\nyou cannot sign up to Kaggle from multiple accounts and therefore you cannot submit from multiple accounts and\\nprivately sharing code or data is not permitted outside of teams sharing data or code is permissible if made available to all players such as on the forums  \\n\\nWeve reached out to several teams about this issue   Please let us know ASAP if you have multiple accounts and weve not reached out to you   \\nWe are aware that the rules havent been as clear as we might have liked   Please be reminded that\\n\\nyou cannot sign up to Kaggle from multiple accounts and therefore you cannot submit from multiple accounts and\\r\\nprivately sharing code or data is not permitted outside of teams sharing data or code is permissible if made available to all players such as on the forums  \\r\\n\\nWeve reached out to several teams about this issue   Please let us know ASAP if you have multiple accounts and weve not reached out to you  \\n It is a mistake  were sorry for it but weve decided not to correct it because it might not be fair to some contestants if we change the data midstream  \\nShouldnt be too importantonly happened to  chunks  \\nIts the same mistake that caused a few chunks to have some missing data within the chunks\\r\\n\\r\\n   Sounds like theres a thriving community in Melb which looks to have been the strongest performing city   Congrats all Thanks for the nice wishes   Of course Kaggle wouldnt exist without a brilliant community of data scientists who can solve really challenging problems   Looking forward to seeing what we can do in  Donovan weve looked into this and it turns out that a bug with our process meant that we hadnt received the past few weeks of queries   Weve found your email and you will receive a response shortly as will others who slipped through the cracks   Apologies\\r\\n to you and others who have not received a response as a result of this error   One of my coworkers\\xa0said were really doing well if you think of Kaggle every time you see the Facebook logo   \\xa0 Nice \\nFor interest we typically see strong metrics on Kaggle during holidays because people have more discretionary time  which at least suggests our community isnt too busy with family   Another possible explanation is that people have exhausted their travel budget both time and money on holiday travel and need to wait a little while before booking more travel    Clear and entertaining   Nice work Why does lower bound get mentioned so much more than upper bound Ive played with PCA before but never association plots or MCA   Glad to see an example usage and be able to add these to my toolkit   Thank you\\nFor the association plots I assume the width of the box refers to the number of Tweets referring that that airline\\nI assume    is the proportion of comovement explained by the first dimension   Is that correct Is it typical for the first dimension to explain so much of the comovement Any thoughts on how to interpret this dimension \\nSmall nit You might want to change res to reason   I initially assumed res stood for residual   And reduce the font size for the x axis label on plot    Thats the most interesting plot to me but its hard to read the labels because they overlap    This is a nice notebook   \\nSuggestions\\n\\nTo make this easier to follow for those who havent yet looked at the data itd be great if you added a section showing a few rows   Or possibly even a few exploratory chartshistograms   Perhaps after the Loading the data section   \\nItd also be nice to see the before and after you preprocess the data ie before and after the Using textmining to format our data section   \\nRename the Using textmining to format our data to something like Cleaning the data  \\n Great I always look at the top rated notebooks before looking at the data because the notebooks usually give me a sense for whats in the data and what I could do with it    Love it Interesting that for everyone other than Woodrow Wilson the names popularity monotonically declines over the course of the presidency   \\nDwight looks like it increases in popularity during WW which makes sense   One suggestion is to add years to the x axis label for each chart to make things like this easier to spot   I Tweeted this script and somebody replied asking \\nIs there a corresponding drop in the name frequency of the losing presidential candidate right after the election   I was thinking another interesting extension would be to answer the question Whats most influential in determining baby name trends out of  \\n\\nPresidents and first ladies   \\nMusician that was  for longest on the billboard charts in a given year  \\nBest actoractress in the Oscars  \\nBasketball football baseball MVPs  \\nNobel prize winner names  \\nTime person of the year\\n\\nIf nobody else tackles this I might try it   \\nThis builds off a conversation I had with my coworker Meghan who said itd be interesting to see whether Presidents or royal babies had a bigger impact on baby names   From this page\\n   This is great   \\nIm surprised North America is not higher for sugar   The sweetness of food was one of the first things I noticed when we moved to the US from Australia   Although it could be because a lot of the sweetness comes from high fructose corn syrup which is not captured    Nicely done and fun writing style   \\nOne additional conclusion is that real data is messy    Big Data Borat captures it best \\nIn Data Science  of time spent prepare data  of time spent complain about need for prepare data   \\n   Interesting how noisy the very early years are   I suspect the s data is very poor quality    Really nice script   \\nInteresting to see the temperature uncertainty chart   Gives a nice visual of when the data starts becoming more reliable   \\nAlso nice idea to put dt into a variable importance plot to see its relative important   Obviously would have been more interesting if wed provided more data   \\nOne suggestion is to better label your plots   Theres some good stuff here but it stakes a while to figure out what each chart is showing I actually looked at your code to figure it out   I suspect this script will be more popular with some labels that make it easier to follow    Sven have you been able to figure out an interpretation of this chart Thanks   Itd be helpful if you labeled the charts and possibly added sub label pointing to your interpretation    It may not be useful for the reasons you mention but it looks nice    Would be cool to see the by city version   I assume you didnt use it initially because of the size of the data set BTW I assume red  hot Would be helpful to have a key    Its awesome Really nicely put together   Bluefool I thought you came out really well    Is this a work in progress Or is there an error The charts are showing up blank for me   Juanchaco this is neat but itd be easier to follow if you added a description between charts   At the moment Im scanning the codecomments to try and figure what each chart is showing    Ha   Id not known about this   For others    Cool   As someone who lives in San Francisco Im curious    Akshay this script would be more interesting if you found a neat way to visualize temperature by country    Love this chart And the title is funny    Just a heads up that were still working through the winners solutions   Will need more time to before announcing the final results as official    Quick update We will announce the official results on Wednesday March  at am ET    Thanks for the thoughtful comments   \\nFirst off as always we will not make retrospective changes to how we handle past competitions including this one   When issues like this come up we use it as an opportunity to evaluate how we might improve in the future  \\nInternally our debate focused on three issues  \\n\\nrecognition for those who completed stage one but not stage two \\nachievements and how the competition appears on profiles th out of  looks more impressive than th out of   \\nhow points are handled  \\n\\n   recognition for those who completed stage one but not stage two\\nWe need to view the stage one leaderboard as having no weight if it gets a weight we incentivize overfitting or hand labeling for stage one  \\n   achievements and how the competition appears on profiles\\nIf we did what Julian suggests and add stage one participants to the bottom of the stage two leaderboard we undermine our rankings by making it very easy for somebody to get an impressivelooking top  achievement by finishing th out of  with a naive submission   \\n     how points are handled \\nThe one change we will make in future is the way points are handled   We will add a multiplier to the number of points for a twostage competition   We have not settled on a formula for doing this yet but commit to communicating it clearly in the rules of the next twostage competition  \\nThese are difficult issues but we think this approach strikes the best balance between competing considerations    Julian responded in the other thread \\n   Hi all\\nThe results on the final leaderboard are now official   Congratulations to the winners and all involved   This is among the hardest and most ambitious competitions weve hosted   We couldnt be prouder of the results   \\nThe competition has received some press coverage with a chance of more to come\\n  \\n  \\nAnthony This is the photo from the Kaggle office   This lunch was one of the highlights of my six years of Kaggle   Not something I will forget in a hurry    Minor comment theres a typo in the title Prelimnary should be Preliminary    I only make this comment because its a nice script and I dont want grammar sticklers to be put off the typo    Jeff interesting   Seems like makes the system a little conservative in the handling of new players    Change made   Thanks for the feedback    Cool to get real world SKU data but how would I create a recommendation engine with just SKUs and ie without customer data I guess Ill see when you upload your code    Thanks Allen Implemented most of these changes    Allen Jeff or anyone else how do people typically prevent ratings from becoming stale with Trueskill for example nonactive racers maintaining high rankings   I am planning to make the rankings apply over a  month rolling window but am curious if there were other approaches such as the inclusion of some kind of time decay     This version should now be correct   The model does not systematically make money but its only using very basic features barrier weight and rider   Hopefully its a good start and somebody can take it and build on it    Hi all\\nJust a follow up on the request to not share solutions as mentioned above this is not a legal obligation but rather a request from the host   We try and avoid requests like this because it limits the learning that comes out of a competition   \\nOur takeaway from this thread is that if there is a confidentiality request we will flag it up front to avoid an unwelcome surprise at the end of a competition   \\nAnthony Rakhlin one thing were experimenting with is asking hosts to write blog posts summarizing the outcomes from a competition   This wont be timely because itll happen after theyve spoken to the winners and digested the results   Unclear what reception we will get from hosts but its something were testing out    Foxtrot did you find a bug or a mistake in my code Or is it that the returns just dont look right given how simple the features are Hi Foxtrot   I commented out that line because I changed the problem to predict the probability of victory for each horse rather than  position   \\nxgb  XGBClassifierobjectivebinarylogistic  fitdftrain  dropdftrainwinpositionmarketidaxis\\nSo I thats not the source of leakage   Im not actually certain there is leakage   If you run the code multiple times the return switches from being positive to being negative depending on the traintest split   Having run it a bunch of times I suspect the expected return is actually negative    Right   But that doesnt happen in this case because Im predicting probabilities   So the predictions are                Foxtrot nice pickup  thanks   Uncommented the shuffle the deck line   \\nI suspect theres more leakage in this analysis because the trainingtest split is random and not timebased   The way to get around this for this data set is to train on the form table and test against the runners table   Or even more useful would be to get less anonymized data if Luke or someone else has it    At the every least race dates are valuable make it possible to protect against leakage   \\nBut more generally getting access to the complete raw data is valuable   Everything one does to disguise the data destroys information and limits what a data scientist can do with it   Some small examples related to knowing the venue\\n  might give information on what surface the horses are running on\\n  a km race at one racecourse could be a straight whereas at other courses it could involve a turn \\nThis might impact the performance of specific horses and cant be accounted for when the variable is venueid   \\nFinally for boosting engagement with the data set anonymized data is less fun to play with than richer raw data   I reversed Moody Bluefools downvote \\nBy upvoting   I dont have database write privileges    Chris I built a model to project out my Sep  and Oct  results   Shame theres no Oct  race   According to my model Id finish first Queue XKCD      \\n William nice Kernel   Looks like your predicting author based on the number of comments subject etc   Whats the thinking behind predicting who the author is I was thinking it might be interesting the predict the number of comments a proxy for how interesting the article is\\nAlso I was looking at your error chart and your comment that It seems like we are    better than chance     Curious how youre measuring that I tried to read it off the chart but couldnt see where it came from    William fyi    That was quick Your code is much cleaner   Puts pressure on me to go back and clean up mine    This was really just a quick analysis   If I had more time Id have actually looked at the words being used in the posts rather than guessingrelying on memory   E  g   I probably shouldve included RNN   I could also have included package names Tensorflow Keras etc   In fact another version of this might look at the packages that are most commonly used by winners    Ps   Thanks for the bug report on the file download   Well look into it   I was wondering why the data set had  downloads       Jordan looks great Now that the data is in CSV format I featured it   \\nWriting a first exploratory Kernel to show people whats in the data is typically helpful for driving driving engagement from the community    Sounds good   I was thinking itd be fun to apply Trueskill to this dataset I used it here   Id like to do it but probably wont have time over the next few weeks unfortunately    Marginal Revolution featured Will Novaks analysis of their posts   If there are interesting findings made on Techcrunch data Ill send it across to Techcrunch   Theytheirreaders may be similarly interested    David I have been able to create the RDS file\\nlibraryggmap\\nmapdata  getopenstreetmapbbox  c            colorbw scale  round\\nsaveRDSmapdatafilestpete  rds\\n\\nHow do I create the text file \\nAlso why arent you using OSM files the Open Street Map XML file format rather than using GGMap to export to txt or RDS \\nIve spent a while trying to import OSM files for plotting in Python but with no success   Wondering if you also found this difficult   \\nps   This is for my GPS watch data so the coordinates above are not the Chicago coordinates   \\n   Anokas thanks My bearing calculation is incorrect   My next step with this notebook is to debug it   \\nHow would I use\\nnp  raddegnp  arctan\\n Anokas I figured out my problem   I was feeding latitude and longitude into the calculate bearing function in the wrong order feeding in lat as long and vice versa   \\nAdded your more elegant bearing calculation formula as well   Thanks   Ryan appreciate you raising the concern   I want to share Kaggles perspective  \\nFirst off internally we are incredibly excited about the launch of codeonly competitions   Its the biggest change weve made to competitions since we launched in    It increases the range of competitions we can run including time series competitions such as this one reinforcement learning competitions and competitions on larger data sets  \\nOn this competition specifically this is the first step in what we hope will turn into a bigger relationship between Kaggle Two Sigma and the community   This first competition is aimed at testing out the concept of codeonly competitions gauging the communitys interest in financethemed competitions and getting a sense for the types of signals the community finds on a typical financialmarketsrelated data set   \\nrakhlin regarding your comments about anonymization and the use of an API these were primarily driven by Kaggle   The anonymization is in place to discourage people from looking up the answers   The API allows this to be a proper time series competition   We try to make our competitions fair this means making it difficult for the rare participant who is inclined to circumvent the rules  \\nUltimately our goal is to give our community as many opportunities as we can   This means a mix of commercial competitions research competitions playground competitions getting started competitions and most recently open data sets   Ultimately the community will select what interests them   And Kaggle will continue to offer the things that resonate  \\nAnthony Did you try the my submissions tab on the left hand side dashboard You could also look at some of the weather data and Kernels that others have shared \\n   Hi Jackie   Perhaps this dataset is a good starting point    Totally agree with frustrations expressed here   Neither Kaggle DHS nor the community wants geo restrictions  \\nIn this case its a legal restriction   DHS worked hard to allow any international participation albeit without prize money  \\nFrom Kaggles perspective we prefer to host a competition with a geo restriction and make the opportunity available to the community than not host   We think this is a better albeit imperfect outcome   I understand and agree with frustrations around nonUS citizenspermanent residents being ineligible for prize money   None of Kaggle DHS or the community wants this restriction  \\nThe reason for the restriction is the America COMPETES Act   DHSs legal team worked hard to find a way to allow international participation at all albeit without prize money while not violating the act  \\nFrom Kaggles perspective we had a choice host this competition with a geo restriction or not host at all   This was a tough choice   Hosting the competition meant running a twospeed competition where some are eligible for prize money while others are not   Doing this violates our desire for meritocracy where we offer equal opportunities to every community member regardless of education background and of course country  \\nIf we dont host then we dont expose our community to one of the most interesting and valuable datasets thats ever been hosted on Kaggle   As weve seen over the past five years with the rise of deep neural networks the availability of datasets allows us to push forward to science of machine learning   Not exposing this dataset to our community meant depriving our community and the machine learning world more generally of a chance to push forward the science with a novel and challenging dataset  \\nWe decided that it was better to make the dataset available   Many in this forum have said this was a mistake   This was a challenging issue I hope this at least gives you a window into our thinking  \\nAnthony kitefoil    James those speeds are a actually little slow   I will upload my latest data soon but thats  knots upwind and  knots downwind Ive improved with training   And there are kitefoilers who are much quicker than me       RounakBanik nice dataset Can it be updated to be current If you can make it current and update this notebook I will send it to the TED team   \\nWe did this with Marginal Revolution and it ended up getting featured on their blog\\n  \\nWho knows maybe the TED team will be really interested and can highlight it in some way    Rounak apologies   I missed that youd updated this   I will send to TED   Fingers crossed they will pick it up Kamil and HamaChi I actually dont think HamaChis expectations are too high   Having demanding users is healthy   It pushes us to improve our products  \\nWere aiming to make Kaggle Kernels into the leading cloudbased workbench for data science not just a free compute environment   Were migrating onto GCP and making some architecture changes that we expect will make a big difference  \\nStay tuned  \\nAnthony We have spent considerable time discussing the situation internally here at Kaggle and with Zillow   We thought Kaggle was putting sufficient emphasis on Zillow’s eligibility criteria by posting it on the competition overview page   Its clear from this thread that this nonstandard eligibility rule was not sufficiently publicized to the community  \\nAs an acknowledgement of this we are making the following adjustments\\n\\nWe are reinstating the affected teams so they will be eligible to receive points and prize money\\nWe are adding supplementary prizes if needed   These prizes aim to avoid penalizing teams that would have been in the top three teams if we didn’t reinstate teams who aren’t in compliance with the eligibility criteria   For example a team that’s in fourth place on the final leaderboard will be awarded nd place prize money if the nd and rd place teams don’t meet the eligibility criteria  \\n\\nNote the eligibility criteria for round two remains unchanged pasted below for convenience   We will soon be reaching out to everyone who places in the top  teams to verify their employment or institutional affiliation to ensure compliance with the official competition rules   Anybody who works for a company that is referenced in the eligibility criteria will not be accepted into round two   If you were on a team with one or more members who are ineligible you will still be accepted to move forward into round two but the teammates who doesn’t qualify will not   \\nIn the future we will make a bigger effort to make nonstandard rules clearer posting them as a pinned forum post for example and inviting questions and clarifications   But please also remember that the rules are important   Ultimately it is each Kagglers responsibility to read the rules before choosing to participate   Nobody wants a situation where Kagglers are putting considerable effort into a competition that they are ineligible to participate in   \\nAnthony\\nEligibility Criteria\\nMembers of the following entities are not eligible to participate in either round of the Zillow Prize Contest  any commercial entity that engages in the sale valuation or analytics of residential or commercial real estate  any entity that offers services in the leasing and property management space including vacation rentals and  any entity that monetizes residential real estate related data   Officers directors employees and advisory board members and their immediate families and members of the same household of Sponsor Kaggle and each of Sponsor’s and Kaggle’s respective affiliates subsidiaries agents judges and advertising and promotion agencies   In addition you are not eligible to participate in the Zillow Prize Contest if you are a a resident of a country designated as an embargoed country by the United States Treasury’s Office of Foreign Assets Control see    for additional information or b are an individual that appears on the United States Treasury’s Office of Foreign Assets Control Specially Designated Nationals and Blocked Persons List see    for additional information   Bojan totally agree that we dont want to incentive Kagglers to hide personal information   In this case nobody will be removed from the round one leaderboard because of personal information they have shared   And everybody will have to volunteer their affiliations to be eligible for round two   That removes any discrimination based on publicly shared user information or Wired Magazine profiles    \\nCompetitions with eligibility criteria are quite rare so we havent figured out all the nuances   Cant promise we wont make mistakes in the future but we aim to keep improving    We spoke about that option in connection with the TSA competition   Its not clear that it was legal in that case   \\nThis question is moving into a more general discussion of eligibility criteria   If we want to move the conversation to a more general discussion of eligibility criteria I suggest we move it to general    The eligibility criteria still applies to entry to round    So only teams that meet the eligibility criteria will be accepted into round     Have you tried Trueskill before It should be more powerful because from memory it doesnt assume a fixed standard deviation so it takes into account the certainty about a rating when adjusting ratings after a game   Itd be interesting to compare the performance of Elo vs Trueskill   Theres a nice Python implementation of Trueskill    Love this story   Tenacity counts for a lot   Well done Ryan On Monday Numerai announced that they were giving away their cryptocurrency to Kaggle users with a rating above Novice and accounts created before March    This wasnt done in partnership with Kaggle  we had no idea it was coming   \\nFollowing that announcement there has been a massive increase in the number of login attempts to the Kaggle website   These are attempts to break into Kaggle accounts in order to claim the cyptocurrency airdrop   This is both stressing our systems and putting Kaggle accounts at risk  \\nAs a result we have\\n\\nremoved the ability add    as your website URL\\nforced a password reset for anybody whose website was set to   \\nsent an email to the Numerai CEO letting him know that this has happened\\n\\nIf you find that your password has been reset please go through the Forgot Password flow  \\nAnthony  Im always happy when the Kaggle credential gives our community opportunities that they wouldnt otherwise have    In this case the way offer was structured effectively created a bounty for hacking Kaggle accounts   And the fact that we had no notice meant that we couldnt think through the implications and prepare    \\nwere going to look at changes to the Kaggle login flow this week   \\nNumerai has suspended the Kaggle Airdrop   \\n nathanforyou nice feature  \\nNote These community guidelines are replaced by revised guidelines available here   \\n\\nThe Kaggle community has a lot of diversity with members from over  countries and skill levels ranging from those learning Python through to the researchers who created deep neural networks   We have had competition winners with backgrounds ranging from computer science to English literature   However all our users share a common thread you love working with data  \\nAs our community grows we want to make sure that Kaggle continues to be welcoming   To that end we are introducing guidelines to ensure everyone has the same expectations about discourse in the forums\\n\\nBe patient   Be friendly    \\nDiscuss ideas dont make it personal    \\nThreats of any kind are unacceptable     \\nLowlevel harassment is still harassment    \\n\\nThe Kaggle team determines whether content is appropriate   If you see something that violates these guidelines you can bring it to our attention using the flag option on messages and topics   If you have a serious concern you should report it to supportkaggle  com   All reports will be kept confidential    \\nSad to share that Kaggle Master Vlado Tomecek passed away on Sunday October    \\nI received the note attached above from Dana Vlados sister   \\nI remember sitting in on his call with Draper Labs the competition that he won   His solution involved a tremendous tenacity and creativity    In our exchange Dana mentioned that data science was Vlados passion and main focus    On behalf of the Kaggle team we wish Vlados family well   And we will miss his presence in the community    justinminsk this is nice FWIW I suggest updating the title to something like Identifying the best college wine   The kernel is more fun than the title suggests     Faraz has this been address If not send me a private message with a link to the comment and Ill make sure we take a look ASAP    I’m a longtime competition lurker whos finally mustered the courage to join a competition 😅   I picked this competition because I find the topic interesting if I was going to college today I’d probably pick biology   And also as a chance to try out Google AutoML Vision   \\nKaggle is part of Google now which gives me exposure to some of the technologies that are generating excitement inside Google   AutoML is a tool with a lot of buzz   \\nI hadn’t paid much attention to the Automated Machine Learning Tools AMLTs until the KaggleDays competition in San Francisco this past April   Two AMLTs Google AutoML and H ended up participating in the hackathon and getting top  performances here’s the Google AI writeup of their performance   While I believe it requires human creativity to do problem setup and domainspecific feature engineering KaggleDays left me wondering whether humans need to be doing generic feature engineering architecture selection picking activation functions and setting learning rates   \\nI plan to share details of my experiments and experiences with Google AutoML Vision in this thread   I’m also open to suggestions from others in the community for things I should try   But bear with me if I take sometime to respond like many other Kagglers I have a busy day job   First Few Experiments\\nI started with the images created by xhlulu’s kernel thank you xhlulu   In that kernel xhlulu converts the data to x RGB jpegs   \\nBy default Google AutoML randomly splits between TRAIN VALIDATION and TEST   And the predicted outputs labels and confidence scores for each image I define a threshold and it outputs all labels and confidences above that confidence threshold   \\nI made predictions for both sites and then picked the label that had a higher confidence score across both sites   Below is a table of my scores   Note AUC is the accuracy metric that Google AutoML Vision reports   Haven’t put the time into figuring out what it means but I assume it’s probably treating each class as binary onoff and then aggregating up  \\n\\n\\n  \\n   Training Hours\\n   \\n   Resolution\\n   \\n   Number of Images\\n   \\n   Extension\\n   \\n   AutoML AUC\\n   \\n   Public Leaderboard Score\\n   \\n  \\n  \\n   \\n   \\n   \\n   \\n   \\n   \\n   jpeg\\n   \\n     \\n   \\n     \\n   \\n  \\n  \\n   \\n   \\n   \\n   \\n   \\n   \\n   jpeg\\n   \\n     \\n   \\n   \\n   \\n  \\n  \\n   \\n   \\n   \\n   \\n   \\n   \\n   jpeg\\n   \\n     \\n   \\n   \\n   \\n  \\n  \\n   \\n   \\n   \\n   \\n   \\n   \\n   jpeg\\n   \\n     \\n   \\n     \\n   \\n  \\n\\n\\nI then made small modifications xhlulu’s code to create x RGB pngs to test the impact of higher resolution images  \\n\\n\\n  \\n   Training Hours\\n   \\n   Resolution\\n   \\n   Number of Images\\n   \\n   Extension\\n   \\n   AutoML AUC\\n   \\n   Leaderboard Score\\n   \\n  \\n  \\n   \\n   \\n   \\n   \\n   \\n   \\n   png\\n   \\n     \\n   \\n   \\n   \\n  \\n  \\n   \\n   \\n   \\n   \\n   \\n   \\n   png\\n   \\n     \\n   \\n   \\n   \\n  \\n  \\n   \\n   \\n   \\n   \\n   \\n   \\n   png\\n   \\n     \\n   \\n     \\n   \\n  \\n  \\n   \\n   \\n   \\n   \\n   \\n   \\n   png\\n   \\n     \\n   \\n     \\n   \\n  \\n  \\n   \\n   \\n   \\n   \\n   \\n   \\n   png\\n   \\n     \\n   \\n     \\n   \\n  \\n\\n\\n   is my best score   It’s a result of a poorly thought out experiment so I’m surprised it’s my best score   I trained a model for  hours that included the control images   But after inspecting the testset predictions I realized my model was often picking labels   which are control labels that don’t appear in the test set   I ended up filtering the testset predictions to go to the next most confident label when a label  was predicted   \\n\\n\\n  \\n   Training Hours\\n   \\n   Resolution\\n   \\n   Number of Images\\n   \\n   Extension\\n   \\n   AutoML AUC\\n   \\n   Leaderboard Score\\n   \\n  \\n  \\n   \\n   \\n   \\n   \\n   yes\\n   \\n   png\\n   \\n     \\n   \\n     \\n   \\n  \\n  \\n   \\n   \\n   \\n   \\n   yes\\n   \\n   png\\n   \\n     \\n   \\n   \\n   \\n  \\n  \\n   \\n   \\n   \\n   \\n   yes\\n   \\n   png\\n   \\n     \\n   \\n   \\n   \\n  \\n  \\n   \\n   \\n   \\n   \\n   yes\\n   \\n   png\\n   \\n     \\n   \\n     \\n   \\n  \\n  \\n   \\n   \\n   \\n   \\n   yes\\n   \\n   png\\n   \\n     \\n   \\n     \\n   \\n  \\n  \\n   \\n   \\n   \\n   \\n   yes\\n   \\n   png\\n   \\n     \\n   \\n     \\n   \\n  \\n\\n\\nGoogle AutoML Vision allows me to define my own TRAIN VALIDATION and TEST splits   For my next experiments I’m going to start using this feature   I’m going to define my own splits to\\n   Include the control data in TRAIN but not VALIDATION or TEST to allow the model to use the control data for training but to discourage the model from predicting labels  on the competition test set\\n   Split by experiment rather than randomly  Experience with Google AutoML Vision\\nI’m getting pretty decent results considering how naive my models are   I’ve found Google AutoML easy in many ways   To get a basic model all I need to do is\\n    upload the RGB images to Google Cloud Storage\\n    upload a CSV file with a pointer to the GCS bucket and the target label\\nThere are a bunch of limitations with the products   My biggest issues so far have been\\n   inability to add other metadata e  g   would like to be able to add metadata on controls plate id and position on plate   I can probably get around this using a postprocessing step but it’d be nice to be able to add this metadata into the single model  \\n   there isn’t a feature that allows batch prediction on a large number of images   I work around this by hitting the prediction API  times to generate my submission file   \\n   the model evaluation page is not very helpful for debugging model performance   \\n   I had to use a lot of hours of training to get good results you can see that the models keep improving with additional training time   This means I have to wait a long time and spend a lot of money  \\n   can’t change the loss function   Based on the forums it seems like the loss function might be an important setting for this competition   \\nThere are some more minor frustrations that I had to workaround which I’m happy to share if others are planning to try it out and want to learn from some of the frictions I encountered   lkhphuc unlikely you need anywhere near as many training hours if youre not using AutoML   The upside of AutoML is that you dont have to set any hyperparameters and still get a decent model   One of the drawbacks is its so compute intensive    xhlulu I actually regret converting to png   It would have been a purer comparison if Id also used jpeg   \\nI tried running the px conversion in a kernel but I didnt have quite enough compute time   Let me find out how I can share the px dataset with everyone    xhlulu unfortunately cant share the dataset through the datasets platform   Thatd allow users to access the dataset without having to accept the competitions rules    apap Im picking the site that has the highest confidence score across the two sites   For my best model LB    my predictions for site  and site  agree  of the time    Not sure   Sorry    zaharch sorry for the delay   I wanted to see if AutoML could be run out of Kernels   Turns out it can\\n  \\nThe  hour limit for kernels is not an issue because my kernel kicks off a  hour training job and then stops   \\nMy next step is to share my inference code which sends the test images to the trained model and generates predicted labels   \\nAutoML doesnt give architecture or hyper parameter recommendations   It just creates an end point that you can send images to and get back predicted labels and the models confidence in that label     ttylacm dont know what hardware Google AutoML is using under the hood   Its invisible to the user   Good chance its using TPUs though     ratthachat thanks Looks like youre a few places ahead of me on the leaderboard   Watch out I have a few new ideas to try 😏  Oops   I made a change that introduced an error in version    Version  works   Im currently committing a version  which should work as well     OK now also added the code for doing inference using Google AutoML\\n  \\nI didnt do my strongest model to avoid messing with the leaderboard I picked a model that performs a little below to strongest performing kernel   To change models all one needs to do is change the modelid      ankitkp giuliasavorgnan and jiangkun improving reliability of kernels is a big push for the team   I shared this thread   \\nOne question that came up are you all using GPUs Or just CPUs jiangkun I dont follow   What does that mean Managed to jump to    by exploiting the structure of the structure of the data mentioned here    zaharch Im really interested in seeing this as well   I am curious to try TPUs sometime and would be interested in summary of your experience   Also curious if you were able to realize a performance speedup when using TPUs    Nice Thanks for sharing    Really love this writeup   Particularly how you step through the different things you tried and how they mapped to improvements in your score   Very easy to follow    deep there is a mistake in the dataset   San Francisco is listed as  square miles   When its actually  square miles   \\nAlso itd be helpful to have a description of the data including where it came from   And to have the Area column as an integer or float rather than a strong    I uploaded a dataset of doctors and nurses per capita for  countries from the OECD    Im going to add a pointer to the recovered cases dataset in the pinned dataset thread      cpmpml pointed out that having the number of recovered cases could be helpful   Just pointing out thats available in this dataset already shared by sudalairajkumar    Nice I would havent thought of this dataset    In case its helpful to compare with past pandemics\\nSARS dataset\\n  \\nEbola dataset\\n  \\nMERS dataset\\n  \\nht sudalairajkumar I found these datasets from one of his Tweets   I suspect its pretty easy to get on a countrybycountry basis   Question is whether theres a good global source   \\nBTW trying to keep this thread relatively clear for actually datasets   Were using this thread to ask questions and discuss ideas for datasets   Nice idea but a better fit for discussion in this thread    sasrdw this dataset seems to have a decent amount of what youre looking for\\n  \\n\\nThat version is three years old so may be worth updating   Although at a glance many of the measures are updated pretty infrequently    Pretty sure this is the weather data many of us have been looking for to look at the impact of temperature and humidity on transmission rate\\n    This looks like a nice version of that dataset\\n  \\nAdded it to the data sharing thread\\n   davidbnn posted a dataset of global weather conditions at\\n   I wonder if Google Trends data might be interesting Perhaps searches for hand washing or hand sanitizer in a particular city might correspond with a lower transmission rate   Or even just searches for COVID in a particular city might correspond with a lower transmission rate indicate people are taking the virus more seriously in that city   hannesmarais Im not an epidemiologist but at a glance this looks really impressive   \\nI had a quick look and saw a few places where your system seems to be answering the prompt  \\nThis seems to give an answer to the persistence on surface question\\n\\nThis seems to give an answer to the immune response question\\n\\nAre there many places where you think your system is accurately answering the prompt Are you in a position to give a bit more background on what your system is doing savannareid I would also be really interested   At a glance and to my untrained eye hannesmarais system looks really impressive   jaimeblasco uploaded one for school closures    I assume country level data is still likely useful\\nIf you can match the files in that dataset I can likely update it on their behalf   Otherwise I suggest you just create a new version    Nice Mind also sharing it in this thread Nice to have all public datasets in one thread   As I understand it this has been an influential preprint showing the relationship between temperature and humidity and transmission rate   \\nIt looks at  Chinese cities and finds that a  degree celsius increase in temperature leads to a    drop in R   And a  increase in relative humidity leads to a    drop in R  \\nAs I understand it R is a a commonly used measure by epidemiologists known as the effective reproductive number and is the average number of secondary cases per infectious case   \\nIf R is greater than  then an epidemic will spread   If its less than  it will die out   My understanding is that the estimate of R for COVID is in the range of      cpmpml nice dataset Do you mind adding a pointer here\\n  \\nWant to have all external datasets on the same thread   The Economist did an analysis of tourism flows tofrom China a few weeks ago   Could look at the dataset they used as a starting point   koryto  this is a great direction Makes a lot of sense to start joining a lot of these datasets into a table    absolutely davidbnn this is great Will save a lot of users a lot of pain and makes it easier to explore the impact of weather   Nice work nightranger nice Starting to join datacreate feature matrices is a really nice way to make the datasets surfaced in this thread more useful    koryto one direction you can take is trying to combine as many of the datasets mentioned here into your feature matrix as you can make fit    Nice Glad to see that larger and larger feature matrices being built up    Agree completely with Pauls guidance   \\nBig goal is to product findings easy for the medical and health policy communities to digest    This is a nice notebook   Itd be helpful if you also linked to the paper as well as providing the excerpt    this is very useful   Can you use your approach to look at words like weather temperature and humidity That is shown to have an impact on transmission rate e  g   referenced here  \\nItd be interesting to see what your approach turns up     ajrwhite this is very useful   I find key phrases most useful part of your notebook   Few requests\\n\\nCan you print more key than  key phrases when your search returns more results e  g   chronic respiratory diseases  \\ncan you print key phrases for additional  you look at risk factors e  g   hypertension   \\nCan you use your approach to look at words like weather temperature and humidity That is shown to have an impact on transmission rate e  g   referenced here   Itd be interesting to see what your approach turns up    \\n Getting estimates for R and R from the literature would be great   One problem with those snippets is it doesnt have the actual numbers    We have been asked by our health policy collaborators to put together a summary page that surfaces the most useful findings from the Kaggle community   The research on COVID is moving quickly making it hard for virologists and the health policy community to stay on top of the latest   They are working long days and need information presented in as clear and concise a format as possible   We’ve formatted the summary page to be digestible to them   \\nThe page is currently organized into three sections findings tools and datasets   Im going to focus on the findings and tools sections since theyre most relevant to this challenge   \\nFindings map to the ten tasks issued by the White House Office Of Technology Policy addressing open questions about COVID   Our goal is to help answer as many open questions as possible and represent the state of knowledge on each of those questions   \\nFindings should be focused concise extract quotes and numbers out of papers and also provide a link to the underlying source   It’s helpful if you can produce findings that map as close as possible to the format of whats currently presented on the summary page   Some of the most impactful work so far have involved simple methods like string matching and regular expressions   \\nFor those working on tools Google Scholar and AI Semantic Search are already mature products   If you are building a tool make sure your tool adds value beyond those products   \\nIf you have contributions that you think we should be highlighting on the summary page please email me at akaggle  com or share on this thread  \\nAlso feel free to ask any clarifying questions on this thread     Most of the datasets have come from the forecasting challenges   However for those interested Ill provide some guidance on what datasets are helpful  \\nIts useful to share individual datasets with promising signals   Its even more useful if you can join to other relevant datasets that virologists and the health policy community can analyze   And by all means use any datasets to try and produce findings   If you do make useful findings please document them in a clearly written notebook thats easy for others to follow    We have been asked by our health policy collaborators to put together a summary page that surfaces the most useful findings from the Kaggle community   Virologists and the health policy community are working long days and need information in as clear and concise a format as possible   We’ve formatted this page to be as easily digestible to them as possible  \\nThe page is currently organized into three sections findings tools and datasets   Im going to focus on datasets and findings since they are most relevant to this challenge   \\nDatasets So far the forecasting challenge has done a nice job of surfacing potentially useful datasets   The most valuable contributions have been joining those datasets to create valuable resources for testing which factors impact transmission   Shout out to davidbnn who joined each region in the Johns Hopkins University data to the nearest weather station   \\nFindings These  clearly written notebooks that help show which factors have an impact on transmission rate   \\nCalls to action\\n   create enriched datasets that would allow researchers to easily test which factors impact transmission   These datasets should be well documented and kept updated   \\n   write simple notebooks that show the impact of factors on transmission useful whether or not you find a correlation\\nIf you have contributions that you think we should be highlighting on the summary page please email me at akaggle  com    somethingkag this is not a template but ajrwhites  notebook does a really nice job addressing a handful of the subtasks   \\nIt directly addresses the subtasks   The informationdata is very clearly presented    skylord nice   Are you updating this daily If so Ill include it    Franck you should absolutely be open to looking at external data sources to answer the questions   One nice thing about starting with the papers is we build up a picture for what has already been studied and where the gaps are    No   This is not a supervised machine learning competition   We are hosting a supervised COVID machine learning competition at\\n   ajrwhite  really nicely said  Ken added to summary page    Thanks nofoosports   At a glance your notebook looks really nice   Likely have some folk with medicalpublic health backgrounds starting to help out with curation from tomorrow so planning to take a close look with them   Ken thanks for grouping the notebooks by task   Makes them much easier for us to process    Thanks Mike   Will take a look today    Just added a challenge for sharing useful COVID related datasets  \\nMotivation for adding that challenge is that a lot of datasets shared in this thread a are really useful b potentially less relevant for the forecasting challenge   We wanted to create a specific outlet for all COVID related datasets     remanuele please add as a Kaggle dataset   That makes it easily accessible to those writing notebooks    For those interest Safegraph have a really interesting human movement dataset   Its location data from millions of anonymized smart phones   Its currently on AWS but they can move it to GCP to make it easier to use from a Kaggle notebook if theres sufficient interest  \\nYou can get access by filling out this form   I received quite a bit of feedback from the healthcare and health policy communities about our page summarizing community contributions  \\nAs well as including the title of the paper itd be helpful if we also shared the name of the journal in the summary tables   The name of the journal is a proxy for the the quality of the paper a result published in JAMA or The Lancet carries more weight  \\nMore challenging is to also classify the type of evidence that a study is based on   Theres a hierarchy of evidence and its helpful to know what evidence was used to draw a particular conclusion   Here are a few references that explain the hierarchy of evidence\\n     \\n        ajrwhite can you update this notebook to include the new data dump\\nAlso is it easy for you to add the journal name to the article title column per this guidance If so itd be great if you could put include it   \\nIf you can Id format itd be great if you could put it outside the link to make it easier to distinguish from the title\\nPersistence of coronaviruses on inanimate surfaces and their inactivation with biocidal agents The Journal of Hospital Infection mikehoney a really nice tool for exploring the data but doesnt directly give me the answers to specific tasks without me choosing my own filters    mlconsult the Chloroquine section had mostly relevant articles so added it to the summary page under Effectiveness of drugs being developed and tried to treat COVID patients   mlconsult really appreciate you doing this It makes our job of curating much easier    hannesmarais I have been meaning to go back through your system and see how its been performing since you made improvements   Are there questions that it does particularly well on If so which ones\\nAlso can you explain the difference between the question sets original Kaggle vs extended Kaggle from Savanna Reid skylord added here  \\nBTW you may want to crosspost to this challenge Wow   This is impressive   Itd be great if you loaded it as a Kaggle datasets Just noticed paultimothymooney already has it on Kaggle   This seems to be an important paper   Is it in the CORD dataset zohrarezgui nice idea to pull down clinical trials data   But the dataset has no information on the therapy being trialed though   Is it possible to pull more information about the nature of trial mlconsult nice added   \\nCan you pull how large the difference is between  and \\nAlso what does relscore mean mlconsult couldnt find a task that this addresses but included on the contributions page because its come up as an open question recently    nofoosports this is great Ive started adding your answers to the contributions page  \\nA couple of things that would be helpful\\n\\nstart ordering by date I like to present results chronologically on the contributions page\\nalso helpful if you can start breaking out answers to subtasks a little more\\n\\nFor example for this subtask theres\\nRange of incubation periods for the disease in humans and how this varies across age and health status and how long individuals are contagious even after recovery  \\nitd be helpful to have papers and excerpts that just answer \\n Range of incubation periods for the disease in humans \\n how this varies across age and health status\\n how long individuals are contagious\\nYoull notice Im starting to break up even the subtasks on the contributions page to make them more digestible   This is feedback were consistently getting our audience is very busy and want things as scannable as possible   Another request its even helpful if you can break out subtasks into several components   As an example take this subtask Range of incubation periods for the disease in humans and how this varies across age and health status and how long individuals are contagious even after recovery  \\nitd be helpful to have papers and excerpts that just answer\\n Range of incubation periods for the disease in humans\\n how this varies across age and health status\\n how long individuals are contagious\\nIm starting to break up even the subtasks on the contributions page to make them more digestible   The feedback were consistently getting is our audience is very busy and want things as scannable as possible  \\nI just shared this feedback on nofoosportss excellent notebook but wanted to make it more broadly visible   Also a nit but you have grouped two subtasks accidentally\\nPrevalence of asymptomatic shedding and transmission e  g   particularly children and Seasonality of transmission   csheesun were typically finding results easier to read when contributors are submitting one notebook per task    We had another call today and received another round of feedback   First off were getting good feedback that    is heading in a really helpful direction   \\nThe big issue is still that theres limited signal on how strong the underlying paper is   Since COVID is so new and most articles are in preprint journals medRxiv and bioRxiv that doesnt give much signal I think this is consistent with what savannareid has been saying   \\nThey are interested in whether we can mine other signals from the dataset   I think were going to get more feedback in the next week Ill provide it when I get it but an example that came up was pulling out the number of participants in a study the more participants the more weight should be given to a study for eg   \\nThis didnt come up on the call but I was also wondering about authors previous publication record might be a useful signal    \\nWould love to add a column thats a proxy for publication quality by the end of the week if anyone can crack the code on that   \\ndavidmezzetti started a discussion thread on the topic so suggest the conversation continue there    Another bit of feedback we received is that the health policy and medical communities are interested in tools that might help them get to answers more quickly themselves   The dream tool is a questionanswering engine that can reliably answer freeform questions   But theres interest in seeing other tools as well that might go beyond what things like Google Scholar and other existing search engines can do   If anyone builds a tool that they think meets this criteria please either share it here or email me     Wow amazing you got this done so quickly Ultimately our goal is to be as useful as possible to health policy decision makers   So you are free to search Pubmed if you think itll improve your results    davidmezzetti would it be easy to add the hFactor of the last author As I understand it the lastauthor is the corresponding author and the credibility of the study rests on their shoulders   That could be another helpful proxy    davidmezzetti ignore the hfactor suggestion   Ive just run it past a few people and the feedback were getting is that its probably not such a useful metric   \\nWere getting feedback that some measure of sample size would be really valuable in conjunction with the level of evidence metric   The kind of guidance we were given was \\n For level I either the number of studies or the total number of participants across all studies\\n For level II the number of participants in the randomized control trial etc   \\nItd be great if you could take a swing at this    amazing youve made such quick progress   Look forward to tomorrows update    A recent paper suggests a potential link between transmission and air pollution\\n  \\nPaper suggests that explains regional difference is Italy between transmission rate    mlconsult thanks for sharing   paultimothymooney owns looking at tools being developed   Hes going to take a look at this  \\nIf he thinks its promising itll definitely need to be stood up as a web app   Feedback were getting is many of the end users are not technical enough to fork a notebook    Nice work mlconsult   paultimothymooney as an FYI this is now a web app    One of the main bits of feedback were getting about the results were presenting is that decision makers would like to better understand the strength of the evidence in a study   davidmezzetti has made a really nice start by including  level of evidence   \\nI spoke with Stelios Serghiou and Byron Wallace   Byron actually uses NLP to assess the quality of clinical trials   \\nIn speaking to Stelios and a few others Im getting the feedback that pulling out the underlying study design might be even more helpful than level of evidence   For each study type there are measurements that can help proxy the strength of the evidence   Below is a table that attempts to map study type to measurements   These are the things that Stelios looks for when hes assessing the quality of a paper   \\nItd be great if you can attempt to extract study type and the corresponding measurements to indicate evidence strength   This applies to those presenting task answers in tables as well as those who are building searchquestion answering tools    \\n Level of Evidence  Study Type                                   Measurements to indicate evidence strength                                                                                                                                                                            \\n      \\n I                  Meta analysis        I heterogeneity and tau squared heterogeneity     number of studies   \\n II                 Experimental Studies Randomized                Randomization      Sample size      Loss of follow up      Length of follow up\\n III              Experimental Studies NonRandomized                Randomization      Sample size      Loss of follow up      Length of follow up \\n III              Observational studies  Prospective Cohort       Sample size     Loss of follow up  \\n III              Observational studies Retrospective Cohort      Sample size     Sampling method how was the sample captured\\n III              Observational studies  Case Control             Sample size     Selection bias for controls when controls come from a different population from cases \\n                    Observational Crosssectional                   Sample size     Sampling method how was the sample captured     Temporality did the exposure e  g   COVID come before the outcome e  g   hospitalization desired or after undesired \\n IV                 Observational Case series  case study      \\n                                                                                                                                                                                                      Note savannareid pointed out that this doesnt include modeldriven results like those used to estimate things like reproduction rate   For modeldriven results measurements of evidence strength are likely simple size some measure of model fit and some information on where the underlying data was drawn from    aravindmc nice work Weve listed it on the contributions page  \\nAs Paul said were getting a lot of feedback about including measures of the strength of evidence   Shared some of the feedback we are hearing here   Itd be great if you could extract and include some of that metadata in your search results    arturkiulian absolutely   The overall objective is to product something useful to the healthcare and health policy communities   If those things help you produce more relevant results then please do use them    ajrwhite and davidmezzetti and others looking to contribute the the contributions page savannareid hand coded the design and sample size for all the risk factors you can see\\xa0what it looks like here   She also reformatted the tables   \\nCan you try and pull update your risk factors notebooks for the new data dump And can you try and pull out design and sample size\\nAlso if you can follow this table format we can dump it directly onto the page we now have a more automated process for generating the tables\\n\\nDateStudyStudy LinkJournalSevereFatalityDesignSample\\n Hi all   We now have a team of medical students helping to curate the results that go on the contributions page   Theyve been reviewing the results of notebooks that focus on the  transmission incubation and environmental stability task as a starting point   Each student owns curating a table for each of the  subtasks   \\nThe goal for these reviews is to\\n   to come up with a standard format for those subtasks just like savannareid has done for risk factors   This will give you a target format to produce your results in   \\n   give feedback to authors of notebooks that are reviewed on how to make your algorithms more relevantaccurateuseful\\nThe goal is to continue to populate the contributions page which is gaining a nice following attractive   K unique visitors in the last week up  from the week before   And when we have a meaningful amount of coverage and a solid process for keeping the page uptodate as the new papers come in with less manual curation than we are currently doing were going to aim to start publishing the updated literature review with a leading medical journal so that its more visible in the medical community\\nEncourage those of you who are interested in contributing notebooks in the format helpful for the contributions page to join the Slack channel with the team of medical students   Here is an invite link   Im going to attempt to pass on feedback Im hearing in this thread   But joining that slack channel gives you more direct and interactive way to hear from the experts who are reviewing your work   \\nFinally if you want to see the curation process in action and the notebooks that are being reviewed you can check out this Google Doc   Please let me know if you have a notebook you want reviewed for a particular subtask and I can add it to the queue the queue is maintained on the MASTER SHEET tab    Some consistent themes in the feedback so far\\n   often algorithms are pulling snippets from the abstract   When the key number finding or result is in the full text of the article with the summary in the abstract being too high level to be useful   \\n   often algorithms are pulling snippets that are a reference to work from another paper rather than work that is core to the paper being cited This challenge is getting the attention of the health policy and medical communities who are working night and day to better understand COVID   In February  COVID papers were published per day in March that increased to  by mid April that number is up to    It is very difficult to stay up with the literature   \\nWeve been taking some of the most promising notebooks and working with a large team of volunteers epidemiologists MDs and medical students to turn those results into a regularly updating literature review   You can see the work in progress here   \\nNow that we have a clearer understanding of what this needs to look like were calling on our community to more directly feed this effort by outputting results in a standard format   All notebooks should output CSV files in the formats listed in the task descriptions   \\nHow you can contribute\\nRead the instructions within each individual task and make a submission\\nWe also encourage participants that have made submissions and want feedback to join this Slack channel   It includes the domain experts who are curating the tables    This challenge is getting the attention of the health policy and medical communities who are working night and day to better understand COVID   We have heard feedback that an AIpowered literature review is a powerful way to synthesize new research   I have started a new thread outlining how you can feed into an AIpowered literature review   \\nThat thread aims to be a clear and direct way you can make contributions that will have impact   I am unpinning this thread to direct conversation there     mlconsult looks like youve done a really nice job of pulling out sample column   Nice one \\nOther than that column Im not seeing a close mapping to the table format mentioned above   Am I missing something  Oops my bad   Read too quickly  dirktheeng weve been chatting with kylelo and team about the importance of having tables   As paultimothymooney mentions theyre working on approach to making the tables machine readable   \\nDo you think you could reliably take images of tables and turn them into Pandas Dataframes Defer to kylelo but that may be a way to get you tables sooner   \\nAnthony Nice update   One change that would be helpful is if you can break out Study Study Link and Journal into three separate columns per this guidance   That will result in less manual curation to get the results into the format for the contributions page   that was quick Updated the target CSVs with additional tables from the Transmission Incubation and Environmental Stability task    sapal I just tried it from Incognito and it worked for me   And others have been getting in   Perhaps you hit it during a temporary outage Suggest trying again    sasrdw thanks for starting this thread    david any sense for how the IMHE model would have performed in these competitions Curious where would it have placed    Made another update to the target CSVs\\n changes to risk factors removed TB changed BMI to overweight added an extra two columns to break out the results column and added extra data\\n changed to TIE cleaned up the seasonality tables added extra data fixed issues found via error checking Wow   This is really nice   Interesting that the Kaggle community keeps getting stronger vs IHME   I guess there are three possible explanations\\n   the Kaggle models are getting better as more data comes in more empirical and less theoretical than IHME\\n   the Kaggle models are getting better because people have more time to work on them\\n   IHME are better at forecasting further out\\nSounds like you believe its  david Also really curious about week  is the trend continuing or are we plateauing vs IHME   Assume you left it off because we dont have enough data for week  yet    david funnily enough I was just looking at his dashboard\\n  \\nI assume theyre all outputting data in a standard format if hes able to put them on the same dashboard   Is it easy for you to benchmark the performance of those other models as well \\nWere trying to figure out whether the top Kaggle models are likely adding value beyond the standard epidemiological models   Were considering launching a longer running competition and combining top performing Kaggle models into an IHMEstyle dashboard     For interest section    of this paper discusses the metrics used by the epidemiological modeling community    A nudge we still have tables that need to be claimed   If you think its something you can help with please see the relevant list of tasks   I did some more benchmarking  \\nI compared the current leader for week   and  with IHME and LANL models across three loss functions MAE RMSE and RMSLE   Im only comparing the forecasts for fatalities for US states  \\nIn week  and  the Kaggle leader clearly wins on RMSLE   But performs a lot less well on the other loss functions   \\n cpmpml regarding your comment about the Kaggle comp optimizing different things the same is probably true for IHME and LANL   \\nOn MAE and RMSE it looks like you basically get good scores by getting New York and maybe New Jersey correct they account for  of fatalities\\n\\nAnd getting New York right involves predicting a data revision   NYC revised their fatalities measure to include probable COVID deaths on April    This revision started included people who died but never received a COVID test but had COVIDlike symptoms  \\nThis chart comes from inversions excellent notebook that shows charts for LANL IHME and two Kaggle ensembles on a state by state basis  \\n cpmpml the basis for the claim that getting New York right is most important comes from the observation that New York accounts for  of the total absolute error for the kagglelead and IHME models so that state is driving most of the error   \\nI havent had a chance to take a close look at whats happening with your model   Your numbers are really impressive across the board       unfortunately I have to return to my day job now though this was a small weekend project that I did during my daughters nap   \\ndavid I bet if you asked IHME and LANL if the benchmarking is fair theyd also have complaints  as well optimized for a different loss function etc   Nonetheless I was curious to see what would happen if I benchmarked across a range of metrics that the models didnt necessarily optimize for   I was hoping it might tell me something about the robustness of results  \\nTo explain why this setup LANL only does forecasting for US states so needed to restrict to this subset to include them   And LANL and IHME model updates dont perfectly correspond with our competition dates    made another update to CSVs   Covers the new data and adds a bunch of new columns Nice presentation   Thanks for sharing    Ive put together got a notebook that benchmarks the performance of professional epidemiological models and compares them with a strong Kaggle model  \\nI was curious to keep track of how the Kaggle models compare with the professional  epidemiological models   For now Im only tracking one Kaggle model the previous weeks leader to avoid making a selection expost   So the week  model Im using is one of sasrdws and davids models   Its currently doing really well   Although interestingly its making very different forecasts from the professional epidemiological models  \\nI have created a benchmark panel that includes actuals and the forecasts from different models at different points in time  \\nHeres the notebook that generates the benchmark panel   \\nEncourage others interested to play with it either add one of your models or play with some Kaggle ensembles can download submission files from public notebooks   mrisdal you are dominating enough leaderboards   I need to pick up my game       Maintaining this thread to describe known issues   If you post an issue its helpful if you can link to a notebook that is prefixed the the title ISSUE The CONVENIENT files have negative daily case and death counts in some places   The RAW files are cumulative case and death counts   The CONVENIENT files the diff of the RAW files   \\nHeres a link the notebook demonstrating the issue with some of my early attempts to understand it    Please add any requests you have for this dataset in this thread   If any requests get a number of upvotes indicating some amount of broader interest I will attempt to address the request    Add data in the JHU daily report files for US counties and the global file  \\nFor global file this includes IncidenceRate and CaseFatalityRatio  \\nFor US county file it includes IncidentRate PeopleTested PeopleHospitalized MortalityRate TestingRate and HospitalizationRate Include population by country in the global metadata file    Remove provincestate from the country level data to make it more convenient to process    Yes   And also means you can schedule a notebook to run on a recurring basis    xhlulu I used GCPs Cloud Scheduler combined with the Kaggle API   \\nBut a few people have asked about scheduling functionality so we are considering adding it as a feature   If this is interesting to you itd be great if you could contribute to the discussion thread that mrisdal started earlier today     fvcoppen I want to add additional data sources over time   So if you find good data sources you think I should add please let me know    sagnik  have you seen this post I describe the issue there   Let me know if you have suggestions for resolving it    maithiltandel it looks like a nice dataset   In the dataset description itd be helpful if you explained more about the dataset itself rather than the findings   For example where the dataset comes from a link to the raw source perhaps a link to the notebook you used to transform it    Looks like a great dataset can you add documentation Would love to know more about the dataset where its from what it can be used for etc    sansuthi I Tweeted the dataset   Do you have a Twitter account you want me to link to in a thread on that Tweet  mohit fyi think link you posted is broken   I think this is the link you meant to post works if you copy the text version clicking the link URL takes you to    Have an issues with the dataset that you want resolved Requests for additional metrics that should be added Please add your suggestions to this thread    kaviml thanks for the lovely post   Its really motivating for us to receive comments like this   \\nsurajjha in replying Im trying to compete with herbison to be the best among the staffs 😂   Jokes aside thank you also for your nice words    To the Kaggle community\\nToday were sharing that D   Sculley will be taking over as Kaggle’s new leader alongside other ecosystemfocused machine learning efforts   Ben and I are leaving Kaggle and Google for our next adventure going back to our startup roots   \\nKaggle recently passed its  year anniversary   We started with a lighthearted competition aimed at predicting the voting matrix for the Eurovision Song Contest in    Back then it would have been hard to imagine that Kaggle would play a meaningful role in the future of machine learning and AI   \\nThere are several aspects of what Kaggle has achieved that we are really proud of   First and most importantly the impact on peoples’ lives   Many have learned machine learning through Kaggle   Of our almost MM users   MM have submitted to the Titanic getting started competition and almost MM have completed exercises from Kaggle’s courses     K college courses have run classroom competitions with K students submitting to those competitions   \\nAnd it’s not just those new to machine learning who use Kaggle to learn   Advanced users get handson experience on lots of different problems and the opportunity to learn from the winning solution   As Grandmaster Vladimir Iglovikov likes to say “I think about machine learning competitions as about a gym but for a machine learning muscles”   \\nKaggle has also provided a credential to the machine learning world   We have given those who are sufficiently determined another way to break into the field   Already back in  Facebook was using Kaggle to find strong machine learning talent   By  we were well established as a great way to land an elite AI job   Today wellregarded AI companies like NVidia and H  ai hire teams of Kaggle grandmasters    \\nWe’re also proud of the role the Kaggle community has played in highlighting what works well in practice    machine learning papers per day are published on Arxiv and countless machine learning packages are developed   Kaggle users explore these techniques in a competitive environment and spread those that work   Frameworks like Keras and XGBoost took off in the Kaggle community along with useful preprocessing and data augmentation libraries like albumentations for computer vision   Many techniques have spread through Kaggle including UNets for segmentation denoising autoencoders and adversarial validation   And Kaggle has helped prove out new applications for machine learning including medical imaging and automated essay grading \\nAnd finally we’re proud of the fact that while we started with machine learning competitions we’ve launched other services   Notebooks have increased the ways our users can share learnings and our courses have made Kaggle more approachable to new users   And there’s no machine learning without data and we’re proud of our collection of over K public datasets which makes Kaggle one of the world’s largest repositories of public datasets   \\nOf course none of this would have been possible without both the community and the Kaggle team past and present   Over the years we have had the privilege of meeting so many passionate and energetic community members with so many inspiring stories   And the opportunity to work with a talented and motivated team   We’re grateful to all of you    \\nAnd while we are proud of what Kaggle has achieved so far we’re also very excited to see what the D   and the team accomplish in the years ahead   D   has been operating on the cutting edge of machine learning for the past  years working on very large scale machine learning systems inside Google doing foundational research on topics like MLOps and leading large research teams   D   also has a long history with Kaggle starting with the semisupervised machine learning challenge he launched in  which as it happens was won by some familiar faces   We’re excited what the combination of D  ’s background and history with Kaggle will bring to the future   \\nFor those who want to get to know D   better he is going to be answering the most upvoted questions over on this post   \\nAnthony and Ben'"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "forum_data_agg['clean_messages'][1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "#function to split string by uppercase\n",
    "def split_uppercase(text):\n",
    "    text_list = text.split()\n",
    "    new_list = []\n",
    "    for i in text_list:\n",
    "        if i.isupper() == False: #don't split acronyms\n",
    "            word = re.sub(r'([A-Z])', r' \\1', i)\n",
    "            new_list.append(word)\n",
    "        else:\n",
    "            word = i\n",
    "            new_list.append(word)\n",
    "    words = ' '.join(new_list)\n",
    "    return words\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "forum_data_agg['clean_messages'] = forum_data_agg['clean_messages'].apply(split_uppercase)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' Hi  Tanya  Kaggle will maintain a rating system  If you win but youre ineligible for prize money you will still get a strong rating  Anthony  Here are some papers that analyze  Eurovision voting patterns  You might find some of them helpful  Gatherer  Comparison of  Eurovision  Song  Contest  Simulation with  Actual  Results  Reveals  Shifting  Patterns of  Collusive  Voting  Alliances  Eurovision  Song  Contest  Is  Voting  Political or  Cultural  Ginburgh and  Noury  Suleman  Efstathiou and  Johnson  Eurovision  Song  Contest as a ‘ Friendship’  Network  Dekker  More research enjoy Love thy  Neighbor  Love thy  Kin  Voting  Biases in the  Eurovision  Song  Contest culture and religion  Explaining the bias in  Eurovision song contest voting  Hybrid  System  Approach to  Determine the  Ranking of a  Debutant  Country in  Eurovision  Eurovision  Judgment  Versus  Public  Opinion –  Evidence from the  Eurovision  Song  Contest  Giovanni Thanks for your feedback  Using the forum to give feedback is a good idea  It allows others to see and comment on suggestions  We might set up a proper feedback forum but for the moment this topic will have to suffice I also agree that the forum is a bit clunky  However we have a large list of feature requests and only limited resources for the moment it might take us some time to address this  Apologies I dont think the prize money in this competition is that relevant the prize is relatively small  Correct me if  Im wrong but I think contestants are driven by intrinsic factors A karma system that rewards forum posts is a good idea  Again apologies for any delay in implementing this there are lots of features on our to do list  Anthony  Manish thanks for the feedback  The site is hosted on an  Amazon EC server on the east coast of  America  Its a fast server but the site has been more popular than we expected  Were currently working on speeding up the site by reducing the number database queries  We may have to implement auto scaling if the site keeps growing so rapidly  Anthony  Just made a change which should speed things up  Let me know if it has made a difference for you  Jonathan thanks for your feedback x  Were currently working on caching database queries  There are a lot of good suggestions here that well try before autoscaling  Please use this topic to give us feedback  If youd rather do so in private email me at anthony goldbloomkaggle com  Use this topic to discuss any competitions you would like us to run  If you would rather contact me privately email anthony goldbloomkaggle com I accidentally deleted the following post made by another user  Im reposting it on their behalf Are there categorical andor binary variables in the data set  Other than the target variable  For instance  Variable Open in test data seems to have categories  If there are categorical variables do we get to know what the categories mean Thank  You  Sali  Mali has pointed out that there is an error in the AUC calculation for entries with tied scores that is when two or more scores have precisely the same value  We will look at the problem over the next hours and will rescore all entries  Apologies Anthony  The AUC calculation glitch has been fixed and all entries have been rescored  Sali  Mali thanks again for pointing this out  Thanks for the feedback  What sort of features do you have in mind  Or can you point to a forum that you we should emulate I just added a quick reply box to make the forum less clunky  Great suggestion I have put this on our extensive features to add list  Colin the choice of scoring system was quite deliberate  Will the competition host considered using  Area  Under the ROC  Curve where participants submit probabilities but said that he deals with physicians who just want to know the proportion of predictions that are correct  Rajstennaj and  Colin were really pleased that you believe that theres value to the project  Let me know if there is anyway that we can help to facilitate a community  We did set up the general  Kaggle forum under  Community Forum with such a community in mind does it provide sufficient infrastructure  You are free to start any new threads on that forum  Regards Anthony out of thats pretty impressive  Pity they didnt enter the  Kaggle comp I think youre right some competitions exhibit more regularities than others  Soccer may be a difficult sport to model  Hi PG  Not sure that I fully understand the question  Are you referring to the situation where a classifier returns only or rather than a score or probability  Perhaps you can use an example to illustrate the question  Regards  Anthony  Hi PG  You should give the score for all timestamps a higher score means the instance is more likely to be a member of the positive AUC measures your classifiers ability to split the classes so you dont need to decide which scores predict positive instances and which predict negative instances  Have I addressed your concern  The public leaderboard is only indicative because competitors can use information on their score to get information on a portion of the test dataset  The final results are a quite different and b better reflect actual performance  Thanks for participating in this competition  Ive attached the solution file to this post UPDATE  The solution is no longer attached but youre welcome to make submissions to this competition  Hi  Matt Thanks for the nice words and the suggestion  Ive posted the solution file  Hi  Dirk The  Elo  Benchmark is based on the training dataset only  Having had many email conversations with  Jeff I can tell you that the seed ratings matter a lot  Youll notice that  Jeff made two submissions for the  Elo benchmark thats because hes refining his seeding method I believe he plans to make a few more refinements  Jeff uses an iterative process to seed the rating system  For example he might start by giving everybody and then letting  Elo run for months  He then seeds  Elo with the month ratings and runs  Elo again  He does several iterations of this  Does this help Anthony  Just to clarify the results page will show the leaderboard for all competitors regardless of whether they used future information or not  We will make an honourable mention to the leading competitor who doesnt use future information however their entry will be audited  Kaggle is currently developing a league table that ranks competitors  When it comes to this competition your position on the leaderboard which is indifferent to the use of future information will be what counts towards your  Kaggle ranking  Hi  John Have just confirmed with  Jeff methodologies will be shared publicly  Regards Anthony  The competition has been designed to make cheating really difficult  At the end of the competition the winners methodologies will be replicated to help ensure everything is above board  Hi  Matt The reason we prevent participants from submitting an unlimited number of times is because otherwisea our servers may not be able to handle all the traffic anda it would be easier to decode the portion of the test dataset thats used to calculate the public leaderboard  The technique you describe often referred to as cross validation is very sensible and we encourage others to use it  Anthony  Good suggestion  Were open to ideas on how we can facilitate this  My thinking is the best thing to do is to implement a more functional forum which were doing  We can then encourage those who are still working on the problem to continue to use the competition forum as a way to collaborate  Hi  Dirk Weve updated the data description thanks for the pointer  The competition does require participants to forecast the next four observations  Weve updated the format of tourismdata csv so that there is always a value in the last row  Regards  Anthony INFORMS can offer an awardhonourable mention to those who dont use future data  However the  Kaggle leaderboard will not seperate those who use future information from those who dont  Hi  Greg Apologies there was a bug that cut off the last characters  The problem has been fixed but unfortunately the fix will only apply future submissions  Thanks for pointing this out and sorry for the inconvinience  Anthony  Hi  Matt I believe that  Will the competition host is preparing a blog post that discusses some of the methods that people applied to this competition based on the feedback we received  Is this the sort of thing you had in mind Anthony  Uri you raise an interesting point  However is five months long enough for somebodys rating to move enough for you to notice this  David this is a great suggestion  The HIV competition shows that  Kagglers can do great things  My initial concern with any public dataset is that people can look up the answers  We would need researchers to withhold a small portion of the dataset for evaluation I think the first step is to get in touch with those who set up the  Alzheimers project  It also makes sense to contact the  Michael J  Fox foundation  If anybody has any connection to either of these projects please let me know  Otherwise  Ill keep you posted on any progress  Anthony  Jase the score on the full dataset is calculated onthefly so we actually know who is winning based on the full test dataset  Ron the submission that is performing best on the public leaderboard may be different from the submission that is performing best on the full test dataset  We dont link the best submission on the public leaderboard to the best overall submission so that participants dont become confusedconcerned if their scoreposition on the public leaderboard worsens  Leigh my thinking as well  In a tradeoff between having a veracious public leaderboard and a veracious end result the end result is most important  Jeff good suggestion  Ive put together an  Excel sheet that might be helpful for cross validation  You paste your predictions for months into column G and it aggregates by player by month and then calculates the RMSE  Hope its helpful  Anthony  Ben The evaluation method was chosen because  Jeff has found that scoring based individual games with RMSE unduly favours systems that predict a draw  Mark  Glickman raised another issue RMSE is better suited to normally distributed rather than binary outcomes  So in order to use RMSE aggregation is preferable  Of course we could have evaluated on a game by game basis using a different metric  My biggest problem with the current evaluation method is that counting a draw as half a win seems a little arbitrary  However in order to benchmark  Elo such an assumption is necessary  Mark and  Jeff argue that a draw is generally worth half a win so this assumption isnt too problematic  Anyway hope this gives you some insight into our thinking  Regards Anthony  Jeff please correct me if  Im mistaken but I believe systems that predict draws are favoured because a high proportion of games are draws at the top level per cent in the training dataset  Of course you can do better but a system that predicts for every game will perform better than it should  Has anybody tried  Trueskill yet probably a better starting point than  Elo  This blog post does a nice job of stepping through  Trueskill  Matt am interested in your thinking on this  Why MAE over MSE or RMSE  Is it just that the metric is more intuitive or something subtler  Hi  David I have written to the  Alzheimers  Disease  Neuroimaging  Initiative ADNI and the  Michael J  Fox  Foundation I am scheduling meetings with both for  September  Will keep you posted on any progress  Can you put up a link to the datapaper you found Thanks again for the suggestion  Its great if we can use the power of this platform to tackle meaningful problems  Regards Anthony  Uri the correlation between the public leaderboard score and overall score is significantly higher now  Here is the solution file for anybody interested  Uri  Im reluctant to release confidence interval information because I want to minimize the advantage to early submitters  Early submitters already have the small advantage of having seen their submissions on two different public leaderboards  By releasing confidence interval information  Im giving early submitters access to information that isnt available to later entrants  Jase aside from changing the size of the public leaderboard portion of the test dataset we also selected it more sensibly so it better represents the overall test dataset JPL a competition using internet chess data is a good suggestion  For interest the reason we are running the competition using top players is because  Elo ratings matter most for top players since it is used to determine who can play in which tournaments  Out of interest has anybody entered this competition using  Glicko  Glicko or  Chessmetrics  Are either of you happy to send me your unmodified  Glicko submission  It would be good to add a  Glicko  Benchmark team to the leaderboard  My email address is anthony goldbloomkaggle com  Would like to do the same for  Glicko and  Chessmetrics if anybody has tried those I have also contacted  Ron about using his  Trueskill submission as a  Trueskill  Benchmark  Jase I posted a link to your  Glicko code on the hints page  Its very good of you to share it  Im really surprised that  Glicko is performing worse than the  Elo benchmark  Do you think this is because  Jeff put lots of work into optimally seeding the  Elo benchmark  Or is  Glicko just not as good  Vateesh thanks for sending the files  The files that you sent are actually different I also had a look at your submissions and you have a few files with the same name but different numbers  Also I was not able to replicate the problem as you describe it  Perhaps you can try again and let me know if youre still experiencing the error  Was just chatting to  Jeff  Time permitting he is going to benchmark some of these other systems  This way they will all be benchmarked on a consistent basis using the same seeding procedure and the same degree of tuning  Hi  Vess  This should not be a problem given the way submissions are stored  Hi  Edward  You will appear on the leaderboard as soon as you make your first submission  Hi  Edward  Try using examplesubmission csv available at and replacing the score column with your predicted scores  If youre still having trouble email the file to me anthony goldbloomkaggle com and  Ill have a look  Given the way the competition has been setup theres no way to prevent people from using future data  Even if the winner presents a model that doesnt include future data they may have overfitted to replicate the predictions of a model that does include future data  Uri thanks for pointing out the problem  Were currently working on a big upgrade to the website the new site should be launched by the end of this month  The upgrade will involve a more functional forum  In the meantime I will try and fix this problem  Anthony  Uri  Im not able to replicate the error either on the live site or on the development version  Can you let me know if you experience it again  Hi  Hans which post  Still cant replicate the bug intermittent problems are really annoying  As mentioned were doing a massive site upgrade at the moment so thats taking up the majority of our development time  How serious is the problem  Can we live with it for the next few weeks until we deploy  Kaggle  In theory yes  The problem is that theres no way to be certain that the winner didnt use future information  Even when we check the winning model its possible they have used a model with future information to probe the test dataset  Eric thanks for the feedback  Theres not really any reason to insist on a particular file extension  Were currently doing a big site upgrade so  Ill add this to our list of feature requests  Seyhan the leaderboard portion of the test dataset is selected randomly  It is somewhat representative of the overall standings I would really like to be more active in the forums looks like theres some lively discussion happening  Ive been flat out working on the site upgrade which is only a few weeks away from launch  Anyway  Id like to share a few thoughts on this discussion  First off there is quite a strong correlation between the public leaderboard and the overall standings  Secondly the lack of relationship between the scores and the scores might indicate overfitting  This may be the case if youre experiencing a larger improvement on the dataset than the dataset  On a related point I notice that youre all performing very well  It could be that youve reached a local maximum i e the best possible score given the techniques youre using  Just to reemphasis  Jeffs point you should pay more attention to your cross validation than to the leaderboard  The leaderboard is calculated on a very small amount of data so it is only indicative  Phillipp Sorry for the delay in doing this I havent had computer access over the last few days  The  Spearman correlation between public scores and overall scores is I also calculated the correlation for different submission quintiles to make sure the relationship holds at the top it does Top  Its also worth mentioning that the trouble participants are having reflects realworld difficulties in formulating a chess rating system  This competition is not just a game but a genuine attempt to explore new approaches to rating chess players  Anthony  Out of interest why arent people rerunning old approaches that had previously been scored on the new cross validation dataset  Greg thanks for pointing this out  Im currently traveling but will look into this over the weekend  Wil if you can get historical data from freechess org possibly by agreeing to share the winning method with them wed be happy to host a comp here  This way you could specify that the winning method must be an instant gratification system  It would also result in a system thats tuned to lower ranked players  Thanks for pointing out the error  It has now been fixed  Apologies for any confusion  Cole sorry for the slow response  In this competition all your submissions count  In future we will ask participants to nominate submissions  Phil that is correct  You must remember that  Kaggle hopes to do more than just host fun competitions we want to help solve real problems  This is why were reluctant to force participants to choose just one model they may make a poor choice and the compettion host may end up with a suboptimal model  Our compromise position is to allow partipants to nominate five entries a feature which well roll out for future competitions  Phil number is correct  Luck will play a part but I suspect the test dataset is large enough to limit its impact I agree in a competition like this one  But as mentioned above we want to host competitions that are useful as well as fun  An upcoming competition will require participants to predict who has prostate cancer based on variables  In a competition like that it would be a shame to miss out on the best model  Requiring participants to nominate five submissions seems like a good compromise  Greg this problem has now been fixed  Thanks again for pointing it out  Hi  Cole Apologies for the ambiguity  The time is as it appears on the competition summary page adjusts according to the timezone on your computer clock so itll be  Saturday or  Sunday depending on your timezone  You can also see a countdown on the  Kaggle home page  Anthony  Dirk I just changed the file posted on the  Data page to a unix format  Hope this solves the problem  Durai apologies for the slow response  All up countries were represented  Here is the list in order of most participants to fewest  United  States  United  Kingdom  Australia  Canada  Thailand  India  Germany  Spain  China  Netherlands  France  Italy  New  Zealand  South  Africa  Sweden  Argentina  Croatia  Ecuador  Greece  Indonesia  Iran  Ireland  Mexico  Poland  Portugal  Russia  Singapore  Turkey and  Ukraine  Ricardo you are correct I gave the country list for the wrong competition countries were represented  United  States  Colombia  India  Australia  United  Kingdom  France  Thailand  Canada  Germany  Argentina  Japan  Afghanistan  Albania  Austria  Belgium  Chile  China  Croatia  Ecuador  Finland  Greece  Hong  Kong  Iran  Poland  Portugal  Slovak  Republic  Venezuela  Uri makes a very good point  One way we could run a competition without knowing future matchups is to have participants rate every player  Once we know the matchups we can infer predictions based on players ratings  The only downsides to this approach are  It doesnt allow for probabilistic predictions since there are many ways to map ratings into probabilities  We couldnt show a live leaderboard which helps to motivate participants  Interested in others thoughts on this particularly the importance of a live leaderboard  Ron this is fantastic  Looks like a sizable proportion of the black dots are sitting in a vertical line  Though  Im sure the  Elo  Benchmark would look much worse  Out of interest what software did you use to generate the viz ps  Im guessing the anomalies that this viz highlights e g that white is a smaller advantage for lower rated players could inform future versions of your rating system  Philipp I dont fully understand your suggestion  Do you mind trying to explain it again  Possibly by reference to an example As a general principle tne problem with attempting to prevent people from using neural networks and the like is that participants use them anyway and then overfit other systems to replicate the neural networks results I actually think that having neural networks et al in the competition is valuable  Even if they wont be implemented as rating systems they may have some benchmarking value  Assuming they predict most accurately they give a sense for what level of predictive accuracy is possible from any given dataset  As an aside if we require participants to submit ratings and dont give them access to the matchups that theyll be scored on this should force participants to create a rating system shouldnt it BTW  Jeff re I have been and continue to be amazed by the level of participation so far I had no idea so many people would participate  Congratulations on organising such a popular competition PEW what criteria would you use to evaluate such systems B T W I think youd be surprised at the proportion of the top who are building rating systems  Philipp thanks for pointing out this bug  The error was only aesthetic had been accidently hardcoded into the new theme  The platform was still only permitting two submissions  Anyway the error has been fixed  Philipp thanks for your nice words  Hopefully having a more professional look and feel will help us attract interesting competitions with bigger prize pools  Hi all Wondering why the benchmark is still leading when it is publically available  Have people had trouble replicating the authors methodology  Or is everybody trying their own approaches  Anthony  Hi  Jesse You are correct this is instruction is wrong  The monthly columns mm should be lines long including the header and the quarterly columns qq should be lines long  The examplesubmission csv file available on the data page gives an example  Im at a conference today but will correct the instruction as soon as I get the opportunity  Something was amiss  There was an error in the data uploaded on  Kaggle  Kaggles fault not the authors  The changes are not particularly big so models that performed well on the previous dataset should continue to perform well  To give you the opportunity to rerun your models and make new entries we have extended the competition deadline by two weeks and lifted the daily submission limit to three per day  And I believe  George intends to release the code used to create the benchmark  Apologies for the error  Dont hesitate to ask if you have any questions  Unfortunately the movie isnt out in  Australia yet weve still got another week to wait  Sorry for the slow response  Ive been flat out with the new site launch  Below is the list of rows used to calculate the public leaderboard  Dirk  Ive changed the line break format  Let me know if this doesnt fix the problem  Jason theres a bug that prevents users seeing previous scores when they have longish technique descriptions  We are aware of the problem and will fix it as soon as we can  Diogo thanks for pointing out this error  We will setup pagination on the submission page shortly  Hi all Just to let you know that we have extended the deadline for this competition by just over a week  Both  Jeff and I will be travellng around mid  November so wouldnt be able to deal with the competitions conclusion  Anhony  Apologies I hadnt antipicated that this might be an unpopular move I should have canvassed opinion first  If others also disapprove I will changed back the deadline  Kaggle is not a dictatorship  The downside of changing back the deadline is that it limits our ability to generate publicity  This bothers me becausea top performers deserve recognitionb publicity for the competition is publicity for  Kaggle and more publicity more members more competitions andc it lessens the chances of getting  F I D Es attention A compromise might be to extend the previous deadline by three days to  Wednesday  November when  Jeff is offline but I am available  Thoughts  Hi  Philipp The  Chessbase articles were written by  Jeff he has a relationship with the editor  Jeff being away when the competition finishes means that its unlikely that  Chessbase will report on the end of the competition a real pity if we hope to grab  F I D Es attention  It is unfortunate that were both away when the competition ends  Obviously not foreseen when it launched otherwise we would have set a different deadline  Anthony  Jeff we must have posted simultaneously  You raise a good point  If  Philipp and others are OK with the th then we should go with the compromise date  This would mean that  Ill be available to report preliminary results and should mean were ready to report the final results by the time you return  Preliminary will be unconfirmed results from the raw leaderboard  Final results after the top ten have all agreed to share their methodology  Ive changed the deadline to the th  As for  Uri breathing down your neck remember that the public leaderboard is only indicative and that the final standings may be different  Tim  Kaggle is currently in the process of putting together a league table which ranks participants based on competition performances  If you perform well in this competition it will count towards your ranking  This first chart how the leading score has changed on a daybyday basis  The red line shows the  Elo benchmark and the blue line shows the leading score  The  Elo benchmark was outperformed within hours which is why its always above the best entry  Interesting to see some recent progress after a period of stagnation well done  Philipp  My guess is that any major improvement from this point on will be the result of somebody trying something quite different  This chart shows the number of daily entries  Higher early but seems to have stabilised at around per day  Happy to put up other charts if people have requests  Phil I made an error in the ten per cent listed above try scoring with the following rows  Philipp theres certainly a largish gap between the top five  Of course this is purely indicative  What really matters is the score difference on the final leaderboard  Philipp great suggesion  Weve got a stack of features we want to implement but  Ill put this in our long term wish list I tried puting up a general forum for such discussions but found that it was very lightly used  Features in the pipeline include fixing bugs or incomplete features on the new site upgrades to  Kaggle infrastructure to allow us to score very large entries  Kaggle ranking system an  Elo for  Kagglers based on  Microsoft  Trueskill extended social networking features including live chat recent activity feeds  Philipps competition analytics suggetion and possibly some other data viz tools Competitions in the pipeline include predicting social network connections predicting the likely success of grant applications for a large  Australian  University forecasting travel times for freeways in  Melbourne  Australia predicting prostate cancer from a high dimensional dataset subject to ethics approval diagnosing breast cancer from mammographics density images also subject to ethics approval Any other suggestions  Any thoughts on what our priorities ought to be  Steffen you can enter using a model coded in any language  John Drew I presume those who enter using software other than R are still eligible for prizes  Diogo thanks for pointing out this bug  Few minor teething problems with the new site we should have them sorted out before long  Jason L T are you thinking along the lines of karma points for participating in forum discussions  Or would you like the forums to be more of a QA with  Stackoverflow style ratings I like the idea of guest blog posts and community tutorials  After the chess competition ends some might be interested in posting details of their workflowmethodcode LT the general forum has been taken down for the moment  When I get a little time I will attempt to revive it and start encouraging people to use it  Philipp it may not matter that people only compete in a handful of competitions because each competition contains quite a lot of information  Unlike a single chess game participants are competing against many players  Regardless well do plenty of testing with  Trueskill before implementation  As for the points system points seem a little abitrary I like the idea of ratings that account for the strength of a competitions participants I tend to agree with your point on forum participation points  The  Stackoverflow approach seem like a nice way around the problem  There are lots of directions we could take  Kaggle  But for the moment were focused on competiitons  Yuchun Apologies for this error  The public leaderboard is portion of the test dataset is actually the first per cent because we hadnt implemented the code to select a random portion of the leaderboard yet  For info the reason we kept getting different per cents is because the random seed in the database was set to zero which told our code to choose a random random seed  Anthony  Hi  Artem For the intuiton behind AUC have a read of the evaluation page  Kaggle implementation of AUC works roughly as follows  Sort submissions from highest to lowest  Goes down the sorted list and for each prediction plot a point on a graph that represents the cummulative percentage of class A predictions against the cummulative percentage of class B predictions  Join up all the points to form a curve  The AUC is the area under this curve HT  Phil  Brierley for this explanation  William no thresholding is required which is part of the beauty of AUC  In fact given that the algorithm works by sorting participants make submissions containing any real number higher means more confidence that the observation is of the positive class  Hope this response doesnt serve to confuse people  Anthony  Hi  Jon Its a fixed per cent chosen randomly  Anthony  Hi  Tamas As your results suggest the order does matter and the  I Ds dont  Anthony  Artem  Ive gone through the steps using your example data  Let me know if  Ive made any errors  The  Kaggle algorithm basically works as follows First order the data predicted real  Then calculate the totals for each class in the totals totals  Initialise the cumulative percentagespercentslast percentslast  Iterate for each solutionsubmission pair counts counts counts counts percents countstotalspercents countstotalsrectangle percentspercentslastpercentslasttriangle percentspercentslastpercentspercentslast area area rectangle trianglepercentslast percentspercentslast percents So in your example First submissionsolution paircounts counts percents percents triangle rectangle  Cumulative area percentslast percentslast counts counts percents percents triangle rectangle  Cumulative area percentslast percentslast counts counts percents percents triangle rectangle  Cumulative area percentslast percentslast counts counts percents percents triangle rectangle  Cumulative area AUC  Also heres  Kaggles PHP code to calculate  A U Cprivate function  A U Csubmission solution arraymultisortsubmission SORTNUMERIC SORTDESC solution total array A B foreach solution as s if s total A elseif s total B nextissame thispercent A thispercent B area count A count B index foreach submission as k index if nextissame lastpercent A thispercent A lastpercent B thispercent B ifsolutionindex count A else count B nextissame ifindex countsolution ifsubmissionindex submissionindex nextissame mycount if nextissame thispercent A count A total A thispercent B count B total B triangle thispercent B lastpercent B thispercent A lastpercent A rectangle thispercent B lastpercent B lastpercent A A rectangle triangle area A AUC area return AUC JC I agree that those who enter early have an advantage  However the main source of advantage comes from the fact that they have had the opportunity to spend longer on the problem and try more things  Philipp the current leader has made entries  If this competition took ternary scores loss win draw this would amount to possible combinations making  Phillips entries a drop in the ocean  In fact the test dataset is richer because participants predict the probability of victory  Nonetheless for future competitions we will ask participants to nominate five entries that count towards the final standings PEW we are not requiring participants to guess but rather encouraging them to rely on their cross validation when determining which models to choose  The problem with allowing people to enter many times and try many parameter tweaks is that they are more likely to accidentally overfit on the test dataset  By this I mean they are more likely to find a parameter tweak that works well on the test dataset but doesnt work as well for future chess games  On your second point you are correct to say that I am worried about statistical guessing  The requirement that participants submit code does not obviate this concern because models can be overfitted once the answers are known  In the extreme case somebody could fit a decision tree that classifies every game perfectly if they know the answers  Showing the standings but not the scores makes statistical guessing only slightly more difficult because participants are close enough that the leaderboard ordering gives meaningful feedback on which guesses are better and which are worse  As an aside it seems that I have failed to convey the message that the public leaderboard is purely indicative and that cross validation is important I would even go so far as to say that it may be problematic if the public leaderboard bears too close a resemblance to the overall standings I like  Uris suggestion  It gets around the problem that LT mentons while potentially encouraging people to try things beyond parameter tweaks  Couple of potential problems A participant exhausts the submission limit and another entrant makes and shares a breakthrough eg the use of  Chessmetrics in this competition  Anybody who has exhausted the submission limit wont have the opportunity to build on the breakthrough  This seems less than ideal given that we want to get the best results possible  It might encourage people to make all their entries at the end so that they dont reveal the strength of their hand  What do others think  Philipp  Kaggle has been experiencing a massive lift in site visits and signups since the new site launched from unique visitors to  This accounts for the increase in entries  Thanks everyone for making this an amazing competition Big congratulations to the winner  Outis  Also to the runner up  Jeremy  Howard who only joined the competition late in the piece and to  Martin  Reichert who finished third  Hopefully well get some of the top ten to tell us about their methods on the blog  In the meantime I encourage you all to tell us a little about what you tried on the forums  Also for interest heres a chart that shows how the best score evolved over time  Rapid improvements initially but after a month progress stalled as participants approached the fronteir of what is possible from this dataset  Apologies  Uri and LT seems that any reply is redundant now  Also big thanks to all those who participated in forum discussions  You helped make this a far more interesting competition I think I can help with this I dont give names just score combinationsscore publicscore  Jeff can I post the test labels on the forum I only seem to have the aggregate solution on hand attached  Jeff do you have the game by game labels Edit looks like you posted a minute before me  Hi  Nick Youre welcome to bring additional data as long as its publicly available  Anthony  Attached is some sample code that can be used to constuct an entry that generates a forecast based on the average travel time on a given route on a given day of the week at a given time  Attached is some sample  Python code that generates forecasts based on the last known travel time  Im new to  Python so happy to hear any feedback on the code  Mmm file didnt attached  Heres the codephprh fopen R T A Data csv r  File to read fromwh fopensample Historical csv w  Write the entry to this filedatedefaulttimezoneset G M T  Purely to prevent the interpreter from raising a warningtime Stamp array an  Array with the humping off pointsforecast Horizon array forecast horizon in lots of minutes e g minutes minutes hour  This is used for calculating the forecast time stampsforeach time Stamp as ts foreach forecast Horizon as f forecast Time Stamp date N H istrtotimetsf find day of week hour and minute that corresponds to each of the timestamps row while data fgetcsvrh FALSE loop through the datafile if row  Write the header col Count countdata for c c col Count c fwritewh datac fwritewhn if inarray date N H i strtotimedataforecast Time Stamp if the day of week hour and minute that corresponds to a forecast timestamp is found then save to an array called ts Array for c c countdata c if emptydatac datac x ts Arraydate N H i strtotimedatac datac rowforeach time Stamp as ts foreach forecast Horizon as f fwritewh date Ymd  Histrtotimetsf for c c col Count c fwritewh arraysumts Arraydate N H istrtotimetsfccountts Arraydate N H istrtotimetsfc writes the average for a given day of the week hour and minute to the submission file fwritewhn fcloserhfclosewh  File didnt attached  Heres the codeimport csvimport datetimerhopen R T A Data csvr read in the data whopensample Naive Python csvw create a file where the entry will be savedrh C S V csv readerrhtime Stamp an  Array with the cutoff pointsforecast Horizon forecast horizon in lots of minutes e g minutes minutes hour  This is used for calculating the forecast time stampsrow inialise the row variablefor data in rh C S V loop through the data if row if the first row then write the header for j in rangelendata wh write dataj wh writen if data in time Stamp if the row is a cutoff point for i in forecast Horizon for each forecast horizon write the cutoff travel time as the forecast the definition of  Naive date Str strdatetime datetimeintdataintdataintdataintdataintdata datetime timedeltai calculte the time stamp given the forecast horizin wh writedate Str write the timestamp to the first column of the CSV for j in rangelendata wh write dataj write the cutoff travel time to the subsequent columns wh writen row rh closewh close  Dirk thanks for pointing this out  Ive written to the RTA about this and they responded saying Indeed our control room have confirmed significant increase in traffic volumes following the removal of the tolls  This has had an impact on the overall travel times across the M  Something to be aware of when using the older data  Hi  Dennis  The per cent doesnt count towards the final standings and is selected at random across the timestamps and routes  As for the SMTP error its been fixed  The problem was the result of a flood of signups which caused  Google to shut off our mail server  Were now using our own mail server  Anthony  Apologies for the error its deciseconds not centiseconds so is seconds  Ive fixed the description  This is something that should be dealt with on a case by case basis  If you find a dataset youd like to use ask on the forum and  Ill run it by the RTA  For information  Im trying to get hold of some incident data  Will keep you posted on this  Lee this is great  Dirk did the same thing with some  Python sample code I wrote for the social networking competition  If you guys keep showing me how things can be done better I may become a half decent coder  Toppy thanks for the pointer A higher priority at the moment is to get forum attachments working again  Hi  Peter  Ill follow up in this  At the very least we should be able to provide information on the length of different routes  Anthony  Hi  Carlos  Unfortunately not  Clause c in  Kaggles  Terms and  Conditions saysc employees or agents of the  Competition  Host are not eligible to participate in any  Competition posted by the  Competition  Host  To answer the second question we would more information about the nature of the business and what your friend does  Anthony  Frank this is great I particularly like the heatmap is it possible to zoom Also itd be neat to see some animation on the M map showing how travel times evolve over the course of a dayweek dots getting bigger and smaller  Though I suspect this might be a lot of work  Anthony  David I believe that when loops the measuring device fail travel times are estimated  Im working towards putting together data on when travel time readings are suspect C does seem to be an expressive language  Im a  Linux user though so not inclined to pick it up  Armin I agree  Makes more sense for me compile this information once for everybody  Will try and get it done this week  Daniel  Dennis is correct in saying that averaging the values leads to floating point numbers  The answers are integers but the RMSE is calculated using floating point arithmetic  Thanks B  Yang  The benefit of publishing code is that you get sensible suggestions in return  Andrew good discovery  Ill pass the question onto the RTA  Edit  Wouldnt it be obvious if they werent making the adjustment since peak traffic times would change  This code does generate a sample entry  To use it a download the PHP interpreterb create a file name xxx php copy the code above and download the data files to the same directoryc run the command PHP xxx php  Youre correct the future is used to predict the present  However I dont think the temporal leakage invalidates the algorithms developed in this competition  Daniel and  Dennis are correct  Keep in mind that the per cent is a random selection of the that doesnt count towards the final standings which are calculated based on the other per cent  The cutoff times are all between am and pm  They were selected using a simple formula that favoured high volatility cutoff times over low volatility cutoff times  So youll see more peak hour morning and afternoon cutoff times  The rationale behind this is that its more important to predict accurately during high volatility times so we want to favour models that do best at these times  That explains why the RMSE is higher than for randomly chosen cutoff points  Aidan have asked the RTA about this  This was the response  The cutoff is due to free flow conditions imposed by the system during data unavailability  Ive written again asking for a little more detail  Will post the response when it comes  Paresh thanks for the thought provoking question I agree with  Dennis I am more interested in the time delay than the percentage delay  On a related matter we think it is more important to predict correctly when travel times are volatile e g before and after work  To favour models that predict more accurately during high volatility times we selected more high volatility cutoff points so youll notice more cutoff points during the morning and afternoon  Phil thanks for sharing this  Just got to find a  Windows machine to run it on  Hi  Markus I can help out on the second part of your query  Ive posted some PHP AUC code on another forum post software packages like R have easy to use packages that calculate AUC  Anthony  You can email me the file if you like anthony goldbloomkaggle com  Id be happy to take a look at it  Rasmus apologies I deleted the wrong post  Anyway you asked how travel times are measured  There are regularly spaced loops along the M  These loops measure each cars speed and the number of cars that travel across the loop every three minutes travel times are then calculated using a formula  The formula has been tested and calibrated using test cars that travel along the freeway and record their travel times  Thomas I selected specific cutoff times randomly but chose timeday combinations that are volatile across the dataset  Vitalie the volumes data is used to calculate travel time see this post for more info  Our priority at the moment is to get the incident loop error and route length data together  However I can find out if this data can be made available if you think it might be useful  As  Dennis says itll be highly correlated with travel time and we obviously wouldnt release it for the blanked out times  Jeremy I wasnt aware that public documents with traffic details were available  To the extent that any information is available for blanked out times this would most definitely be considered cheating  As for question I am aware of this in fact the issue came up in another post  The rules state that the winning model must be implementable by the RTA in order to be eligible for the prize  The averaging model passes this test  As an aside I dont believe the temporal leakage invalidates the algorithms developed in this competition  Benjamin once we get the incident data I will put in a request for this data I have some information on suspect loop readings that  Im working to release  This has information on when loop readings may be unreliable for various reasons I dont yet know whether or not this will help with the free flow issue  Anyway I will upload them as soon as I can get it into a useful format I suspect the reason the free flow times are different is because route lengths are different  Rob on your point about missing data it might be helpful if I explain how I put the files together I received data in the following formatroute  I Dtimestamptravel time xxx xxx I transposed them into in the hope that theyd be more manageable  When timestamps were missing I just filled in a blank row  Hassan the most important file is  R T A Data csv  You can create a sample entry by downloading  R T A Data csv and create Historical php attached to the same directory navigating to that directory in the terminalcommand prompt running php create Historical php  This will create an entry based on a historical average for that timeday and is a good starting point BJB very generous of you to upload a  Java code  Ive now enabled java file uploads so you should be able to upload the file burak the times in sample Entry csv are the times you need to generate forecasts for  Theres more info on how the cutoff points were selected in this forum post  Aaron you raise a good point  According to the route definitions I have route extends from loop A to loop A while route extends from A to A so should encompass all of  Denniss observation that sometimes has longer throughput times than is strange  Ill double check the definitions with the RTA  Aaron another good question  Have also passed this on to the RTA  The number directly to the left of the team name is the teams position and the number to the right of the team name is the teams score or  Root  Mean  Squared  Error RMSE  Alexander to me this means that the algorithm can take a timestamp as an input and can generate forecasts for the next mins mins etc  Lee thanks for pointing this out  This post post  Alexander there is no truncation of floats  Mmm my message seems to have disappeared from the board  Anyway heres a repeat  Aaron the units are deciseconds  Nick actually its a hybrid approach  You can nominate five entries that count towards the final standings  You do this from the submissions page the last five are chosen by default  At the end of the competition the best of your five nominated entries counts towards your final position  And  Nick on your new question the one of the five you nominate that scores best on the per cent counts  The per cent is meaningless as far as the final standings are concerned  Jose do you want me to ask if its permissible to use NOAA data  If so are you asking about the data that  Brad mentions above  William thanks for the question  Teams are allowed to merge  One individual cannot be part of several teams our systems ensure this anyway as long as somebody doesnt have multiple accounts  Agree that we should make this more explicit in the future  As for finding people who submit from multiple accounts we are actually in the process of implementing rules that alert us when it looks like this is happening  In the future for large prize money competitions we may look at verifying identities  Really nice feedback very thought provoking  The API suggestion is nice  It does seem that it would prevent people from using the future to predict the present  However the testtraining split is still necessary to prevent overfitting and we could still only give partial leaderboard feedback the API doesnt secure against overfitted parameter tweaks  Also the API approach would add new problems models will take longer to run because of the delay in receiving data points as you say it would add a huge load on  Kaggles servers  As for the problems you list here are my responses Predictions can’t use all available prior data since the test data doesn’t provide results This is necessary to ensure against overfitting  If all the data is used to calibrate a model its impossible to know if the model will fit future datasets as well  Limited training and test data creates too much variance between the public score and actual score The mistake made in the first competition was with the size of the public leaderboard portion of the test dataset my fault not  Jeffs  It was too small which lead to the low correlation between public and overall scores  For the RTA competition we raised the proportion to ensure a stronger correlation  This proportion was calibrated after some testing of the correlation between the two parts of the test dataset  We intend to continue this practice going forward  Model parameters can’t be tuned because actual scores aren’t provided  If we allowed parameter feedback on the whole test dataset this would almost definitely lead to overfilling parameter tweaks that work on the test dataset but wont work for for future datasets  Number of submissions is severely limited because they are so large this will become a bigger problem as larger test datasets are created I dont think more daily submissions are necessary because the majority of model building should be done with reference to a cross validation dataset  Leaderboard doesn’t reflect actual leaders Again this was my mistake I made the public leaderboard portion of the test dataset too small  This is not a flaw with the general approach  Future data can be used to predict the past Jeff suggested a really nice solution to this test set includes some spurious games so that people can’t mine the test set for useful data about the future  These spurious games wouldnt be used in final evaluation  The API also provides a really nice answer to this problem  Attached is some R code to create a GLM entry for this competition  As always happy to hear feedback from others about how this could have been done more elegantly  Anthony  Nathaniel is right the data is correct its just a problem with heading formatting  Will fix this shortly and reupload the data  Rob thanks for jumping in  Eleni just uploaded  Route Length Approx csv which has approximate route length data  Konstantin just uploaded  R T A Error csv the is valid data  Its available on the data page  Finally fixed the headings  Just to reiterate all the data are correct its just the capitalization in the headings that caused trouble  As for the inconsistent numbers of delimiters also fixed my software package stopped printing delimiters when there were no more values or  N As in a row  Jack the country of birth issue is now fixed  Please download the latest version of the data P V  Kiran it means that if your solution is implemented using a software package that is not available to the  University of  Melbourne it must be possible to translate your solution into a different packagelanguage  No  Towards the end of this competition you will be asked to nominate five entries that count towards the final result  Dielson good pick up  Dennis is correct the date format doesnt matter  What is important is that you put the correct data in the correct cells  Mooma I appreciate your frustration but sensor malfunctions are part and parcel of dealing with realworld data  If we had the data ready at the outset we might have excluded failed sensors and downweighted the impact of partially failed sensors when evaluating predictions  Konstantin  Dennis is correct it is not safe to assume that there is no errors in the control data  Nathaniel thanks for pointing this out  Definitely worth investigating  The  Number of  Successful  Grants and  Number of  Unsuccessful  Grants fields dont change in the test dataset for obvious reasons  The journal citations also remain constant in the test dataset to prevent participants using the future to predict the past  Jose and  Joseph just spoke to the RTA about this  The answer is no because it might allow future weather conditions to be used to predict the present  Ahmed just got an answer from the RTA on this  Heres the response The answer is maybe RTA would request that anyone wishing to use the data for further research purposes write to the RTA and make their case describing what they wish to do ie the purpose of the research and how they would use the data  The RTA will consider each application on its merits  Let me know if youd like me to pass on the relevant email address  Im reluctant to do it in the forum but will offer an introduction to anybody who asks  Just elaborate a little the types of solutions that cant be implemented are those that are encumbered by patents or other intellectual property restrictions  Michelangelo truth is that you can submit any real number we suggest a number between and because of the convenient interpretation AUC ranks your scores the higher the score the more confident you are that the instance is a member of the positive class  Nathaniel I have looked at the problem in some detail and have spoken to the  University of  Melbourne  They are looking into it and hope to have an answer for us tomorrow before they break for  Christmas  Many thanks to everyone for all your great activity on this fascinating problem insightful questions and comments on the forum good early results on the leaderboard and interesting discussions  There have been a lot of questions about exactly what constitutes an acceptable model for the RTA  So far my guidance on this matter has possibly been too fuzzy and I hear a lot of you looking for more definite rules  Therefore we have come up with the following specific rule regarding the allowed model inputs  Your model can be of any form you like as long as it takes its input only from the following parameters  Time of prediction  Day of week  Is holiday  Month of year  Route number to be predicted  The time taken for route r for datetime t where r is any route and t is any time less than the datetime being predicted for as many routes and datetimes as you wish  The sensor accuracy measurements for any routes r and datestimes t defined as above  The estimated route distances as provided by  Kaggle To clarify the following are not permitted  The use of any data other than those provided by  Kaggle for this competition and the list of NSW holidays  The time taken for any routes in the future compared to the prediction being made your model can still be trained using all data as long as the resultant model only uses the inputs listed above  Furthermore the algorithm must not be encumbered by patent or other IP issues and must be fully documented such that the RTA can completely replicate it without relying on any black box libraries or systems  The university has spent the last two days on the problem  They suspect its an internal inconsistency in their database the figures are drawn from different parts of their database  Well have to wait until the end of the  Christmas break to get a final verdict  Hi  Alexander  No  Using full timestamp makes it possible for a model to implicitly incorporate external data and future data  You may also use holiday data extracted from the PDF file that you linked to in order to get holiday information for previous years  However we will not be providing a file of this information directly  This is correct  Anthony  Deepak thanks for pointing this out  We will ask the university about this as well  Unfortunately we cant expect an answer until early next year person I Ds refers to all the columns that have investigator  I Ds e g column has investigator column has investigator  Ignore the comment numerical values that should be  As  Jeremy  Howard pointed out earlier in this thread the key point that answers most of these questions is that the limitation is only on the functional form of the final model  More specifically  Xiaoshi  Lu  You can build your model filtering aggregating etc using all the datetime information you like  The final functional form that you end up with however should only use the predictors listed above  Mooma  The inputs listed include this  The time taken for route r for datetime t where r is any route and t is any time less than the datetime being predicted for as many routes and datetimes as you wish  So what you ask is specifically allowed  Of course for you to create your input file which includes for example the time taken one hour earlier you will need to use the full datetime  However the resultant model will not directly use this instead it will only use the time taken on that route as allowed by the rules  Alexander  Groznetsky  Imagine using a very flexible model neural net for instance which trains with all datetime info included in the input parameters  It might implicitly end up using the route times later in the day to predict those earlier  This is an example of how a model could be useless in practice even although it appears highly predictive on the competition data  Jose I notice that you are now on the leaderboard  It can take a few minutes before you show up  Anthony  Matthew  Using GPL code is fine  The isholiday variable can be a direct input rather than a variable that is derived by reference to a timestamp  You contact me directly at anthony goldbloomkaggle com  Dennis you can use isspringbreak rather than isholiday  David its really neat  For info it works in  Safari but the page videos are aligned a little strangely  Martin  Dane is correct the information in  Route Length Approx csv is in metres  So route is approximately km  The  In the  Money indicator is based on the public leaderboard only  It doesnt reveal anything about the final standings  Martin when I open the file it shows and  What application are you using Anthony I believe it refers to grants made when the researcher was at another university  Nicholas a  Matlab solution is fine as long you dont include libraries that use patented or undocumentedsecret algorithms  Apologies  Will I was on a plane and only just got your message  Will make the adjustment this afternoon  Id also like to congratulate the top teams and congratulate  Dirk for running an excellent competition  Thanks to everybody who participated and a big thanks to  Dirk for putting together a really nicely designed competition  The test labels are attached to this post  Rafael and are fine is also fine as long as the data is derived entirely from the time series as you say B  Yang first off congratulations again on a fantastic performance Your frustration is understandable but we cannot enforce rules that dont exist what is common sense to some is not common sense to others  As  Jeremy points out in the RTA competition the rules say  The winning entry has to be a general algorithm that can be implemented by the RTA  An algorithm that involved looking up future answers could not be implemented by the RTA  Reginald please email your submission to anthony goldbloomkaggle com and  Ill have a look  Edith thanks for the feedback  We agree with your comments and we are working on making the terms more competitor friendly  The university has done an investigation and has found that the issue arises from an inconsistency in their database  Wu  Wei a route is made up of several loops A figure of means that per cent of the loops in the given route are giving suspect readings  Apologies I didnt clarify this with  Mahmoud before the launch but we have discussed this offline  This competition requires you to choose five entries that count towards the final result  To choose five entries visit your submissions page and click the star next to the relevant entry to select it  If you do not choose any entries your last five entries will be chosen by default  Michelangelo the per cent comes from the test dataset  Eu  Jin  Lok the sampling is done randomly  For anybody interest heres the actual solution  Hi  Greg The answers will be made available on the forum I can ask whether the data can be used for publishing research if you like Kind  Regards Anthony  Hi  Greg and  Suhendar The university doesnt want the data to be used for any purpose other than for this competition  Anthony  Hi  Greg It would be nice if the dataset could be used for other work  However if we dont allow competition hosts to place restrictions on the use of their data then we wouldnt get access to it in the first place  Will post the solution file now  Regards Anthony  The solution file is attached to this post  Thanks all for participating Anthony  Entries made before we fixed the leaderboard were scored incorrectly I have now rescored the relevant entries  The error was the fault of  Kaggle and not the competition organizers  Apologies Anthony  Hi  Cerin Apologies for the errors  They all stemmed from the fact that the servers hard drive filled up  Ive cleared some space  For information were currently rewriting the entire site for the  Heritage  Health  Prize  You can expect the next version to be faster and include many more features  Thanks for your patience Anthony  Hi  Cerin Ali is right your entries will count towards the final standings  Anthony  Hi all Submitting from multiple accounts is most definitely against the rules  We have done some analysis and found that it happens very rarely  However we are working to put the systems in place to identify and block those who attempt to do it  Kind  Regards Anthony  The solution is attached  Thanks all for participating Anthony  Harri Thanks for the thoughtful post  The IJCNN people agree with you and have decided not to disqualify  Shen  As mentioned above  Kaggle will soon have the systems in place to detect multiple accounts in real time so that such issues dont arise  Anthony I have sympathy for peoples frustrations  In this case the competition host decided that the results should stand so we are facilitating their decision  Chris makes a good point about the rules being scattered throughout the site  We will be sure to address this in future competitions  We will also ensure that they are tightly enforced  For information a lot of effort has gone into framing the  Heritage  Health  Prize rules  Finally thanks for the feedback  Its discussions like this that will help us improve  Kaggle  Kaggle has received legal advice after the controversy surrounding this competition  We have been advised that it sets a dangerous precedent for us to ignore our own terms and conditions notably clause preventing multiple signups  We have therefore acted in accordance with this clause disqualifying those who clearly submitted from multiple accounts  Thank you all for your patience on this issue and rest assured that we are working to ensure that it is not a feature of future competitions  Entrants are welcome to use other data to develop and test their algorithms and entries until UTC on  April if the data are i freely available to all other  Entrants’ and i published or a link provided to the data in the “ External  Data” on this  Forum topic within one week of an entry submission using the other data  Entrants may not use any data other than the  Data  Sets after UTC on  April without prior approval  Also covered by  Slate and  Forbes and the  Wall  Street  Journal a couple of weeks ago  And  Smarter  Planet  The criteria was that somebody had to make at least one claim in Y be eligible to make a claim in Y  Outliers have been removed from the dataset as well as those suffering from stigmatized diseases  Just to clarify when  Jeremy says we cleaned it as much as we can we didnt do much to the claims data on purpose  We figure it makes more sense for you to make your own cleaning assumptions rather than have us impose them on you  Not only are patients who died in Y not in the dataset but patients who died in Y are also not in the dataset because they didnt remain eligible to claim for the whole of Y  Apologies this was an error  Thanks for drawing our attention to it  The missing values are for those people who have been in hospital for more than two weeks  They should be replaced with a  You can either do this yourself or download the updated dataset  For information members who have in hospital for more than two weeks have been grouped for privacy reasons they are rare so may otherwise be identifiable  The implication of this grouping is that if you expect somebody to be in hospital for more than two weeks you should predict days  This grouping should not have a big impact because a members who are in hospital for more than two weeks are rare about one per cent of members b the evaluation metric favors algorithms that accurately predict fewer days in hospital on the assumption that these are more preventable  Dorofino  Great idea  Forming a team is a really good way to learn  Are you affiliated with the  New  York R  Users  Group  For info  Ive heard rumblings about them setting up a team  Good luck with this  Anthony  Hi  Rich  Just spoke to HPN about this  For the moment they dont want to provide general guidance and ask that you make a request through the contact us form  Your request should detail the topic of your proposed research  Definitely worth making it clear that youre just looking to publish the method that you use to enter the competition  Anthony Y Y Y etc refer to different years  We havent revealed which years to help keep the data private  The years are sequential  We are not revealing what years  Yn refer to nor whether or not they refer to calendar years for data privacy reasons  Apologies for the missing values it was an error  You can either replace the missing values with or download the updated data set  If youre interested in the reason for the missing data see  Hi bacg  Days In Hospital refers to Y the second year while the claims refer to Y the first year  Not everything that has a length of stay counts as a hospitalization  In fact you dont have enough detail in the  Claims table to calculate  Days In Hospital  The detail has been suppressed for privacy reasons  Anthony  Hi mbenjam  We would have loved to release more detailed data but have to be mindful of data privacy  Anthony  Have received advice from the HPN lawyers  Im really sad to say that the answer is no on all accounts  Eu  Jin youve obviously not seen this  Wgn the intention is not to rule out the publication of research  Ive passed on your message to HPN and a clarification will be forthcoming  The lawyers are taking a conservative stance on this issue  Apologies its really disappointing to have people ruled for this reason flsdcom I have a meeting with them in minutes I will be sure to raise this point  In response to ashashos original question I have sought a reexamination of the issue  The HPN lawyers explained that the reason for the hard line is that they have no way to verify that residency permits comply with US legislation  Im really sorry to say that theres not more I can do  The accuracy threshold will be announced when we release the full claims dataset on  May I want to reassure everyone that HPN is working hard behind the scenes to clarify the IP issue  It is not their intention to prevent people from using standard tools nor to discourage anyone from applying their innovative ideas to this problem  For background at  Mondays launch event  Dr  Richard  Merkin the man behind the prize spoke of the long tradition of innovation that has resulted from past prizes  He spoke of the  Longitude  Prize apparently  Newton and  Galileo had attempted to solve this problem but the winner was a self educated clockmaker from  Yorkshire  Napoleons food preservation prize won by a confectioner and resulted in the invention of canned food the  Orteig  Prize to fly nonstop from  New  York to  Paris won by the unlikely  Charles  Lindbergh  It is his hope that this prize will spur similar innovation to solve one of  Americas most vexing problems  We appreciate your patience while we await clarification  Kind  Regards  Anthony  This is a sample of the final dataset but the final dataset is not in the  Terabyte range  To the best of my knowledge this dataset is on the larger side for medical datasets which tend to be quite small  This algorithm will not need to operate in a realtime environment and so there is no restriction on execution time rudychev received an answer from HPN on this A patient who visits a clinic outside the network should be captured in this dataset  Of course as  Jeremy keeps reiterating there is always a disconnect between reality and the contents of a database  The decision to predict days in hospital was made to make the test dataset richer so we can better sort out good algorithms from bad  The logarithm in the evaluation metric was chosen to favor models that predict short stays more accurately as these are assumed to be more readily preventable  As for the question of nefarious intentions I can tell you what I know about  Dr  Richard  Merkin the man behind the prize  He is a big philanthropist who devotes time and resources to funding scientific projects schools and the arts  In my opinion HPN did not need to put up million to get an amazing algorithm  Kaggle has found in its own competitions that with prizes as small as or a chess DVD participants approach the limit of whats possible on a dataset  In our communications with HPN we have been told that the million prize is an attempt to draw mass attention to this prize and the issue in general  Dr  Merkin wants to promote the potential for medical data mining in lowering healthcare costs  The prize also serves to introduce a large number of talented data scientists to medical data  Finally rest assured that HPN are working hard behind the scenes to clarify the IP issue mgomari one issue we have to keep in mind are the tradeoffs in releasing data  For data privacy reasons HPN have a granularity threshold which theyre not willing to breach  The data anonymization team represented by keleman in the forums are trying to release  C P T Codes probably at an aggregated level  Apparently its pretty lineball and releasng  Days In Hospital Y might put this in jeopardy I describe the data privacy considerations like a waterbed you push down on one part of the bed and it creates a bulge somewhere else  After  May youll be able to use  Days In Hospital Y and  Days In Hospital Y to predict  Days In Hospital Y ogenex even if we release  Days In Hospital Y you wont be able to do a consistency check  Not all length of stays count as hospitalizations as calculated for this competition and you dont have enough detail in this dataset to work out which count and which dont  For those who dont know jphoward was  Kaggles most successful competitor before joining the team  His tutorial gives really clear explanations of the tools and techniques that made him such a successful competitor  Hi  Jim  That is correct  For information the reason for the misnomer is that it was days when we sent it to the anonymization team but they had to group the days to ensure the required level of data privacy  Anthony sciolist yes teams are required to publish publicly ashojaee the clarifications havent been made yet mkarbowski as jphoward keeps pointing out theres often a massive disconnect between reality and the contents of a transactional database  See ejloks humorous post for even odder records  Agree  See the updated evaluation page  We intentionally decided against cleaning the data so as not to impose our assumptions on participants  Domcastro one of  Kaggles first suggestions was to remove the registration fee  For info the registration fee wasnt ever to raise money but to try and deter people who werent serious from downloading this sensitive data  Kaggle pointed out that anybody with malevolent intentions would probably still pay the modest registration fee so its effect would be to deter people who didnt think they had a chance of winning  Kaggle went on to argue that these people may also come from interesting backgrounds and may be the ones most likely to apply creative thinking to the problem DIH includes inpatient admissions and emergency room visits  As mentioned previously you dont have enough detail to calculate it from the claims table  We want the forum to be tightly integrated into the site e g to be able to link to forum posts from profiles and vice versa YAF is the best NET forum software out there and integrating it into  Kaggle is more trouble than its worth  Also moserware is a brilliant programmer so its the type of thing he could put together in less than a week  Realworld data is messy  Well put up a data dictionary soon quotedaveime  Seriously I understand the need for randomizing and anoymizing the data but unless they have some way to unrandomize it afterwards any algorithms we create will serve no real world application quote daveime the data is messy not because its been peturbed but because its realworld data  Anonymization focused on generalizing again not peturbing  The the nineyear old pregnant males actually exist in the raw data  For info  Im told that this is one of the cleaner medical claims datasets around mgomari the difference between and is counted as two days  Overlaps were accounted for so were not double counted fjn  Pi does not have to be an integer blonchar youre correct HPN are limited in what it can release by the need to protect patient privacy frankthedefalcos com or the women who have been treated for erectile dysfunction mgomari the answer to both questions is yes jesensky you will be able to use  Days In Hospital Y and  Days In Hospital Y as an input to  Days In Hospital Y I like your thinking on the USE OF OTHER DATA loophole if the answer had been no  Creative thinking cybaea many thanks for a great discovery  After doing some digging weve discovered that the oddeven observation is an artifact of the cleaning procedure  We have worked out a remedy and it will be applied to the dataset that will be released on  May  In the meantime it shouldnt make a huge different to models that are currently being developed boegel yes  On  May we will be issuing significantly more data  Day In Hospital Y csv will be changed then liveflow I may be misunderstanding the question but the competition requires participants to use data from Y Y and Y to predict Y  No  Some Y patients are no longer eligible in Y  We still provide Y patients who arent eligible in Y because theyre useful to train on  No  Again for privacy reasons  Information  Man that is not the intent of the rule  The HPN lawyers are working on clarifying this at the moment  Dougie D every member listed in  Days In Hospital Y is eligible to claim in Y so if they have DIH they are above  The same will apply for the members listed in  Days In Hospital Y and  Days In Hospital Y when we release those files  Days In Hospital is calculated based on the  Length Of Stay variable  However you dont have enough detail to calculate  Days In Hospital from  Length Of Stay irwint good pickup thanks  Now fixed gschmidt not sure if this answers your question but the geographic spread is limited to the area in which HPN operates southern  California I believe  As to whether patients change doctors on  May youll have a few years worth of data so will be able to work this out alexx the HPN lawyers are working on a clarification  This will be released by the time entries can be made on  May metaxab the competition was designed this way to replicate how the model might be used in real life  In a real life situation you wouldnt be able to predict hospitalization with contemporaneous claims  Days In Hospital Y is derived from the claims table where a hospital stay includes an inpatient stay or an emergency visit  Note you dont have enough information to calculate  Days In Hospital Y from the claims table  Hi  Drew  It will be in place by  May when entries are accepted  Anybody who accepted the existing rules will receive the notification via email  Anthony  In this dataset missing  Pay Delay either means unknown or greater than  In the  May release the anonymization team will topcode  Pay Delay so there will be fewer missing values and will mean  You will get some procedure code information in the  May release I understand the frustration but data privacy is a priority for HPN  For generating features I recommend  S Q L Lite though  My S Q L does the same thing I know  Jeremy and  Jeff like  Cs  Linq  For building models I use R rks we will post a sample entry with the rest of the data on  May  Ralph H  Days In Hospital counts days not nights  So if  Days In Hospital is then they have not been to the hospital at all  If they were in and out of the ER then  Days In Hospital would be trezza and RHM Y contains data for a period trezza unfortunately not  The anonymization team have identified this as a data privacy risk  Hi  Allan  Thats because some members have had claims suppressed  In release coming soon well make it clear which members this applies to  Anthony  Hi  Domcastro  Can I use R  Yes  Can I use  Weka  Yes  Can I use  Excel  Yes  If I organise the data in a novel way and just use a standard processing algorithm such as  Naive  Bayes is this OK  Yes  You must preserve the order in  Target csv  Release zip does supersede  Release zip  Unfortunately not  Apologies for any inconvenience  Darragh its a list of all members in the dataset  No  Chris just heard back from the data anonymization team  Members have been renumbered cacross HPN had a granularity threshold that they wanted to remain below  Some  L O Ss had to be suppressed to achieve this target  If there is a blank LOS and  Sup L O S is then this is how it was when it came out of the HPN dataset  If there is a blank LOS and  Sup L O S is then the LOS has been suppressed  Hope that helps mkwan you fill in the team wizard when you make your first entry  Team mergers will be granted at the organizers discretion  Yes  Chris R nice to see you competing in this  Sampling is random  We cant give you an HPN benchmark because theyve not tackled this problem before boegel  Days In Hospital Y contains members who made a claim in Y and were eligible to make a claim in Y  Days In Hospital Y contains members who made a claim in Y and were eligible to make a claim in Y  Similarly target csv contains members who made a claim in Y and were eligible to make a claim in Y  To be eligible means to be an HPN member regardless of whether or not a claim was made  Therefore the members in  Days In Hospital Y are not missing from target csv but rather didnt make a claim in Y or werent eligible to make a claim in Y  Therefore all members in target csv were eligible to make a claim in Y so we have an answer for each of these members JESENSKY by my calculation members appear in  Days In Hospital Y and  Days In Hospital Y but not  Days In Hospital Y perhaps you can confirm this figure  These members are missing from  Days In Hospital Y because they didnt make a claim in Y despite being eligible  Apologies if we didnt communicate this effectively in the description pages  Dan B youre right about the selection bias  But because HPN are releasing almost no information on the members themselves theres nothing to model on for patients without claims  Pro Tester theres nothing in the raw data that distinguishes a death from a patient that leaves HPN for another provider SSRC mapping LOS to DIH is impossible  Not every LOS entry corresponds with a DIH e g hospice stay  One reason somebody may have DIH in y but no claims is if they werent eligible to claim in Y in which case their Y claims wouldve been removed  George there are members in the dataset but you are only tested on members  Thats because the extra members arent eligible to claim in Y or didnt claim in Y  They have only been provided to help you train your model  Tom SF  Haines  Jeremy is not the author of the rules  He is merely trying his best to point people to the section that makes the rules as competitor friendly as possible given  H P Ns requirements  Also if you would like to publish your algorithm I strongly encourage you to put in a research request using the  Contact  Us form is the maximum  Ive said this before but I think  Jeremys tutorial is really excellent although it is not focussed on HHP  He is hoping to get the opportunity to do an HHP tutorial in the next few months  Further to  Wills point those who followed the  Netflix  Prize will remember the jump from the  Simon  Funk discovery  She will be added in the next release  Darragh I passed your question onto HPN  Heres the reply  Is there a delay between the scheduling of the surgery and when it takes place  Yes  But that is just a matter of scheduling not something forced by the government  It would also of course depend on how urgent the surgery is  The intent of that provision is to prevent the data being shared with those who have not agreed to the competition rules  Jeff was just referring to the measures he would take to ensure the data isnt accessible to others  Jose thanks for your diligence on this  Its difficult for us to give specific guidelines  Again HPN is just trying to prevent the data from being accessible to those who havent accepted the rules  Jim its being assessed against Y hospitalizations  Bernhard your interpretation sounds about right to me  Thanks  Dave  The data description has been fixed  Hi guys  Glad you like  This dataset reminds me of the RTA data which was really popular  On the IP question when no rules are explicitly stated the  Kaggle terms and conditions prevail  Specifically clause  By accepting an  Award you agree to grant a license to the  Competition  Host to use any  Model used or consulted by  You in generating  Your  Entry in any way the  Competition  Host thinks fit  This license will be nonexclusive unless otherwise specified  Anthony  Hi  Bobby  Can you clarify what you mean by this  Are you asking if they are obliged to share their model if they finish in first place  Anthony  Hi  Willem to what extend the results have to be identical for example small differences in the random number generator may give different results although they should be similar  They do need to be identical  You can give your random number generator a seed to make sure the resultls are the same each time in how much time should the results be reproduceable my current best result is a mix of many models each may take minutes to hours to generate  There is no rule about execution time the algorithm should produce similar results on a new dataset this doesnt sound very realistic I dont think there is any way to win this competition without optimizing for this specific dataset  Results on other datasets may be very bad with the given optimizations  Probably very good results can be produced by the same algorithm after some tuning but this is a process that requires a lot of knowledge about the used algorithms and a lot of time and patience  Not sure I follow why this is an issue  Remember the  Milestone prize is judged in a portion of the test dataset that participants have not been given any feedback on  Perhaps  Im misunderstanding the concern HTH  Antthony  Regarding the requirement that solutions be identical  Willem it would be better to have participants spend time on innovation rather than reproducibility however its important to have strict rules so that the competition remains as fair as possible B  Yang with regards to the compiler issue we can address it if the issue arises  For example we might start by ensuring that the same compiler is used for verification  Sali  Mali it is exceptable to describe the algorithm and not how it is derived  We are seeking clarification from HPN on the inconsistency that you describe  Apologies for the delay  Regarding the requirement that the algorithm perform similarly on a separate dataset  This is best answered by explaining the rationale behind the rule  It is there to catch any cheating or blatant overfitting  If youre not blatantly overfitting then youre likely to be on safe ground  Hi all  Not ignoring this thread  Just seeking clarification from HPN on one issue  Anthony  John  Only the lowest of the five entries count  Note for the milestone prize only one can be selected  Anthony I have checked with HPN and a milestone prize winner can choose not to disclose their method but will not be eligible for the milestone prizes  Sorry for the delay on this was just clarifying some issues with HPN  Is it inconsistent as  Sali  Mali pointed out in another thread to require documentation of the winning algorithms be publicly disclosed to all competitors given  Rule  Entrant  Representations  It seems that this disclosure will encourage other competitors to use aspects of the winning  Prediction  Algorithm which cause violation directly or otherwise of i iii and possibly iv of that  Rule  Rule does not apply to the extent that it prevents a competitors other than a milestone prizewinner from using code published by a milestone prizewinner in accordance with competition rules and b a milestone prizewinner from competing subsequently in the competition using code for which it was awarded the milestone prize  Can you clarify that code libraries and software specifications are not required to be publicly disclosed to competitors  These materials and intellectual property appear to be referenced separately from  Prediction  Algorithm and documentation  Chris correctly points to  Jeremys response in an earlier forum post “ Only the paper describing the algorithm will be posted publicly  The paper must fully describe the algorithm  If other competitors find that its missing key information or doesnt behave as advertised then they can appeal  The idea of course is that progress prize winners will fully share the results theyve used to that point so that all competitors can benefit for the remainder of the comp and so that the overall outcome for health care is improved ”  Will  Kaggle or  Heritage have a moderation or appeals process for handling competitor complaints  From the winning entrants pointofview they would not want to be forced through the review process to allow backdoor answers to code and libraries which accelerate a competitors integration of the winning solution  Kaggle and the HHP judging panel will moderate the appeals process  Can you comment on the spirit and fairness of the public disclosure of the  Prediction  Algorithm documentation and its impact on competitiveness  In particular if the documentation truly does meet the requirement of enabling a skilled computer science practitioner to reproduce the winning result then this places the winning team at an unfair disadavantage all competitors will have access to their algorithms and research in addition to the winning algorithm  This rule is in place to promote collaboration  Those who would prefer not to share can opt out of the prize  Can you provide more detailed clarification on the level of documentation required by conditional milestone winners  The guideline provided by the rules would cover a range of details and description spanning from lecture notes to detailed tutorial to whitepaper to conference paper etc  Hopefully this was adequately dealt with in  Jeremys response requoted above  Let me know if further clarification is needed  Can you comment on the reproducibility requirement  For example it is possible to construct algorithms with stochastic elements that may not be precisely reproducible even using the same random seed is it sufficient for these algorithms to reproduce the submission approximately  What if they dont reproduce exactly or reproduce at a prediction accuracy that is worse than the submission score possibly worse than other competitor submissions  Exactly reproducibility is required  Correct  If you were per cent sure that somebody would spend days in hospital in Y and per cent sure they would spend day in hospital than you might predict that they spend would days in hospital pham you do not have enough detail in the claims data to reproduce the DIH properly  Youve likely reproduced DIH from claims data as accurately as is possible  Sir Guessalot thanks for the pointer  Its been added to our issue tracker I must admit we have higher priority issues to tackle but well get there eventually  Just to keep you all in the loop the plan is to announce the milestone prize winners at  O Reillys  Strataconf  Will let you know the exact date as soon as were told  Full milestone prize rankings will be released after the announcement is made  The rules do not prohibit  Oracle  Data  Miner  Hi all HPN are currently looking for data scientists  Heritage  Provider  Network the sponsor of the  Heritage  Health  Prize is looking to hire data scientists to take its data and analytics department to the next level  If you are interested in healthcare join the largest physicians group in  California and one of the largest in the  United  States and use your data mining skills to make a difference in the provision of health care to individuals throughout  Southern  California  If interested please send an email indicating your interest to datascientistheritagemed com  Anthony  Provisional milestone prize winners will receive an email over the weekend  An announcement will be made at  Strataconf on  September  Jason the anonymization guys have withheld this information intentionally to make the data set more secure  Sorry  Correct libraryrandom Forestsetwd C Usersantgoldbloom Dropbox Kaggle Competitions Credit  Scoringtraining read csvcstraining csv R F random Foresttrainingctraining Serious Dlqinyrs sampsizecdo trace T R U Eimportance T R U Entreeforest T R U Etest read csvcstest csvpred data framepredict R Ftestcnamespred  Serious Dlqinyrswrite csvpredfilesample Entry csv  Alec setting the random seed is a good idea  Domcastro your hypothesis is correct  Youre correct  Shouldnt include headers  Congratulations team  Market  Makers and  Willem  Great coverage in the  Wall  Street  Journal here  For those interested heres the footage from the award ceremony  Does being a member of HPN mean you usually referred to an innetwork provider of say lab testing unless obviosuly it is some specialty unavailable  Yes  Can you be a member of HPN and have govt sponsored insurance eg  Medicare  Medi Cal  Yes for  Medicare I can follow up on  Medi Cal if you like  Have passed these questions onto HPN  Will respond as soon as I get an answer  On  October the judges in their sole discretion decide whether or not the documentation is sufficient taking account of the comments made on this forum  If they decide the documentation is not sufficient they can impel the winners to address their concerns in the seven days following  October  If the winners are asked to resubmit participants have another days from  November to raise any additional complaints  The judging panel are experienced academic reviewers  Hi all  We are in the process of liaising with the judges  Well report their decision as soon as we have everybodys feedback  We have made a slight change to the  Terms and  Conditions adding  No individual or entity may share solutions or code for any competition or collaborate in any way with any other individual or entity that is participating as a separate individual or entity for the same competition  The foregoing shall not apply to any public communications such as forum participation or blog posts  We are also aware that the rules havent been as clear as we might have liked  From now on before you download the data for any new competition you will be reminded that you cannot sign up to  Kaggle from multiple accounts and therefore you cannot submit from multiple accounts and privately sharing code or data is not permitted outside of teams sharing data or code is permissible if made available to all players such as on the forums  Weve reached out to several teams about this issue  Please let us know ASAP if you have multiple accounts and weve not reached out to you  We are aware that the rules havent been as clear as we might have liked  Please be reminded that you cannot sign up to  Kaggle from multiple accounts and therefore you cannot submit from multiple accounts and privately sharing code or data is not permitted outside of teams sharing data or code is permissible if made available to all players such as on the forums  Weve reached out to several teams about this issue  Please let us know ASAP if you have multiple accounts and weve not reached out to you  It is a mistake were sorry for it but weve decided not to correct it because it might not be fair to some contestants if we change the data midstream  Shouldnt be too importantonly happened to chunks  Its the same mistake that caused a few chunks to have some missing data within the chunks  Sounds like theres a thriving community in  Melb which looks to have been the strongest performing city  Congrats all  Thanks for the nice wishes  Of course  Kaggle wouldnt exist without a brilliant community of data scientists who can solve really challenging problems  Looking forward to seeing what we can do in  Donovan weve looked into this and it turns out that a bug with our process meant that we hadnt received the past few weeks of queries  Weve found your email and you will receive a response shortly as will others who slipped through the cracks  Apologies to you and others who have not received a response as a result of this error  One of my coworkers said were really doing well if you think of  Kaggle every time you see the  Facebook logo  Nice  For interest we typically see strong metrics on  Kaggle during holidays because people have more discretionary time which at least suggests our community isnt too busy with family  Another possible explanation is that people have exhausted their travel budget both time and money on holiday travel and need to wait a little while before booking more travel  Clear and entertaining  Nice work  Why does lower bound get mentioned so much more than upper bound  Ive played with PCA before but never association plots or MCA  Glad to see an example usage and be able to add these to my toolkit  Thank you  For the association plots I assume the width of the box refers to the number of  Tweets referring that that airline I assume is the proportion of comovement explained by the first dimension  Is that correct  Is it typical for the first dimension to explain so much of the comovement  Any thoughts on how to interpret this dimension  Small nit  You might want to change res to reason I initially assumed res stood for residual  And reduce the font size for the x axis label on plot  Thats the most interesting plot to me but its hard to read the labels because they overlap  This is a nice notebook  Suggestions  To make this easier to follow for those who havent yet looked at the data itd be great if you added a section showing a few rows  Or possibly even a few exploratory chartshistograms  Perhaps after the  Loading the data section  Itd also be nice to see the before and after you preprocess the data ie before and after the  Using textmining to format our data section  Rename the  Using textmining to format our data to something like  Cleaning the data  Great I always look at the top rated notebooks before looking at the data because the notebooks usually give me a sense for whats in the data and what I could do with it  Love it  Interesting that for everyone other than  Woodrow  Wilson the names popularity monotonically declines over the course of the presidency  Dwight looks like it increases in popularity during WW which makes sense  One suggestion is to add years to the x axis label for each chart to make things like this easier to spot I  Tweeted this script and somebody replied asking  Is there a corresponding drop in the name frequency of the losing presidential candidate right after the election I was thinking another interesting extension would be to answer the question  Whats most influential in determining baby name trends out of  Presidents and first ladies  Musician that was for longest on the billboard charts in a given year  Best actoractress in the  Oscars  Basketball football baseball  M V Ps  Nobel prize winner names  Time person of the year  If nobody else tackles this I might try it  This builds off a conversation I had with my coworker  Meghan who said itd be interesting to see whether  Presidents or royal babies had a bigger impact on baby names  From this page  This is great  Im surprised  North  America is not higher for sugar  The sweetness of food was one of the first things I noticed when we moved to the US from  Australia  Although it could be because a lot of the sweetness comes from high fructose corn syrup which is not captured  Nicely done and fun writing style  One additional conclusion is that real data is messy  Big  Data  Borat captures it best  In  Data  Science of time spent prepare data of time spent complain about need for prepare data  Interesting how noisy the very early years are I suspect the s data is very poor quality  Really nice script  Interesting to see the temperature uncertainty chart  Gives a nice visual of when the data starts becoming more reliable  Also nice idea to put dt into a variable importance plot to see its relative important  Obviously would have been more interesting if wed provided more data  One suggestion is to better label your plots  Theres some good stuff here but it stakes a while to figure out what each chart is showing I actually looked at your code to figure it out I suspect this script will be more popular with some labels that make it easier to follow  Sven have you been able to figure out an interpretation of this chart  Thanks  Itd be helpful if you labeled the charts and possibly added sub label pointing to your interpretation  It may not be useful for the reasons you mention but it looks nice  Would be cool to see the by city version I assume you didnt use it initially because of the size of the data set BTW I assume red hot  Would be helpful to have a key  Its awesome  Really nicely put together  Bluefool I thought you came out really well  Is this a work in progress  Or is there an error  The charts are showing up blank for me  Juanchaco this is neat but itd be easier to follow if you added a description between charts  At the moment  Im scanning the codecomments to try and figure what each chart is showing  Ha  Id not known about this  For others  Cool  As someone who lives in  San  Francisco  Im curious  Akshay this script would be more interesting if you found a neat way to visualize temperature by country  Love this chart  And the title is funny  Just a heads up that were still working through the winners solutions  Will need more time to before announcing the final results as official  Quick update  We will announce the official results on  Wednesday  March at am ET  Thanks for the thoughtful comments  First off as always we will not make retrospective changes to how we handle past competitions including this one  When issues like this come up we use it as an opportunity to evaluate how we might improve in the future  Internally our debate focused on three issues recognition for those who completed stage one but not stage two achievements and how the competition appears on profiles th out of looks more impressive than th out of how points are handled recognition for those who completed stage one but not stage two  We need to view the stage one leaderboard as having no weight if it gets a weight we incentivize overfitting or hand labeling for stage one achievements and how the competition appears on profiles  If we did what  Julian suggests and add stage one participants to the bottom of the stage two leaderboard we undermine our rankings by making it very easy for somebody to get an impressivelooking top achievement by finishing th out of with a naive submission how points are handled  The one change we will make in future is the way points are handled  We will add a multiplier to the number of points for a twostage competition  We have not settled on a formula for doing this yet but commit to communicating it clearly in the rules of the next twostage competition  These are difficult issues but we think this approach strikes the best balance between competing considerations  Julian responded in the other thread  Hi all  The results on the final leaderboard are now official  Congratulations to the winners and all involved  This is among the hardest and most ambitious competitions weve hosted  We couldnt be prouder of the results  The competition has received some press coverage with a chance of more to come  Anthony  This is the photo from the  Kaggle office  This lunch was one of the highlights of my six years of  Kaggle  Not something I will forget in a hurry  Minor comment theres a typo in the title  Prelimnary should be  Preliminary I only make this comment because its a nice script and I dont want grammar sticklers to be put off the typo  Jeff interesting  Seems like makes the system a little conservative in the handling of new players  Change made  Thanks for the feedback  Cool to get real world SKU data but how would I create a recommendation engine with just  S K Us and ie without customer data I guess  Ill see when you upload your code  Thanks  Allen  Implemented most of these changes  Allen  Jeff or anyone else how do people typically prevent ratings from becoming stale with  Trueskill for example nonactive racers maintaining high rankings I am planning to make the rankings apply over a month rolling window but am curious if there were other approaches such as the inclusion of some kind of time decay  This version should now be correct  The model does not systematically make money but its only using very basic features barrier weight and rider  Hopefully its a good start and somebody can take it and build on it  Hi all  Just a follow up on the request to not share solutions as mentioned above this is not a legal obligation but rather a request from the host  We try and avoid requests like this because it limits the learning that comes out of a competition  Our takeaway from this thread is that if there is a confidentiality request we will flag it up front to avoid an unwelcome surprise at the end of a competition  Anthony  Rakhlin one thing were experimenting with is asking hosts to write blog posts summarizing the outcomes from a competition  This wont be timely because itll happen after theyve spoken to the winners and digested the results  Unclear what reception we will get from hosts but its something were testing out  Foxtrot did you find a bug or a mistake in my code  Or is it that the returns just dont look right given how simple the features are  Hi  Foxtrot I commented out that line because I changed the problem to predict the probability of victory for each horse rather than position xgb  X G B Classifierobjectivebinarylogistic fitdftrain dropdftrainwinpositionmarketidaxis  So I thats not the source of leakage  Im not actually certain there is leakage  If you run the code multiple times the return switches from being positive to being negative depending on the traintest split  Having run it a bunch of times I suspect the expected return is actually negative  Right  But that doesnt happen in this case because  Im predicting probabilities  So the predictions are  Foxtrot nice pickup thanks  Uncommented the shuffle the deck line I suspect theres more leakage in this analysis because the trainingtest split is random and not timebased  The way to get around this for this data set is to train on the form table and test against the runners table  Or even more useful would be to get less anonymized data if  Luke or someone else has it  At the every least race dates are valuable make it possible to protect against leakage  But more generally getting access to the complete raw data is valuable  Everything one does to disguise the data destroys information and limits what a data scientist can do with it  Some small examples related to knowing the venue might give information on what surface the horses are running on a km race at one racecourse could be a straight whereas at other courses it could involve a turn  This might impact the performance of specific horses and cant be accounted for when the variable is venueid  Finally for boosting engagement with the data set anonymized data is less fun to play with than richer raw data I reversed  Moody  Bluefools downvote  By upvoting I dont have database write privileges  Chris I built a model to project out my  Sep and  Oct results  Shame theres no  Oct race  According to my model  Id finish first  Queue XKCD  William nice  Kernel  Looks like your predicting author based on the number of comments subject etc  Whats the thinking behind predicting who the author is I was thinking it might be interesting the predict the number of comments a proxy for how interesting the article is  Also I was looking at your error chart and your comment that  It seems like we are better than chance  Curious how youre measuring that I tried to read it off the chart but couldnt see where it came from  William fyi  That was quick  Your code is much cleaner  Puts pressure on me to go back and clean up mine  This was really just a quick analysis  If I had more time  Id have actually looked at the words being used in the posts rather than guessingrelying on memory E g I probably shouldve included RNN I could also have included package names  Tensorflow  Keras etc  In fact another version of this might look at the packages that are most commonly used by winners  Ps  Thanks for the bug report on the file download  Well look into it I was wondering why the data set had downloads  Jordan looks great  Now that the data is in CSV format I featured it  Writing a first exploratory  Kernel to show people whats in the data is typically helpful for driving driving engagement from the community  Sounds good I was thinking itd be fun to apply  Trueskill to this dataset I used it here  Id like to do it but probably wont have time over the next few weeks unfortunately  Marginal  Revolution featured  Will  Novaks analysis of their posts  If there are interesting findings made on  Techcrunch data  Ill send it across to  Techcrunch  Theytheirreaders may be similarly interested  David I have been able to create the RDS file libraryggmap mapdata getopenstreetmapbbox c colorbw scale round save R D Smapdatafilestpete rds  How do I create the text file  Also why arent you using OSM files the  Open  Street  Map XML file format rather than using  G G Map to export to txt or RDS  Ive spent a while trying to import OSM files for plotting in  Python but with no success  Wondering if you also found this difficult ps  This is for my GPS watch data so the coordinates above are not the  Chicago coordinates  Anokas thanks  My bearing calculation is incorrect  My next step with this notebook is to debug it  How would I use np raddegnp arctan  Anokas I figured out my problem I was feeding latitude and longitude into the calculate bearing function in the wrong order feeding in lat as long and vice versa  Added your more elegant bearing calculation formula as well  Thanks  Ryan appreciate you raising the concern I want to share  Kaggles perspective  First off internally we are incredibly excited about the launch of codeonly competitions  Its the biggest change weve made to competitions since we launched in  It increases the range of competitions we can run including time series competitions such as this one reinforcement learning competitions and competitions on larger data sets  On this competition specifically this is the first step in what we hope will turn into a bigger relationship between  Kaggle  Two  Sigma and the community  This first competition is aimed at testing out the concept of codeonly competitions gauging the communitys interest in financethemed competitions and getting a sense for the types of signals the community finds on a typical financialmarketsrelated data set rakhlin regarding your comments about anonymization and the use of an API these were primarily driven by  Kaggle  The anonymization is in place to discourage people from looking up the answers  The API allows this to be a proper time series competition  We try to make our competitions fair this means making it difficult for the rare participant who is inclined to circumvent the rules  Ultimately our goal is to give our community as many opportunities as we can  This means a mix of commercial competitions research competitions playground competitions getting started competitions and most recently open data sets  Ultimately the community will select what interests them  And  Kaggle will continue to offer the things that resonate  Anthony  Did you try the my submissions tab on the left hand side dashboard  You could also look at some of the weather data and  Kernels that others have shared  Hi  Jackie  Perhaps this dataset is a good starting point  Totally agree with frustrations expressed here  Neither  Kaggle DHS nor the community wants geo restrictions  In this case its a legal restriction DHS worked hard to allow any international participation albeit without prize money  From  Kaggles perspective we prefer to host a competition with a geo restriction and make the opportunity available to the community than not host  We think this is a better albeit imperfect outcome I understand and agree with frustrations around non U S citizenspermanent residents being ineligible for prize money  None of  Kaggle DHS or the community wants this restriction  The reason for the restriction is the  America COMPETES  Act  D H Ss legal team worked hard to find a way to allow international participation at all albeit without prize money while not violating the act  From  Kaggles perspective we had a choice host this competition with a geo restriction or not host at all  This was a tough choice  Hosting the competition meant running a twospeed competition where some are eligible for prize money while others are not  Doing this violates our desire for meritocracy where we offer equal opportunities to every community member regardless of education background and of course country  If we dont host then we dont expose our community to one of the most interesting and valuable datasets thats ever been hosted on  Kaggle  As weve seen over the past five years with the rise of deep neural networks the availability of datasets allows us to push forward to science of machine learning  Not exposing this dataset to our community meant depriving our community and the machine learning world more generally of a chance to push forward the science with a novel and challenging dataset  We decided that it was better to make the dataset available  Many in this forum have said this was a mistake  This was a challenging issue I hope this at least gives you a window into our thinking  Anthony kitefoil  James those speeds are a actually little slow I will upload my latest data soon but thats knots upwind and knots downwind  Ive improved with training  And there are kitefoilers who are much quicker than me  Rounak Banik nice dataset  Can it be updated to be current  If you can make it current and update this notebook I will send it to the TED team  We did this with  Marginal  Revolution and it ended up getting featured on their blog  Who knows maybe the TED team will be really interested and can highlight it in some way  Rounak apologies I missed that youd updated this I will send to TED  Fingers crossed they will pick it up  Kamil and  Hama Chi I actually dont think  Hama Chis expectations are too high  Having demanding users is healthy  It pushes us to improve our products  Were aiming to make  Kaggle  Kernels into the leading cloudbased workbench for data science not just a free compute environment  Were migrating onto GCP and making some architecture changes that we expect will make a big difference  Stay tuned  Anthony  We have spent considerable time discussing the situation internally here at  Kaggle and with  Zillow  We thought  Kaggle was putting sufficient emphasis on  Zillow’s eligibility criteria by posting it on the competition overview page  Its clear from this thread that this nonstandard eligibility rule was not sufficiently publicized to the community  As an acknowledgement of this we are making the following adjustments  We are reinstating the affected teams so they will be eligible to receive points and prize money  We are adding supplementary prizes if needed  These prizes aim to avoid penalizing teams that would have been in the top three teams if we didn’t reinstate teams who aren’t in compliance with the eligibility criteria  For example a team that’s in fourth place on the final leaderboard will be awarded nd place prize money if the nd and rd place teams don’t meet the eligibility criteria  Note the eligibility criteria for round two remains unchanged pasted below for convenience  We will soon be reaching out to everyone who places in the top teams to verify their employment or institutional affiliation to ensure compliance with the official competition rules  Anybody who works for a company that is referenced in the eligibility criteria will not be accepted into round two  If you were on a team with one or more members who are ineligible you will still be accepted to move forward into round two but the teammates who doesn’t qualify will not  In the future we will make a bigger effort to make nonstandard rules clearer posting them as a pinned forum post for example and inviting questions and clarifications  But please also remember that the rules are important  Ultimately it is each  Kagglers responsibility to read the rules before choosing to participate  Nobody wants a situation where  Kagglers are putting considerable effort into a competition that they are ineligible to participate in  Anthony  Eligibility  Criteria  Members of the following entities are not eligible to participate in either round of the  Zillow  Prize  Contest any commercial entity that engages in the sale valuation or analytics of residential or commercial real estate any entity that offers services in the leasing and property management space including vacation rentals and any entity that monetizes residential real estate related data  Officers directors employees and advisory board members and their immediate families and members of the same household of  Sponsor  Kaggle and each of  Sponsor’s and  Kaggle’s respective affiliates subsidiaries agents judges and advertising and promotion agencies  In addition you are not eligible to participate in the  Zillow  Prize  Contest if you are a a resident of a country designated as an embargoed country by the  United  States  Treasury’s  Office of  Foreign  Assets  Control see for additional information or b are an individual that appears on the  United  States  Treasury’s  Office of  Foreign  Assets  Control  Specially  Designated  Nationals and  Blocked  Persons  List see for additional information  Bojan totally agree that we dont want to incentive  Kagglers to hide personal information  In this case nobody will be removed from the round one leaderboard because of personal information they have shared  And everybody will have to volunteer their affiliations to be eligible for round two  That removes any discrimination based on publicly shared user information or  Wired  Magazine profiles  Competitions with eligibility criteria are quite rare so we havent figured out all the nuances  Cant promise we wont make mistakes in the future but we aim to keep improving  We spoke about that option in connection with the TSA competition  Its not clear that it was legal in that case  This question is moving into a more general discussion of eligibility criteria  If we want to move the conversation to a more general discussion of eligibility criteria I suggest we move it to general  The eligibility criteria still applies to entry to round  So only teams that meet the eligibility criteria will be accepted into round  Have you tried  Trueskill before  It should be more powerful because from memory it doesnt assume a fixed standard deviation so it takes into account the certainty about a rating when adjusting ratings after a game  Itd be interesting to compare the performance of  Elo vs  Trueskill  Theres a nice  Python implementation of  Trueskill  Love this story  Tenacity counts for a lot  Well done  Ryan  On  Monday  Numerai announced that they were giving away their cryptocurrency to  Kaggle users with a rating above  Novice and accounts created before  March  This wasnt done in partnership with  Kaggle we had no idea it was coming  Following that announcement there has been a massive increase in the number of login attempts to the  Kaggle website  These are attempts to break into  Kaggle accounts in order to claim the cyptocurrency airdrop  This is both stressing our systems and putting  Kaggle accounts at risk  As a result we have removed the ability add as your website URL forced a password reset for anybody whose website was set to sent an email to the  Numerai CEO letting him know that this has happened  If you find that your password has been reset please go through the  Forgot  Password flow  Anthony  Im always happy when the  Kaggle credential gives our community opportunities that they wouldnt otherwise have  In this case the way offer was structured effectively created a bounty for hacking  Kaggle accounts  And the fact that we had no notice meant that we couldnt think through the implications and prepare were going to look at changes to the  Kaggle login flow this week  Numerai has suspended the  Kaggle  Airdrop nathanforyou nice feature  Note  These community guidelines are replaced by revised guidelines available here  The  Kaggle community has a lot of diversity with members from over countries and skill levels ranging from those learning  Python through to the researchers who created deep neural networks  We have had competition winners with backgrounds ranging from computer science to  English literature  However all our users share a common thread you love working with data  As our community grows we want to make sure that  Kaggle continues to be welcoming  To that end we are introducing guidelines to ensure everyone has the same expectations about discourse in the forums  Be patient  Be friendly  Discuss ideas dont make it personal  Threats of any kind are unacceptable  Lowlevel harassment is still harassment  The  Kaggle team determines whether content is appropriate  If you see something that violates these guidelines you can bring it to our attention using the flag option on messages and topics  If you have a serious concern you should report it to supportkaggle com  All reports will be kept confidential  Sad to share that  Kaggle  Master  Vlado  Tomecek passed away on  Sunday  October I received the note attached above from  Dana  Vlados sister I remember sitting in on his call with  Draper  Labs the competition that he won  His solution involved a tremendous tenacity and creativity  In our exchange  Dana mentioned that data science was  Vlados passion and main focus  On behalf of the  Kaggle team we wish  Vlados family well  And we will miss his presence in the community justinminsk this is nice FWIW I suggest updating the title to something like  Identifying the best college wine  The kernel is more fun than the title suggests  Faraz has this been address  If not send me a private message with a link to the comment and  Ill make sure we take a look ASAP  I’m a longtime competition lurker whos finally mustered the courage to join a competition 😅 I picked this competition because I find the topic interesting if I was going to college today  I’d probably pick biology  And also as a chance to try out  Google  Auto M L  Vision  Kaggle is part of  Google now which gives me exposure to some of the technologies that are generating excitement inside  Google  Auto M L is a tool with a lot of buzz I hadn’t paid much attention to the  Automated  Machine  Learning  Tools  A M L Ts until the  Kaggle Days competition in  San  Francisco this past  April  Two  A M L Ts  Google  Auto M L and H ended up participating in the hackathon and getting top performances here’s the  Google AI writeup of their performance  While I believe it requires human creativity to do problem setup and domainspecific feature engineering  Kaggle Days left me wondering whether humans need to be doing generic feature engineering architecture selection picking activation functions and setting learning rates I plan to share details of my experiments and experiences with  Google  Auto M L  Vision in this thread  I’m also open to suggestions from others in the community for things I should try  But bear with me if I take sometime to respond like many other  Kagglers I have a busy day job  First  Few  Experiments I started with the images created by xhlulu’s kernel thank you xhlulu  In that kernel xhlulu converts the data to x RGB jpegs  By default  Google  Auto M L randomly splits between TRAIN VALIDATION and TEST  And the predicted outputs labels and confidence scores for each image I define a threshold and it outputs all labels and confidences above that confidence threshold I made predictions for both sites and then picked the label that had a higher confidence score across both sites  Below is a table of my scores  Note AUC is the accuracy metric that  Google  Auto M L  Vision reports  Haven’t put the time into figuring out what it means but I assume it’s probably treating each class as binary onoff and then aggregating up  Training  Hours  Resolution  Number of  Images  Extension  Auto M L AUC  Public  Leaderboard  Score jpeg jpeg jpeg jpeg I then made small modifications xhlulu’s code to create x RGB pngs to test the impact of higher resolution images  Training  Hours  Resolution  Number of  Images  Extension  Auto M L AUC  Leaderboard  Score png png png png png is my best score  It’s a result of a poorly thought out experiment so  I’m surprised it’s my best score I trained a model for hours that included the control images  But after inspecting the testset predictions I realized my model was often picking labels which are control labels that don’t appear in the test set I ended up filtering the testset predictions to go to the next most confident label when a label was predicted  Training  Hours  Resolution  Number of  Images  Extension  Auto M L AUC  Leaderboard  Score yes png yes png yes png yes png yes png yes png  Google  Auto M L  Vision allows me to define my own TRAIN VALIDATION and TEST splits  For my next experiments  I’m going to start using this feature  I’m going to define my own splits to  Include the control data in TRAIN but not VALIDATION or TEST to allow the model to use the control data for training but to discourage the model from predicting labels on the competition test set  Split by experiment rather than randomly  Experience with  Google  Auto M L  Vision  I’m getting pretty decent results considering how naive my models are  I’ve found  Google  Auto M L easy in many ways  To get a basic model all I need to do is upload the RGB images to  Google  Cloud  Storage upload a CSV file with a pointer to the GCS bucket and the target label  There are a bunch of limitations with the products  My biggest issues so far have been inability to add other metadata e g would like to be able to add metadata on controls plate id and position on plate I can probably get around this using a postprocessing step but it’d be nice to be able to add this metadata into the single model there isn’t a feature that allows batch prediction on a large number of images I work around this by hitting the prediction API times to generate my submission file the model evaluation page is not very helpful for debugging model performance I had to use a lot of hours of training to get good results you can see that the models keep improving with additional training time  This means I have to wait a long time and spend a lot of money can’t change the loss function  Based on the forums it seems like the loss function might be an important setting for this competition  There are some more minor frustrations that I had to workaround which  I’m happy to share if others are planning to try it out and want to learn from some of the frictions I encountered lkhphuc unlikely you need anywhere near as many training hours if youre not using  Auto M L  The upside of  Auto M L is that you dont have to set any hyperparameters and still get a decent model  One of the drawbacks is its so compute intensive xhlulu I actually regret converting to png  It would have been a purer comparison if  Id also used jpeg I tried running the px conversion in a kernel but I didnt have quite enough compute time  Let me find out how I can share the px dataset with everyone xhlulu unfortunately cant share the dataset through the datasets platform  Thatd allow users to access the dataset without having to accept the competitions rules apap  Im picking the site that has the highest confidence score across the two sites  For my best model LB my predictions for site and site agree of the time  Not sure  Sorry zaharch sorry for the delay I wanted to see if  Auto M L could be run out of  Kernels  Turns out it can  The hour limit for kernels is not an issue because my kernel kicks off a hour training job and then stops  My next step is to share my inference code which sends the test images to the trained model and generates predicted labels  Auto M L doesnt give architecture or hyper parameter recommendations  It just creates an end point that you can send images to and get back predicted labels and the models confidence in that label ttylacm dont know what hardware  Google  Auto M L is using under the hood  Its invisible to the user  Good chance its using  T P Us though ratthachat thanks  Looks like youre a few places ahead of me on the leaderboard  Watch out I have a few new ideas to try 😏  Oops I made a change that introduced an error in version  Version works  Im currently committing a version which should work as well OK now also added the code for doing inference using  Google  Auto M L I didnt do my strongest model to avoid messing with the leaderboard I picked a model that performs a little below to strongest performing kernel  To change models all one needs to do is change the modelid ankitkp giuliasavorgnan and jiangkun improving reliability of kernels is a big push for the team I shared this thread  One question that came up are you all using  G P Us  Or just  C P Us jiangkun I dont follow  What does that mean  Managed to jump to by exploiting the structure of the structure of the data mentioned here zaharch  Im really interested in seeing this as well I am curious to try  T P Us sometime and would be interested in summary of your experience  Also curious if you were able to realize a performance speedup when using  T P Us  Nice  Thanks for sharing  Really love this writeup  Particularly how you step through the different things you tried and how they mapped to improvements in your score  Very easy to follow deep there is a mistake in the dataset  San  Francisco is listed as square miles  When its actually square miles  Also itd be helpful to have a description of the data including where it came from  And to have the  Area column as an integer or float rather than a strong I uploaded a dataset of doctors and nurses per capita for countries from the OECD  Im going to add a pointer to the recovered cases dataset in the pinned dataset thread cpmpml pointed out that having the number of recovered cases could be helpful  Just pointing out thats available in this dataset already shared by sudalairajkumar  Nice I would havent thought of this dataset  In case its helpful to compare with past pandemics SARS dataset  Ebola dataset MERS dataset ht sudalairajkumar I found these datasets from one of his  Tweets I suspect its pretty easy to get on a countrybycountry basis  Question is whether theres a good global source BTW trying to keep this thread relatively clear for actually datasets  Were using this thread to ask questions and discuss ideas for datasets  Nice idea but a better fit for discussion in this thread sasrdw this dataset seems to have a decent amount of what youre looking for  That version is three years old so may be worth updating  Although at a glance many of the measures are updated pretty infrequently  Pretty sure this is the weather data many of us have been looking for to look at the impact of temperature and humidity on transmission rate  This looks like a nice version of that dataset  Added it to the data sharing thread davidbnn posted a dataset of global weather conditions at I wonder if  Google  Trends data might be interesting  Perhaps searches for hand washing or hand sanitizer in a particular city might correspond with a lower transmission rate  Or even just searches for COVID in a particular city might correspond with a lower transmission rate indicate people are taking the virus more seriously in that city hannesmarais  Im not an epidemiologist but at a glance this looks really impressive I had a quick look and saw a few places where your system seems to be answering the prompt  This seems to give an answer to the persistence on surface question  This seems to give an answer to the immune response question  Are there many places where you think your system is accurately answering the prompt  Are you in a position to give a bit more background on what your system is doing savannareid I would also be really interested  At a glance and to my untrained eye hannesmarais system looks really impressive jaimeblasco uploaded one for school closures I assume country level data is still likely useful  If you can match the files in that dataset I can likely update it on their behalf  Otherwise I suggest you just create a new version  Nice  Mind also sharing it in this thread  Nice to have all public datasets in one thread  As I understand it this has been an influential preprint showing the relationship between temperature and humidity and transmission rate  It looks at  Chinese cities and finds that a degree celsius increase in temperature leads to a drop in R  And a increase in relative humidity leads to a drop in R  As I understand it R is a a commonly used measure by epidemiologists known as the effective reproductive number and is the average number of secondary cases per infectious case  If R is greater than then an epidemic will spread  If its less than it will die out  My understanding is that the estimate of R for COVID is in the range of cpmpml nice dataset  Do you mind adding a pointer here  Want to have all external datasets on the same thread  The  Economist did an analysis of tourism flows tofrom  China a few weeks ago  Could look at the dataset they used as a starting point koryto this is a great direction  Makes a lot of sense to start joining a lot of these datasets into a table absolutely davidbnn this is great  Will save a lot of users a lot of pain and makes it easier to explore the impact of weather  Nice work nightranger nice  Starting to join datacreate feature matrices is a really nice way to make the datasets surfaced in this thread more useful koryto one direction you can take is trying to combine as many of the datasets mentioned here into your feature matrix as you can make fit  Nice  Glad to see that larger and larger feature matrices being built up  Agree completely with  Pauls guidance  Big goal is to product findings easy for the medical and health policy communities to digest  This is a nice notebook  Itd be helpful if you also linked to the paper as well as providing the excerpt this is very useful  Can you use your approach to look at words like weather temperature and humidity  That is shown to have an impact on transmission rate e g referenced here  Itd be interesting to see what your approach turns up ajrwhite this is very useful I find key phrases most useful part of your notebook  Few requests  Can you print more key than key phrases when your search returns more results e g chronic respiratory diseases can you print key phrases for additional you look at risk factors e g hypertension  Can you use your approach to look at words like weather temperature and humidity  That is shown to have an impact on transmission rate e g referenced here  Itd be interesting to see what your approach turns up  Getting estimates for R and R from the literature would be great  One problem with those snippets is it doesnt have the actual numbers  We have been asked by our health policy collaborators to put together a summary page that surfaces the most useful findings from the  Kaggle community  The research on COVID is moving quickly making it hard for virologists and the health policy community to stay on top of the latest  They are working long days and need information presented in as clear and concise a format as possible  We’ve formatted the summary page to be digestible to them  The page is currently organized into three sections findings tools and datasets  Im going to focus on the findings and tools sections since theyre most relevant to this challenge  Findings map to the ten tasks issued by the  White  House  Office  Of  Technology  Policy addressing open questions about COVID  Our goal is to help answer as many open questions as possible and represent the state of knowledge on each of those questions  Findings should be focused concise extract quotes and numbers out of papers and also provide a link to the underlying source  It’s helpful if you can produce findings that map as close as possible to the format of whats currently presented on the summary page  Some of the most impactful work so far have involved simple methods like string matching and regular expressions  For those working on tools  Google  Scholar and AI  Semantic  Search are already mature products  If you are building a tool make sure your tool adds value beyond those products  If you have contributions that you think we should be highlighting on the summary page please email me at akaggle com or share on this thread  Also feel free to ask any clarifying questions on this thread  Most of the datasets have come from the forecasting challenges  However for those interested  Ill provide some guidance on what datasets are helpful  Its useful to share individual datasets with promising signals  Its even more useful if you can join to other relevant datasets that virologists and the health policy community can analyze  And by all means use any datasets to try and produce findings  If you do make useful findings please document them in a clearly written notebook thats easy for others to follow  We have been asked by our health policy collaborators to put together a summary page that surfaces the most useful findings from the  Kaggle community  Virologists and the health policy community are working long days and need information in as clear and concise a format as possible  We’ve formatted this page to be as easily digestible to them as possible  The page is currently organized into three sections findings tools and datasets  Im going to focus on datasets and findings since they are most relevant to this challenge  Datasets  So far the forecasting challenge has done a nice job of surfacing potentially useful datasets  The most valuable contributions have been joining those datasets to create valuable resources for testing which factors impact transmission  Shout out to davidbnn who joined each region in the  Johns  Hopkins  University data to the nearest weather station  Findings  These clearly written notebooks that help show which factors have an impact on transmission rate  Calls to action create enriched datasets that would allow researchers to easily test which factors impact transmission  These datasets should be well documented and kept updated write simple notebooks that show the impact of factors on transmission useful whether or not you find a correlation  If you have contributions that you think we should be highlighting on the summary page please email me at akaggle com somethingkag this is not a template but ajrwhites notebook does a really nice job addressing a handful of the subtasks  It directly addresses the subtasks  The informationdata is very clearly presented skylord nice  Are you updating this daily  If so  Ill include it  Franck you should absolutely be open to looking at external data sources to answer the questions  One nice thing about starting with the papers is we build up a picture for what has already been studied and where the gaps are  No  This is not a supervised machine learning competition  We are hosting a supervised COVID machine learning competition at ajrwhite really nicely said  Ken added to summary page  Thanks nofoosports  At a glance your notebook looks really nice  Likely have some folk with medicalpublic health backgrounds starting to help out with curation from tomorrow so planning to take a close look with them  Ken thanks for grouping the notebooks by task  Makes them much easier for us to process  Thanks  Mike  Will take a look today  Just added a challenge for sharing useful COVID related datasets  Motivation for adding that challenge is that a lot of datasets shared in this thread a are really useful b potentially less relevant for the forecasting challenge  We wanted to create a specific outlet for all COVID related datasets remanuele please add as a  Kaggle dataset  That makes it easily accessible to those writing notebooks  For those interest  Safegraph have a really interesting human movement dataset  Its location data from millions of anonymized smart phones  Its currently on AWS but they can move it to GCP to make it easier to use from a  Kaggle notebook if theres sufficient interest  You can get access by filling out this form I received quite a bit of feedback from the healthcare and health policy communities about our page summarizing community contributions  As well as including the title of the paper itd be helpful if we also shared the name of the journal in the summary tables  The name of the journal is a proxy for the the quality of the paper a result published in JAMA or  The  Lancet carries more weight  More challenging is to also classify the type of evidence that a study is based on  Theres a hierarchy of evidence and its helpful to know what evidence was used to draw a particular conclusion  Here are a few references that explain the hierarchy of evidence ajrwhite can you update this notebook to include the new data dump  Also is it easy for you to add the journal name to the article title column per this guidance  If so itd be great if you could put include it  If you can  Id format itd be great if you could put it outside the link to make it easier to distinguish from the title  Persistence of coronaviruses on inanimate surfaces and their inactivation with biocidal agents  The  Journal of  Hospital  Infection mikehoney a really nice tool for exploring the data but doesnt directly give me the answers to specific tasks without me choosing my own filters mlconsult the  Chloroquine section had mostly relevant articles so added it to the summary page under  Effectiveness of drugs being developed and tried to treat COVID patients mlconsult really appreciate you doing this  It makes our job of curating much easier hannesmarais I have been meaning to go back through your system and see how its been performing since you made improvements  Are there questions that it does particularly well on  If so which ones  Also can you explain the difference between the question sets original  Kaggle vs extended  Kaggle from  Savanna  Reid skylord added here BTW you may want to crosspost to this challenge  Wow  This is impressive  Itd be great if you loaded it as a  Kaggle datasets  Just noticed paultimothymooney already has it on  Kaggle  This seems to be an important paper  Is it in the CORD dataset zohrarezgui nice idea to pull down clinical trials data  But the dataset has no information on the therapy being trialed though  Is it possible to pull more information about the nature of trial mlconsult nice added  Can you pull how large the difference is between and  Also what does relscore mean mlconsult couldnt find a task that this addresses but included on the contributions page because its come up as an open question recently nofoosports this is great  Ive started adding your answers to the contributions page A couple of things that would be helpful start ordering by date I like to present results chronologically on the contributions page also helpful if you can start breaking out answers to subtasks a little more  For example for this subtask theres  Range of incubation periods for the disease in humans and how this varies across age and health status and how long individuals are contagious even after recovery itd be helpful to have papers and excerpts that just answer  Range of incubation periods for the disease in humans how this varies across age and health status how long individuals are contagious  Youll notice  Im starting to break up even the subtasks on the contributions page to make them more digestible  This is feedback were consistently getting our audience is very busy and want things as scannable as possible  Another request its even helpful if you can break out subtasks into several components  As an example take this subtask  Range of incubation periods for the disease in humans and how this varies across age and health status and how long individuals are contagious even after recovery itd be helpful to have papers and excerpts that just answer  Range of incubation periods for the disease in humans how this varies across age and health status how long individuals are contagious  Im starting to break up even the subtasks on the contributions page to make them more digestible  The feedback were consistently getting is our audience is very busy and want things as scannable as possible I just shared this feedback on nofoosportss excellent notebook but wanted to make it more broadly visible  Also a nit but you have grouped two subtasks accidentally  Prevalence of asymptomatic shedding and transmission e g particularly children and  Seasonality of transmission csheesun were typically finding results easier to read when contributors are submitting one notebook per task  We had another call today and received another round of feedback  First off were getting good feedback that is heading in a really helpful direction  The big issue is still that theres limited signal on how strong the underlying paper is  Since COVID is so new and most articles are in preprint journals med Rxiv and bio Rxiv that doesnt give much signal I think this is consistent with what savannareid has been saying  They are interested in whether we can mine other signals from the dataset I think were going to get more feedback in the next week  Ill provide it when I get it but an example that came up was pulling out the number of participants in a study the more participants the more weight should be given to a study for eg  This didnt come up on the call but I was also wondering about authors previous publication record might be a useful signal  Would love to add a column thats a proxy for publication quality by the end of the week if anyone can crack the code on that davidmezzetti started a discussion thread on the topic so suggest the conversation continue there  Another bit of feedback we received is that the health policy and medical communities are interested in tools that might help them get to answers more quickly themselves  The dream tool is a questionanswering engine that can reliably answer freeform questions  But theres interest in seeing other tools as well that might go beyond what things like  Google  Scholar and other existing search engines can do  If anyone builds a tool that they think meets this criteria please either share it here or email me  Wow amazing you got this done so quickly  Ultimately our goal is to be as useful as possible to health policy decision makers  So you are free to search  Pubmed if you think itll improve your results davidmezzetti would it be easy to add the h Factor of the last author  As I understand it the lastauthor is the corresponding author and the credibility of the study rests on their shoulders  That could be another helpful proxy davidmezzetti ignore the hfactor suggestion  Ive just run it past a few people and the feedback were getting is that its probably not such a useful metric  Were getting feedback that some measure of sample size would be really valuable in conjunction with the level of evidence metric  The kind of guidance we were given was  For level I either the number of studies or the total number of participants across all studies  For level II the number of participants in the randomized control trial etc  Itd be great if you could take a swing at this amazing youve made such quick progress  Look forward to tomorrows update A recent paper suggests a potential link between transmission and air pollution  Paper suggests that explains regional difference is  Italy between transmission rate mlconsult thanks for sharing paultimothymooney owns looking at tools being developed  Hes going to take a look at this  If he thinks its promising itll definitely need to be stood up as a web app  Feedback were getting is many of the end users are not technical enough to fork a notebook  Nice work mlconsult paultimothymooney as an FYI this is now a web app  One of the main bits of feedback were getting about the results were presenting is that decision makers would like to better understand the strength of the evidence in a study davidmezzetti has made a really nice start by including level of evidence I spoke with  Stelios  Serghiou and  Byron  Wallace  Byron actually uses NLP to assess the quality of clinical trials  In speaking to  Stelios and a few others  Im getting the feedback that pulling out the underlying study design might be even more helpful than level of evidence  For each study type there are measurements that can help proxy the strength of the evidence  Below is a table that attempts to map study type to measurements  These are the things that  Stelios looks for when hes assessing the quality of a paper  Itd be great if you can attempt to extract study type and the corresponding measurements to indicate evidence strength  This applies to those presenting task answers in tables as well as those who are building searchquestion answering tools  Level of  Evidence  Study  Type  Measurements to indicate evidence strength I  Meta analysis I heterogeneity and tau squared heterogeneity number of studies II  Experimental  Studies  Randomized  Randomization  Sample size  Loss of follow up  Length of follow up III  Experimental  Studies  Non Randomized  Randomization  Sample size  Loss of follow up  Length of follow up III  Observational studies  Prospective  Cohort  Sample size  Loss of follow up III  Observational studies  Retrospective  Cohort  Sample size  Sampling method how was the sample captured III  Observational studies  Case  Control  Sample size  Selection bias for controls when controls come from a different population from cases  Observational  Crosssectional  Sample size  Sampling method how was the sample captured  Temporality did the exposure e g COVID come before the outcome e g hospitalization desired or after undesired IV  Observational  Case series case study  Note savannareid pointed out that this doesnt include modeldriven results like those used to estimate things like reproduction rate  For modeldriven results measurements of evidence strength are likely simple size some measure of model fit and some information on where the underlying data was drawn from aravindmc nice work  Weve listed it on the contributions page  As  Paul said were getting a lot of feedback about including measures of the strength of evidence  Shared some of the feedback we are hearing here  Itd be great if you could extract and include some of that metadata in your search results arturkiulian absolutely  The overall objective is to product something useful to the healthcare and health policy communities  If those things help you produce more relevant results then please do use them ajrwhite and davidmezzetti and others looking to contribute the the contributions page savannareid hand coded the design and sample size for all the risk factors you can see what it looks like here  She also reformatted the tables  Can you try and pull update your risk factors notebooks for the new data dump  And can you try and pull out design and sample size  Also if you can follow this table format we can dump it directly onto the page we now have a more automated process for generating the tables  Date Study Study  Link Journal Severe Fatality Design Sample  Hi all  We now have a team of medical students helping to curate the results that go on the contributions page  Theyve been reviewing the results of notebooks that focus on the transmission incubation and environmental stability task as a starting point  Each student owns curating a table for each of the subtasks  The goal for these reviews is to to come up with a standard format for those subtasks just like savannareid has done for risk factors  This will give you a target format to produce your results in give feedback to authors of notebooks that are reviewed on how to make your algorithms more relevantaccurateuseful  The goal is to continue to populate the contributions page which is gaining a nice following attractive K unique visitors in the last week up from the week before  And when we have a meaningful amount of coverage and a solid process for keeping the page uptodate as the new papers come in with less manual curation than we are currently doing were going to aim to start publishing the updated literature review with a leading medical journal so that its more visible in the medical community  Encourage those of you who are interested in contributing notebooks in the format helpful for the contributions page to join the  Slack channel with the team of medical students  Here is an invite link  Im going to attempt to pass on feedback  Im hearing in this thread  But joining that slack channel gives you more direct and interactive way to hear from the experts who are reviewing your work  Finally if you want to see the curation process in action and the notebooks that are being reviewed you can check out this  Google  Doc  Please let me know if you have a notebook you want reviewed for a particular subtask and I can add it to the queue the queue is maintained on the MASTER SHEET tab  Some consistent themes in the feedback so far often algorithms are pulling snippets from the abstract  When the key number finding or result is in the full text of the article with the summary in the abstract being too high level to be useful often algorithms are pulling snippets that are a reference to work from another paper rather than work that is core to the paper being cited  This challenge is getting the attention of the health policy and medical communities who are working night and day to better understand COVID  In  February COVID papers were published per day in  March that increased to by mid  April that number is up to  It is very difficult to stay up with the literature  Weve been taking some of the most promising notebooks and working with a large team of volunteers epidemiologists  M Ds and medical students to turn those results into a regularly updating literature review  You can see the work in progress here  Now that we have a clearer understanding of what this needs to look like were calling on our community to more directly feed this effort by outputting results in a standard format  All notebooks should output CSV files in the formats listed in the task descriptions  How you can contribute  Read the instructions within each individual task and make a submission  We also encourage participants that have made submissions and want feedback to join this  Slack channel  It includes the domain experts who are curating the tables  This challenge is getting the attention of the health policy and medical communities who are working night and day to better understand COVID  We have heard feedback that an  A Ipowered literature review is a powerful way to synthesize new research I have started a new thread outlining how you can feed into an  A Ipowered literature review  That thread aims to be a clear and direct way you can make contributions that will have impact I am unpinning this thread to direct conversation there mlconsult looks like youve done a really nice job of pulling out sample column  Nice one  Other than that column  Im not seeing a close mapping to the table format mentioned above  Am I missing something  Oops my bad  Read too quickly dirktheeng weve been chatting with kylelo and team about the importance of having tables  As paultimothymooney mentions theyre working on approach to making the tables machine readable  Do you think you could reliably take images of tables and turn them into  Pandas  Dataframes  Defer to kylelo but that may be a way to get you tables sooner  Anthony  Nice update  One change that would be helpful is if you can break out  Study  Study  Link and  Journal into three separate columns per this guidance  That will result in less manual curation to get the results into the format for the contributions page that was quick  Updated the target  C S Vs with additional tables from the  Transmission  Incubation and  Environmental  Stability task sapal I just tried it from  Incognito and it worked for me  And others have been getting in  Perhaps you hit it during a temporary outage  Suggest trying again sasrdw thanks for starting this thread david any sense for how the IMHE model would have performed in these competitions  Curious where would it have placed  Made another update to the target  C S Vs changes to risk factors removed TB changed BMI to overweight added an extra two columns to break out the results column and added extra data changed to TIE cleaned up the seasonality tables added extra data fixed issues found via error checking  Wow  This is really nice  Interesting that the  Kaggle community keeps getting stronger vs IHME I guess there are three possible explanations the  Kaggle models are getting better as more data comes in more empirical and less theoretical than IHME the  Kaggle models are getting better because people have more time to work on them IHME are better at forecasting further out  Sounds like you believe its david  Also really curious about week is the trend continuing or are we plateauing vs IHME  Assume you left it off because we dont have enough data for week yet david funnily enough I was just looking at his dashboard I assume theyre all outputting data in a standard format if hes able to put them on the same dashboard  Is it easy for you to benchmark the performance of those other models as well  Were trying to figure out whether the top  Kaggle models are likely adding value beyond the standard epidemiological models  Were considering launching a longer running competition and combining top performing  Kaggle models into an  I H M Estyle dashboard  For interest section of this paper discusses the metrics used by the epidemiological modeling community A nudge we still have tables that need to be claimed  If you think its something you can help with please see the relevant list of tasks I did some more benchmarking I compared the current leader for week and with IHME and LANL models across three loss functions MAE RMSE and RMSLE  Im only comparing the forecasts for fatalities for US states  In week and the  Kaggle leader clearly wins on RMSLE  But performs a lot less well on the other loss functions cpmpml regarding your comment about the  Kaggle comp optimizing different things the same is probably true for IHME and LANL  On MAE and RMSE it looks like you basically get good scores by getting  New  York and maybe  New  Jersey correct they account for of fatalities  And getting  New  York right involves predicting a data revision NYC revised their fatalities measure to include probable COVID deaths on  April  This revision started included people who died but never received a COVID test but had  C O V I Dlike symptoms  This chart comes from inversions excellent notebook that shows charts for LANL IHME and two  Kaggle ensembles on a state by state basis cpmpml the basis for the claim that getting  New  York right is most important comes from the observation that  New  York accounts for of the total absolute error for the kagglelead and IHME models so that state is driving most of the error I havent had a chance to take a close look at whats happening with your model  Your numbers are really impressive across the board unfortunately I have to return to my day job now though this was a small weekend project that I did during my daughters nap david I bet if you asked IHME and LANL if the benchmarking is fair theyd also have complaints as well optimized for a different loss function etc  Nonetheless I was curious to see what would happen if I benchmarked across a range of metrics that the models didnt necessarily optimize for I was hoping it might tell me something about the robustness of results  To explain why this setup LANL only does forecasting for US states so needed to restrict to this subset to include them  And LANL and IHME model updates dont perfectly correspond with our competition dates made another update to  C S Vs  Covers the new data and adds a bunch of new columns  Nice presentation  Thanks for sharing  Ive put together got a notebook that benchmarks the performance of professional epidemiological models and compares them with a strong  Kaggle model I was curious to keep track of how the  Kaggle models compare with the professional epidemiological models  For now  Im only tracking one  Kaggle model the previous weeks leader to avoid making a selection expost  So the week model  Im using is one of sasrdws and davids models  Its currently doing really well  Although interestingly its making very different forecasts from the professional epidemiological models I have created a benchmark panel that includes actuals and the forecasts from different models at different points in time  Heres the notebook that generates the benchmark panel  Encourage others interested to play with it either add one of your models or play with some  Kaggle ensembles can download submission files from public notebooks mrisdal you are dominating enough leaderboards I need to pick up my game  Maintaining this thread to describe known issues  If you post an issue its helpful if you can link to a notebook that is prefixed the the title ISSUE  The CONVENIENT files have negative daily case and death counts in some places  The RAW files are cumulative case and death counts  The CONVENIENT files the diff of the RAW files  Heres a link the notebook demonstrating the issue with some of my early attempts to understand it  Please add any requests you have for this dataset in this thread  If any requests get a number of upvotes indicating some amount of broader interest I will attempt to address the request  Add data in the JHU daily report files for US counties and the global file  For global file this includes  Incidence Rate and  Case Fatality Ratio  For US county file it includes  Incident Rate  People Tested  People Hospitalized  Mortality Rate  Testing Rate and  Hospitalization Rate  Include population by country in the global metadata file  Remove provincestate from the country level data to make it more convenient to process  Yes  And also means you can schedule a notebook to run on a recurring basis xhlulu I used  G C Ps  Cloud  Scheduler combined with the  Kaggle API  But a few people have asked about scheduling functionality so we are considering adding it as a feature  If this is interesting to you itd be great if you could contribute to the discussion thread that mrisdal started earlier today fvcoppen I want to add additional data sources over time  So if you find good data sources you think I should add please let me know sagnik have you seen this post I describe the issue there  Let me know if you have suggestions for resolving it maithiltandel it looks like a nice dataset  In the dataset description itd be helpful if you explained more about the dataset itself rather than the findings  For example where the dataset comes from a link to the raw source perhaps a link to the notebook you used to transform it  Looks like a great dataset can you add documentation  Would love to know more about the dataset where its from what it can be used for etc sansuthi I  Tweeted the dataset  Do you have a  Twitter account you want me to link to in a thread on that  Tweet mohit fyi think link you posted is broken I think this is the link you meant to post works if you copy the text version clicking the link URL takes you to  Have an issues with the dataset that you want resolved  Requests for additional metrics that should be added  Please add your suggestions to this thread kaviml thanks for the lovely post  Its really motivating for us to receive comments like this surajjha in replying  Im trying to compete with herbison to be the best among the staffs 😂  Jokes aside thank you also for your nice words  To the  Kaggle community  Today were sharing that D  Sculley will be taking over as  Kaggle’s new leader alongside other ecosystemfocused machine learning efforts  Ben and I are leaving  Kaggle and  Google for our next adventure going back to our startup roots  Kaggle recently passed its year anniversary  We started with a lighthearted competition aimed at predicting the voting matrix for the  Eurovision  Song  Contest in  Back then it would have been hard to imagine that  Kaggle would play a meaningful role in the future of machine learning and AI  There are several aspects of what  Kaggle has achieved that we are really proud of  First and most importantly the impact on peoples’ lives  Many have learned machine learning through  Kaggle  Of our almost MM users MM have submitted to the  Titanic getting started competition and almost MM have completed exercises from  Kaggle’s courses K college courses have run classroom competitions with K students submitting to those competitions  And it’s not just those new to machine learning who use  Kaggle to learn  Advanced users get handson experience on lots of different problems and the opportunity to learn from the winning solution  As  Grandmaster  Vladimir  Iglovikov likes to say “I think about machine learning competitions as about a gym but for a machine learning muscles”  Kaggle has also provided a credential to the machine learning world  We have given those who are sufficiently determined another way to break into the field  Already back in  Facebook was using  Kaggle to find strong machine learning talent  By we were well established as a great way to land an elite AI job  Today wellregarded AI companies like  N Vidia and H ai hire teams of  Kaggle grandmasters  We’re also proud of the role the  Kaggle community has played in highlighting what works well in practice machine learning papers per day are published on  Arxiv and countless machine learning packages are developed  Kaggle users explore these techniques in a competitive environment and spread those that work  Frameworks like  Keras and  X G Boost took off in the  Kaggle community along with useful preprocessing and data augmentation libraries like albumentations for computer vision  Many techniques have spread through  Kaggle including  U Nets for segmentation denoising autoencoders and adversarial validation  And  Kaggle has helped prove out new applications for machine learning including medical imaging and automated essay grading  And finally we’re proud of the fact that while we started with machine learning competitions we’ve launched other services  Notebooks have increased the ways our users can share learnings and our courses have made  Kaggle more approachable to new users  And there’s no machine learning without data and we’re proud of our collection of over K public datasets which makes  Kaggle one of the world’s largest repositories of public datasets  Of course none of this would have been possible without both the community and the  Kaggle team past and present  Over the years we have had the privilege of meeting so many passionate and energetic community members with so many inspiring stories  And the opportunity to work with a talented and motivated team  We’re grateful to all of you  And while we are proud of what  Kaggle has achieved so far we’re also very excited to see what the D and the team accomplish in the years ahead D has been operating on the cutting edge of machine learning for the past years working on very large scale machine learning systems inside  Google doing foundational research on topics like  M L Ops and leading large research teams D also has a long history with  Kaggle starting with the semisupervised machine learning challenge he launched in which as it happens was won by some familiar faces  We’re excited what the combination of D ’s background and history with  Kaggle will bring to the future  For those who want to get to know D better he is going to be answering the most upvoted questions over on this post  Anthony and  Ben'"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "forum_data_agg['clean_messages'][1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "#function to stem words\n",
    "def stem_text(text):\n",
    "    stemmer = SnowballStemmer('english')\n",
    "    words_list = text.split()\n",
    "    new_list = []\n",
    "    for i in words_list:\n",
    "        word = stemmer.stem(i)\n",
    "        new_list.append(word)\n",
    "        \n",
    "    words = new_list\n",
    "    words = ' '.join(words)\n",
    "    return words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data['clean_posts'] = train_data['clean_posts'].apply(stem_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'im find the lack of me in these post veri alarm sex can be bore if it in the same posit often for exampl me and my girlfriend are current in an environ where we have to creativ use cowgirl and missionari there isnt enough give new mean to game theori hello entp grin that all it take than we convers and they do most of the flirt while i acknowledg their presenc and return their word with smooth wordplay and more cheeki grin this lack of balanc and hand eye coordin real iq test i score internet iq test are funni i score s or higher now like the former respons of this thread i will mention that i dont believ in the iq test befor you banish you know your an entp when you vanish from a site for a year and a half return and find peopl are still comment on your post and like your ideasthought you know your an entp when you i over think thing sometim i go by the old sherlock holm quot perhap when a man has special knowledg and special power like my own it rather encourag him to seek a complex cheshirewolf tumblr com so is i d post not realli ive never thought of ei or jp as real function i judg myself on what i use i use ne and ti as my domin fe for emot and rare si i also use ni due to me strength you know though that was ingeni after say it i realli want to tri it and see what happen with me play a first person shooter in the back while we drive around i want to see the look on out of all of them the rock paper one is the best it make me lol you guy are lucki d im realli high up on the tumblr system so did you hear about that new first person shooter game ive been rock the hell out of the soundtrack on my auto sound equip that will shake the heaven we manag to put a coupl pss in no the way he connect thing was veri ne ne domin are just as awar of their environ as se domin exampl shawn spencer or patrick jane both entp well charli i will be the first to admit i do get jealous like you do i chalk it up to my w heart mix with my domin w s and s both like to be notic s like to be known not the same d ill upload the same clip with the mic away from my mouth than you wont hear anyth ninja assassin style but with splatter tik tok is a realli great song as long as you can mental block out the singer i love the beat it make me bounc drop io vswck d mic realli close to my mouth and smokin ace assassin ball play in the background sociabl extrovert im an extrovert and im not sociabl sherlock in the movi was an entp normal hes play as a extj in the book hes an estj as i said the movi look good except for it be call sherlock holm oh i never had fear of kiss a guy i will kiss an anim too so there was noth to vanish just person tast and me not like it the guy i kiss didnt know me it was one of those sound pretti much like my area and what im go through right now tri to figur out which way i want to take my life i want to do so mani thing the biggest problem is that i know if i dont d i was oper under the impress that you were femal i never look at your boxi okay i help out my gay friend all the time and one of them has develop a littl crush on me i get red tt you just describ me and im live the worst nightmar im trap in one place with one one around onli dull wood if i was a serial killer this would be the perfect place but sad im tbh and bias sound like a shadow infp i think mayb he was hurt and turn estj i can tell becaus he has some of the typic infp trait left over check list im sorri it seem that you have came at a bad time weve alreadi reach our quota of infj howev be your femal and i like femal i will make you a deal i will kick one im antp lean toward e im easi for both entp and intp to identifi with i also imagin entp interrog would go a littl bit like jack from except more mechan rig up shock treatment equip in an abandon build out of an old car batti jumper it was a compliment trust me im just as psychopath d except i have emoticon theyr just weird one like laugh when i get hurt or at peopl run themselv over with their lawn mower no it like a theme for where i live and that is whi i know it by heart and i usual dont leav until the thing end but in the mean time in between time you work your thing ill work mine d d im the mbp pleasur to meet you damn need to trust my instinct more i would have been closer i was go to say infp exfp lean toward s with the way she respond d my friend even my gay and lesbian one alway come to me for advic i bow to my entp master entp are so great if it wasnt for entp i wouldnt have been abl to build what im build duck duck duck shotgun what me i never do that'"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data['clean_posts'][1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "forum_data_agg['clean_messages'] = forum_data_agg['clean_messages'].apply(stem_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"hi tanya kaggl will maintain a rate system if you win but your inelig for prize money you will still get a strong rate anthoni here are some paper that analyz eurovis vote pattern you might find some of them help gather comparison of eurovis song contest simul with actual result reveal shift pattern of collus vote allianc eurovis song contest is vote polit or cultur ginburgh and nouri suleman efstathiou and johnson eurovis song contest as a ‘ friendship network dekker more research enjoy love thi neighbor love thi kin vote bias in the eurovis song contest cultur and religion explain the bias in eurovis song contest vote hybrid system approach to determin the rank of a debut countri in eurovis eurovis judgment versus public opinion – evid from the eurovis song contest giovanni thank for your feedback use the forum to give feedback is a good idea it allow other to see and comment on suggest we might set up a proper feedback forum but for the moment this topic will have to suffic i also agre that the forum is a bit clunki howev we have a larg list of featur request and onli limit resourc for the moment it might take us some time to address this apolog i dont think the prize money in this competit is that relev the prize is relat small correct me if im wrong but i think contest are driven by intrins factor a karma system that reward forum post is a good idea again apolog for ani delay in implement this there are lot of featur on our to do list anthoni manish thank for the feedback the site is host on an amazon ec server on the east coast of america it a fast server but the site has been more popular than we expect were current work on speed up the site by reduc the number databas queri we may have to implement auto scale if the site keep grow so rapid anthoni just made a chang which should speed thing up let me know if it has made a differ for you jonathan thank for your feedback x were current work on cach databas queri there are a lot of good suggest here that well tri befor autosc pleas use this topic to give us feedback if youd rather do so in privat email me at anthoni goldbloomkaggl com use this topic to discuss ani competit you would like us to run if you would rather contact me privat email anthoni goldbloomkaggl com i accident delet the follow post made by anoth user im repost it on their behalf are there categor andor binari variabl in the data set other than the target variabl for instanc variabl open in test data seem to have categori if there are categor variabl do we get to know what the categori mean thank you sali mali has point out that there is an error in the auc calcul for entri with tie score that is when two or more score have precis the same valu we will look at the problem over the next hour and will rescor all entri apolog anthoni the auc calcul glitch has been fix and all entri have been rescor sali mali thank again for point this out thank for the feedback what sort of featur do you have in mind or can you point to a forum that you we should emul i just ad a quick repli box to make the forum less clunki great suggest i have put this on our extens featur to add list colin the choic of score system was quit deliber will the competit host consid use area under the roc curv where particip submit probabl but said that he deal with physician who just want to know the proport of predict that are correct rajstennaj and colin were realli pleas that you believ that there valu to the project let me know if there is anyway that we can help to facilit a communiti we did set up the general kaggl forum under communiti forum with such a communiti in mind doe it provid suffici infrastructur you are free to start ani new thread on that forum regard anthoni out of that pretti impress piti they didnt enter the kaggl comp i think your right some competit exhibit more regular than other soccer may be a difficult sport to model hi pg not sure that i fulli understand the question are you refer to the situat where a classifi return onli or rather than a score or probabl perhap you can use an exampl to illustr the question regard anthoni hi pg you should give the score for all timestamp a higher score mean the instanc is more like to be a member of the posit auc measur your classifi abil to split the class so you dont need to decid which score predict posit instanc and which predict negat instanc have i address your concern the public leaderboard is onli indic becaus competitor can use inform on their score to get inform on a portion of the test dataset the final result are a quit differ and b better reflect actual perform thank for particip in this competit ive attach the solut file to this post updat the solut is no longer attach but your welcom to make submiss to this competit hi matt thank for the nice word and the suggest ive post the solut file hi dirk the elo benchmark is base on the train dataset onli have had mani email convers with jeff i can tell you that the seed rate matter a lot youll notic that jeff made two submiss for the elo benchmark that becaus hes refin his seed method i believ he plan to make a few more refin jeff use an iter process to seed the rate system for exampl he might start by give everybodi and then let elo run for month he then seed elo with the month rate and run elo again he doe sever iter of this doe this help anthoni just to clarifi the result page will show the leaderboard for all competitor regardless of whether they use futur inform or not we will make an honour mention to the lead competitor who doesnt use futur inform howev their entri will be audit kaggl is current develop a leagu tabl that rank competitor when it come to this competit your posit on the leaderboard which is indiffer to the use of futur inform will be what count toward your kaggl rank hi john have just confirm with jeff methodolog will be share public regard anthoni the competit has been design to make cheat realli difficult at the end of the competit the winner methodolog will be replic to help ensur everyth is abov board hi matt the reason we prevent particip from submit an unlimit number of time is becaus otherwisea our server may not be abl to handl all the traffic anda it would be easier to decod the portion of the test dataset that use to calcul the public leaderboard the techniqu you describ often refer to as cross valid is veri sensibl and we encourag other to use it anthoni good suggest were open to idea on how we can facilit this my think is the best thing to do is to implement a more function forum which were do we can then encourag those who are still work on the problem to continu to use the competit forum as a way to collabor hi dirk weve updat the data descript thank for the pointer the competit doe requir particip to forecast the next four observ weve updat the format of tourismdata csv so that there is alway a valu in the last row regard anthoni inform can offer an awardhonour mention to those who dont use futur data howev the kaggl leaderboard will not seper those who use futur inform from those who dont hi greg apolog there was a bug that cut off the last charact the problem has been fix but unfortun the fix will onli appli futur submiss thank for point this out and sorri for the inconvini anthoni hi matt i believ that will the competit host is prepar a blog post that discuss some of the method that peopl appli to this competit base on the feedback we receiv is this the sort of thing you had in mind anthoni uri you rais an interest point howev is five month long enough for somebodi rate to move enough for you to notic this david this is a great suggest the hiv competit show that kaggler can do great thing my initi concern with ani public dataset is that peopl can look up the answer we would need research to withhold a small portion of the dataset for evalu i think the first step is to get in touch with those who set up the alzheim project it also make sens to contact the michael j fox foundat if anybodi has ani connect to either of these project pleas let me know otherwis ill keep you post on ani progress anthoni jase the score on the full dataset is calcul onthefli so we actual know who is win base on the full test dataset ron the submiss that is perform best on the public leaderboard may be differ from the submiss that is perform best on the full test dataset we dont link the best submiss on the public leaderboard to the best overal submiss so that particip dont becom confusedconcern if their scoreposit on the public leaderboard worsen leigh my think as well in a tradeoff between have a veraci public leaderboard and a veraci end result the end result is most import jeff good suggest ive put togeth an excel sheet that might be help for cross valid you past your predict for month into column g and it aggreg by player by month and then calcul the rmse hope it help anthoni ben the evalu method was chosen becaus jeff has found that score base individu game with rmse unduli favour system that predict a draw mark glickman rais anoth issu rmse is better suit to normal distribut rather than binari outcom so in order to use rmse aggreg is prefer of cours we could have evalu on a game by game basi use a differ metric my biggest problem with the current evalu method is that count a draw as half a win seem a littl arbitrari howev in order to benchmark elo such an assumpt is necessari mark and jeff argu that a draw is general worth half a win so this assumpt isnt too problemat anyway hope this give you some insight into our think regard anthoni jeff pleas correct me if im mistaken but i believ system that predict draw are favour becaus a high proport of game are draw at the top level per cent in the train dataset of cours you can do better but a system that predict for everi game will perform better than it should has anybodi tri trueskil yet probabl a better start point than elo this blog post doe a nice job of step through trueskil matt am interest in your think on this whi mae over mse or rmse is it just that the metric is more intuit or someth subtler hi david i have written to the alzheim diseas neuroimag initi adni and the michael j fox foundat i am schedul meet with both for septemb will keep you post on ani progress can you put up a link to the datapap you found thank again for the suggest it great if we can use the power of this platform to tackl meaning problem regard anthoni uri the correl between the public leaderboard score and overal score is signific higher now here is the solut file for anybodi interest uri im reluct to releas confid interv inform becaus i want to minim the advantag to earli submitt earli submitt alreadi have the small advantag of have seen their submiss on two differ public leaderboard by releas confid interv inform im give earli submitt access to inform that isnt avail to later entrant jase asid from chang the size of the public leaderboard portion of the test dataset we also select it more sensibl so it better repres the overal test dataset jpl a competit use internet chess data is a good suggest for interest the reason we are run the competit use top player is becaus elo rate matter most for top player sinc it is use to determin who can play in which tournament out of interest has anybodi enter this competit use glicko glicko or chessmetr are either of you happi to send me your unmodifi glicko submiss it would be good to add a glicko benchmark team to the leaderboard my email address is anthoni goldbloomkaggl com would like to do the same for glicko and chessmetr if anybodi has tri those i have also contact ron about use his trueskil submiss as a trueskil benchmark jase i post a link to your glicko code on the hint page it veri good of you to share it im realli surpris that glicko is perform wors than the elo benchmark do you think this is becaus jeff put lot of work into optim seed the elo benchmark or is glicko just not as good vateesh thank for send the file the file that you sent are actual differ i also had a look at your submiss and you have a few file with the same name but differ number also i was not abl to replic the problem as you describ it perhap you can tri again and let me know if your still experienc the error was just chat to jeff time permit he is go to benchmark some of these other system this way they will all be benchmark on a consist basi use the same seed procedur and the same degre of tune hi vess this should not be a problem given the way submiss are store hi edward you will appear on the leaderboard as soon as you make your first submiss hi edward tri use examplesubmiss csv avail at and replac the score column with your predict score if your still have troubl email the file to me anthoni goldbloomkaggl com and ill have a look given the way the competit has been setup there no way to prevent peopl from use futur data even if the winner present a model that doesnt includ futur data they may have overfit to replic the predict of a model that doe includ futur data uri thank for point out the problem were current work on a big upgrad to the websit the new site should be launch by the end of this month the upgrad will involv a more function forum in the meantim i will tri and fix this problem anthoni uri im not abl to replic the error either on the live site or on the develop version can you let me know if you experi it again hi han which post still cant replic the bug intermitt problem are realli annoy as mention were do a massiv site upgrad at the moment so that take up the major of our develop time how serious is the problem can we live with it for the next few week until we deploy kaggl in theori yes the problem is that there no way to be certain that the winner didnt use futur inform even when we check the win model it possibl they have use a model with futur inform to probe the test dataset eric thank for the feedback there not realli ani reason to insist on a particular file extens were current do a big site upgrad so ill add this to our list of featur request seyhan the leaderboard portion of the test dataset is select random it is somewhat repres of the overal stand i would realli like to be more activ in the forum look like there some live discuss happen ive been flat out work on the site upgrad which is onli a few week away from launch anyway id like to share a few thought on this discuss first off there is quit a strong correl between the public leaderboard and the overal stand second the lack of relationship between the score and the score might indic overfit this may be the case if your experienc a larger improv on the dataset than the dataset on a relat point i notic that your all perform veri well it could be that youv reach a local maximum i e the best possibl score given the techniqu your use just to reemphasi jeff point you should pay more attent to your cross valid than to the leaderboard the leaderboard is calcul on a veri small amount of data so it is onli indic phillipp sorri for the delay in do this i havent had comput access over the last few day the spearman correl between public score and overal score is i also calcul the correl for differ submiss quintil to make sure the relationship hold at the top it doe top it also worth mention that the troubl particip are have reflect realworld difficulti in formul a chess rate system this competit is not just a game but a genuin attempt to explor new approach to rate chess player anthoni out of interest whi arent peopl rerun old approach that had previous been score on the new cross valid dataset greg thank for point this out im current travel but will look into this over the weekend wil if you can get histor data from freechess org possibl by agre to share the win method with them wed be happi to host a comp here this way you could specifi that the win method must be an instant gratif system it would also result in a system that tune to lower rank player thank for point out the error it has now been fix apolog for ani confus cole sorri for the slow respons in this competit all your submiss count in futur we will ask particip to nomin submiss phil that is correct you must rememb that kaggl hope to do more than just host fun competit we want to help solv real problem this is whi were reluct to forc particip to choos just one model they may make a poor choic and the compett host may end up with a suboptim model our compromis posit is to allow partip to nomin five entri a featur which well roll out for futur competit phil number is correct luck will play a part but i suspect the test dataset is larg enough to limit it impact i agre in a competit like this one but as mention abov we want to host competit that are use as well as fun an upcom competit will requir particip to predict who has prostat cancer base on variabl in a competit like that it would be a shame to miss out on the best model requir particip to nomin five submiss seem like a good compromis greg this problem has now been fix thank again for point it out hi cole apolog for the ambigu the time is as it appear on the competit summari page adjust accord to the timezon on your comput clock so itll be saturday or sunday depend on your timezon you can also see a countdown on the kaggl home page anthoni dirk i just chang the file post on the data page to a unix format hope this solv the problem durai apolog for the slow respons all up countri were repres here is the list in order of most particip to fewest unit state unit kingdom australia canada thailand india germani spain china netherland franc itali new zealand south africa sweden argentina croatia ecuador greec indonesia iran ireland mexico poland portug russia singapor turkey and ukrain ricardo you are correct i gave the countri list for the wrong competit countri were repres unit state colombia india australia unit kingdom franc thailand canada germani argentina japan afghanistan albania austria belgium chile china croatia ecuador finland greec hong kong iran poland portug slovak republ venezuela uri make a veri good point one way we could run a competit without know futur matchup is to have particip rate everi player onc we know the matchup we can infer predict base on player rate the onli downsid to this approach are it doesnt allow for probabilist predict sinc there are mani way to map rate into probabl we couldnt show a live leaderboard which help to motiv particip interest in other thought on this particular the import of a live leaderboard ron this is fantast look like a sizabl proport of the black dot are sit in a vertic line though im sure the elo benchmark would look much wors out of interest what softwar did you use to generat the viz ps im guess the anomali that this viz highlight e g that white is a smaller advantag for lower rate player could inform futur version of your rate system philipp i dont fulli understand your suggest do you mind tri to explain it again possibl by refer to an exampl as a general principl tne problem with attempt to prevent peopl from use neural network and the like is that particip use them anyway and then overfit other system to replic the neural network result i actual think that have neural network et al in the competit is valuabl even if they wont be implement as rate system they may have some benchmark valu assum they predict most accur they give a sens for what level of predict accuraci is possibl from ani given dataset as an asid if we requir particip to submit rate and dont give them access to the matchup that theyll be score on this should forc particip to creat a rate system shouldnt it btw jeff re i have been and continu to be amaz by the level of particip so far i had no idea so mani peopl would particip congratul on organis such a popular competit pew what criteria would you use to evalu such system b t w i think youd be surpris at the proport of the top who are build rate system philipp thank for point out this bug the error was onli aesthet had been accid hardcod into the new theme the platform was still onli permit two submiss anyway the error has been fix philipp thank for your nice word hope have a more profession look and feel will help us attract interest competit with bigger prize pool hi all wonder whi the benchmark is still lead when it is public avail have peopl had troubl replic the author methodolog or is everybodi tri their own approach anthoni hi jess you are correct this is instruct is wrong the month column mm should be line long includ the header and the quarter column qq should be line long the examplesubmiss csv file avail on the data page give an exampl im at a confer today but will correct the instruct as soon as i get the opportun someth was amiss there was an error in the data upload on kaggl kaggl fault not the author the chang are not particular big so model that perform well on the previous dataset should continu to perform well to give you the opportun to rerun your model and make new entri we have extend the competit deadlin by two week and lift the daili submiss limit to three per day and i believ georg intend to releas the code use to creat the benchmark apolog for the error dont hesit to ask if you have ani question unfortun the movi isnt out in australia yet weve still got anoth week to wait sorri for the slow respons ive been flat out with the new site launch below is the list of row use to calcul the public leaderboard dirk ive chang the line break format let me know if this doesnt fix the problem jason there a bug that prevent user see previous score when they have longish techniqu descript we are awar of the problem and will fix it as soon as we can diogo thank for point out this error we will setup pagin on the submiss page short hi all just to let you know that we have extend the deadlin for this competit by just over a week both jeff and i will be travellng around mid novemb so wouldnt be abl to deal with the competit conclus anhoni apolog i hadnt antip that this might be an unpopular move i should have canvass opinion first if other also disapprov i will chang back the deadlin kaggl is not a dictatorship the downsid of chang back the deadlin is that it limit our abil to generat public this bother me becausea top perform deserv recognitionb public for the competit is public for kaggl and more public more member more competit andc it lessen the chanc of get f i d es attent a compromis might be to extend the previous deadlin by three day to wednesday novemb when jeff is offlin but i am avail thought hi philipp the chessbas articl were written by jeff he has a relationship with the editor jeff be away when the competit finish mean that it unlik that chessbas will report on the end of the competit a real piti if we hope to grab f i d es attent it is unfortun that were both away when the competit end obvious not foreseen when it launch otherwis we would have set a differ deadlin anthoni jeff we must have post simultan you rais a good point if philipp and other are ok with the th then we should go with the compromis date this would mean that ill be avail to report preliminari result and should mean were readi to report the final result by the time you return preliminari will be unconfirm result from the raw leaderboard final result after the top ten have all agre to share their methodolog ive chang the deadlin to the th as for uri breath down your neck rememb that the public leaderboard is onli indic and that the final stand may be differ tim kaggl is current in the process of put togeth a leagu tabl which rank particip base on competit perform if you perform well in this competit it will count toward your rank this first chart how the lead score has chang on a daybyday basi the red line show the elo benchmark and the blue line show the lead score the elo benchmark was outperform within hour which is whi it alway abov the best entri interest to see some recent progress after a period of stagnat well done philipp my guess is that ani major improv from this point on will be the result of somebodi tri someth quit differ this chart show the number of daili entri higher earli but seem to have stabilis at around per day happi to put up other chart if peopl have request phil i made an error in the ten per cent list abov tri score with the follow row philipp there certain a largish gap between the top five of cours this is pure indic what realli matter is the score differ on the final leaderboard philipp great sugges weve got a stack of featur we want to implement but ill put this in our long term wish list i tri pute up a general forum for such discuss but found that it was veri light use featur in the pipelin includ fix bug or incomplet featur on the new site upgrad to kaggl infrastructur to allow us to score veri larg entri kaggl rank system an elo for kaggler base on microsoft trueskil extend social network featur includ live chat recent activ feed philipp competit analyt sugget and possibl some other data viz tool competit in the pipelin includ predict social network connect predict the like success of grant applic for a larg australian univers forecast travel time for freeway in melbourn australia predict prostat cancer from a high dimension dataset subject to ethic approv diagnos breast cancer from mammograph densiti imag also subject to ethic approv ani other suggest ani thought on what our prioriti ought to be steffen you can enter use a model code in ani languag john drew i presum those who enter use softwar other than r are still elig for prize diogo thank for point out this bug few minor teeth problem with the new site we should have them sort out befor long jason l t are you think along the line of karma point for particip in forum discuss or would you like the forum to be more of a qa with stackoverflow style rate i like the idea of guest blog post and communiti tutori after the chess competit end some might be interest in post detail of their workflowmethodcod lt the general forum has been taken down for the moment when i get a littl time i will attempt to reviv it and start encourag peopl to use it philipp it may not matter that peopl onli compet in a hand of competit becaus each competit contain quit a lot of inform unlik a singl chess game particip are compet against mani player regardless well do plenti of test with trueskil befor implement as for the point system point seem a littl abitrari i like the idea of rate that account for the strength of a competit particip i tend to agre with your point on forum particip point the stackoverflow approach seem like a nice way around the problem there are lot of direct we could take kaggl but for the moment were focus on competiiton yuchun apolog for this error the public leaderboard is portion of the test dataset is actual the first per cent becaus we hadnt implement the code to select a random portion of the leaderboard yet for info the reason we kept get differ per cent is becaus the random seed in the databas was set to zero which told our code to choos a random random seed anthoni hi artem for the intuiton behind auc have a read of the evalu page kaggl implement of auc work rough as follow sort submiss from highest to lowest goe down the sort list and for each predict plot a point on a graph that repres the cummul percentag of class a predict against the cummul percentag of class b predict join up all the point to form a curv the auc is the area under this curv ht phil brierley for this explan william no threshold is requir which is part of the beauti of auc in fact given that the algorithm work by sort particip make submiss contain ani real number higher mean more confid that the observ is of the posit class hope this respons doesnt serv to confus peopl anthoni hi jon it a fix per cent chosen random anthoni hi tama as your result suggest the order doe matter and the i ds dont anthoni artem ive gone through the step use your exampl data let me know if ive made ani error the kaggl algorithm basic work as follow first order the data predict real then calcul the total for each class in the total total initialis the cumul percentagespercentslast percentslast iter for each solutionsubmiss pair count count count count percent countstotalsperc countstotalsrectangl percentspercentslastpercentslasttriangl percentspercentslastpercentspercentslast area area rectangl trianglepercentslast percentspercentslast percent so in your exampl first submissionsolut paircount count percent percent triangl rectangl cumul area percentslast percentslast count count percent percent triangl rectangl cumul area percentslast percentslast count count percent percent triangl rectangl cumul area percentslast percentslast count count percent percent triangl rectangl cumul area auc also here kaggl php code to calcul a u cprivat function a u csubmiss solut arraymultisortsubmiss sortnumer sortdesc solut total array a b foreach solut as s if s total a elseif s total b nextissam thisperc a thisperc b area count a count b index foreach submiss as k index if nextissam lastperc a thisperc a lastperc b thisperc b ifsolutionindex count a els count b nextissam ifindex countsolut ifsubmissionindex submissionindex nextissam mycount if nextissam thisperc a count a total a thisperc b count b total b triangl thisperc b lastperc b thisperc a lastperc a rectangl thisperc b lastperc b lastperc a a rectangl triangl area a auc area return auc jc i agre that those who enter earli have an advantag howev the main sourc of advantag come from the fact that they have had the opportun to spend longer on the problem and tri more thing philipp the current leader has made entri if this competit took ternari score loss win draw this would amount to possibl combin make phillip entri a drop in the ocean in fact the test dataset is richer becaus particip predict the probabl of victori nonetheless for futur competit we will ask particip to nomin five entri that count toward the final stand pew we are not requir particip to guess but rather encourag them to reli on their cross valid when determin which model to choos the problem with allow peopl to enter mani time and tri mani paramet tweak is that they are more like to accident overfit on the test dataset by this i mean they are more like to find a paramet tweak that work well on the test dataset but doesnt work as well for futur chess game on your second point you are correct to say that i am worri about statist guess the requir that particip submit code doe not obviat this concern becaus model can be overfit onc the answer are known in the extrem case somebodi could fit a decis tree that classifi everi game perfect if they know the answer show the stand but not the score make statist guess onli slight more difficult becaus particip are close enough that the leaderboard order give meaning feedback on which guess are better and which are wors as an asid it seem that i have fail to convey the messag that the public leaderboard is pure indic and that cross valid is import i would even go so far as to say that it may be problemat if the public leaderboard bear too close a resembl to the overal stand i like uri suggest it get around the problem that lt menton while potenti encourag peopl to tri thing beyond paramet tweak coupl of potenti problem a particip exhaust the submiss limit and anoth entrant make and share a breakthrough eg the use of chessmetr in this competit anybodi who has exhaust the submiss limit wont have the opportun to build on the breakthrough this seem less than ideal given that we want to get the best result possibl it might encourag peopl to make all their entri at the end so that they dont reveal the strength of their hand what do other think philipp kaggl has been experienc a massiv lift in site visit and signup sinc the new site launch from uniqu visitor to this account for the increas in entri thank everyon for make this an amaz competit big congratul to the winner outi also to the runner up jeremi howard who onli join the competit late in the piec and to martin reichert who finish third hope well get some of the top ten to tell us about their method on the blog in the meantim i encourag you all to tell us a littl about what you tri on the forum also for interest here a chart that show how the best score evolv over time rapid improv initi but after a month progress stall as particip approach the fronteir of what is possibl from this dataset apolog uri and lt seem that ani repli is redund now also big thank to all those who particip in forum discuss you help make this a far more interest competit i think i can help with this i dont give name just score combinationsscor publicscor jeff can i post the test label on the forum i onli seem to have the aggreg solut on hand attach jeff do you have the game by game label edit look like you post a minut befor me hi nick your welcom to bring addit data as long as it public avail anthoni attach is some sampl code that can be use to constuct an entri that generat a forecast base on the averag travel time on a given rout on a given day of the week at a given time attach is some sampl python code that generat forecast base on the last known travel time im new to python so happi to hear ani feedback on the code mmm file didnt attach here the codephprh fopen r t a data csv r file to read fromwh fopensampl histor csv w write the entri to this filedatedefaulttimezoneset g m t pure to prevent the interpret from rais a warningtim stamp array an array with the hump off pointsforecast horizon array forecast horizon in lot of minut e g minut minut hour this is use for calcul the forecast time stampsforeach time stamp as ts foreach forecast horizon as f forecast time stamp date n h istrtotimetsf find day of week hour and minut that correspond to each of the timestamp row while data fgetcsvrh fals loop through the datafil if row write the header col count countdata for c c col count c fwritewh datac fwritewhn if inarray date n h i strtotimedataforecast time stamp if the day of week hour and minut that correspond to a forecast timestamp is found then save to an array call ts array for c c countdata c if emptydatac datac x ts arrayd n h i strtotimedatac datac rowforeach time stamp as ts foreach forecast horizon as f fwritewh date ymd histrtotimetsf for c c col count c fwritewh arraysumt arrayd n h istrtotimetsfccountt arrayd n h istrtotimetsfc write the averag for a given day of the week hour and minut to the submiss file fwritewhn fcloserhfclosewh file didnt attach here the codeimport csvimport datetimerhopen r t a data csvr read in the data whopensampl naiv python csvw creat a file where the entri will be savedrh c s v csv readerrhtim stamp an array with the cutoff pointsforecast horizon forecast horizon in lot of minut e g minut minut hour this is use for calcul the forecast time stampsrow inialis the row variablefor data in rh c s v loop through the data if row if the first row then write the header for j in rangelendata wh write dataj wh writen if data in time stamp if the row is a cutoff point for i in forecast horizon for each forecast horizon write the cutoff travel time as the forecast the definit of naiv date str strdatetim datetimeintdataintdataintdataintdataintdata datetim timedeltai calcult the time stamp given the forecast horizin wh writed str write the timestamp to the first column of the csv for j in rangelendata wh write dataj write the cutoff travel time to the subsequ column wh writen row rh closewh close dirk thank for point this out ive written to the rta about this and they respond say inde our control room have confirm signific increas in traffic volum follow the remov of the toll this has had an impact on the overal travel time across the m someth to be awar of when use the older data hi denni the per cent doesnt count toward the final stand and is select at random across the timestamp and rout as for the smtp error it been fix the problem was the result of a flood of signup which caus googl to shut off our mail server were now use our own mail server anthoni apolog for the error it decisecond not centisecond so is second ive fix the descript this is someth that should be dealt with on a case by case basi if you find a dataset youd like to use ask on the forum and ill run it by the rta for inform im tri to get hold of some incid data will keep you post on this lee this is great dirk did the same thing with some python sampl code i wrote for the social network competit if you guy keep show me how thing can be done better i may becom a half decent coder toppi thank for the pointer a higher prioriti at the moment is to get forum attach work again hi peter ill follow up in this at the veri least we should be abl to provid inform on the length of differ rout anthoni hi carlo unfortun not claus c in kaggl term and condit saysc employe or agent of the competit host are not elig to particip in ani competit post by the competit host to answer the second question we would more inform about the natur of the busi and what your friend doe anthoni frank this is great i particular like the heatmap is it possibl to zoom also itd be neat to see some anim on the m map show how travel time evolv over the cours of a dayweek dot get bigger and smaller though i suspect this might be a lot of work anthoni david i believ that when loop the measur devic fail travel time are estim im work toward put togeth data on when travel time read are suspect c doe seem to be an express languag im a linux user though so not inclin to pick it up armin i agre make more sens for me compil this inform onc for everybodi will tri and get it done this week daniel denni is correct in say that averag the valu lead to float point number the answer are integ but the rmse is calcul use float point arithmet thank b yang the benefit of publish code is that you get sensibl suggest in return andrew good discoveri ill pass the question onto the rta edit wouldnt it be obvious if they werent make the adjust sinc peak traffic time would chang this code doe generat a sampl entri to use it a download the php interpreterb creat a file name xxx php copi the code abov and download the data file to the same directoryc run the command php xxx php your correct the futur is use to predict the present howev i dont think the tempor leakag invalid the algorithm develop in this competit daniel and denni are correct keep in mind that the per cent is a random select of the that doesnt count toward the final stand which are calcul base on the other per cent the cutoff time are all between am and pm they were select use a simpl formula that favour high volatil cutoff time over low volatil cutoff time so youll see more peak hour morn and afternoon cutoff time the rational behind this is that it more import to predict accur dure high volatil time so we want to favour model that do best at these time that explain whi the rmse is higher than for random chosen cutoff point aidan have ask the rta about this this was the respons the cutoff is due to free flow condit impos by the system dure data unavail ive written again ask for a littl more detail will post the respons when it come paresh thank for the thought provok question i agre with denni i am more interest in the time delay than the percentag delay on a relat matter we think it is more import to predict correct when travel time are volatil e g befor and after work to favour model that predict more accur dure high volatil time we select more high volatil cutoff point so youll notic more cutoff point dure the morn and afternoon phil thank for share this just got to find a window machin to run it on hi markus i can help out on the second part of your queri ive post some php auc code on anoth forum post softwar packag like r have easi to use packag that calcul auc anthoni you can email me the file if you like anthoni goldbloomkaggl com id be happi to take a look at it rasmus apolog i delet the wrong post anyway you ask how travel time are measur there are regular space loop along the m these loop measur each car speed and the number of car that travel across the loop everi three minut travel time are then calcul use a formula the formula has been test and calibr use test car that travel along the freeway and record their travel time thoma i select specif cutoff time random but chose timeday combin that are volatil across the dataset vitali the volum data is use to calcul travel time see this post for more info our prioriti at the moment is to get the incid loop error and rout length data togeth howev i can find out if this data can be made avail if you think it might be use as denni say itll be high correl with travel time and we obvious wouldnt releas it for the blank out time jeremi i wasnt awar that public document with traffic detail were avail to the extent that ani inform is avail for blank out time this would most definit be consid cheat as for question i am awar of this in fact the issu came up in anoth post the rule state that the win model must be implement by the rta in order to be elig for the prize the averag model pass this test as an asid i dont believ the tempor leakag invalid the algorithm develop in this competit benjamin onc we get the incid data i will put in a request for this data i have some inform on suspect loop read that im work to releas this has inform on when loop read may be unreli for various reason i dont yet know whether or not this will help with the free flow issu anyway i will upload them as soon as i can get it into a use format i suspect the reason the free flow time are differ is becaus rout length are differ rob on your point about miss data it might be help if i explain how i put the file togeth i receiv data in the follow formatrout i dtimestamptravel time xxx xxx i transpos them into in the hope that theyd be more manag when timestamp were miss i just fill in a blank row hassan the most import file is r t a data csv you can creat a sampl entri by download r t a data csv and creat histor php attach to the same directori navig to that directori in the terminalcommand prompt run php creat histor php this will creat an entri base on a histor averag for that timeday and is a good start point bjb veri generous of you to upload a java code ive now enabl java file upload so you should be abl to upload the file burak the time in sampl entri csv are the time you need to generat forecast for there more info on how the cutoff point were select in this forum post aaron you rais a good point accord to the rout definit i have rout extend from loop a to loop a while rout extend from a to a so should encompass all of denniss observ that sometim has longer throughput time than is strang ill doubl check the definit with the rta aaron anoth good question have also pass this on to the rta the number direct to the left of the team name is the team posit and the number to the right of the team name is the team score or root mean squar error rmse alexand to me this mean that the algorithm can take a timestamp as an input and can generat forecast for the next min min etc lee thank for point this out this post post alexand there is no truncat of float mmm my messag seem to have disappear from the board anyway here a repeat aaron the unit are decisecond nick actual it a hybrid approach you can nomin five entri that count toward the final stand you do this from the submiss page the last five are chosen by default at the end of the competit the best of your five nomin entri count toward your final posit and nick on your new question the one of the five you nomin that score best on the per cent count the per cent is meaningless as far as the final stand are concern jose do you want me to ask if it permiss to use noaa data if so are you ask about the data that brad mention abov william thank for the question team are allow to merg one individu cannot be part of sever team our system ensur this anyway as long as somebodi doesnt have multipl account agre that we should make this more explicit in the futur as for find peopl who submit from multipl account we are actual in the process of implement rule that alert us when it look like this is happen in the futur for larg prize money competit we may look at verifi ident realli nice feedback veri thought provok the api suggest is nice it doe seem that it would prevent peopl from use the futur to predict the present howev the testtrain split is still necessari to prevent overfit and we could still onli give partial leaderboard feedback the api doesnt secur against overfit paramet tweak also the api approach would add new problem model will take longer to run becaus of the delay in receiv data point as you say it would add a huge load on kaggl server as for the problem you list here are my respons predict can't use all avail prior data sinc the test data doesn't provid result this is necessari to ensur against overfit if all the data is use to calibr a model it imposs to know if the model will fit futur dataset as well limit train and test data creat too much varianc between the public score and actual score the mistak made in the first competit was with the size of the public leaderboard portion of the test dataset my fault not jeff it was too small which lead to the low correl between public and overal score for the rta competit we rais the proport to ensur a stronger correl this proport was calibr after some test of the correl between the two part of the test dataset we intend to continu this practic go forward model paramet can't be tune becaus actual score aren't provid if we allow paramet feedback on the whole test dataset this would almost definit lead to overfil paramet tweak that work on the test dataset but wont work for for futur dataset number of submiss is sever limit becaus they are so larg this will becom a bigger problem as larger test dataset are creat i dont think more daili submiss are necessari becaus the major of model build should be done with refer to a cross valid dataset leaderboard doesn't reflect actual leader again this was my mistak i made the public leaderboard portion of the test dataset too small this is not a flaw with the general approach futur data can be use to predict the past jeff suggest a realli nice solut to this test set includ some spurious game so that peopl can't mine the test set for use data about the futur these spurious game wouldnt be use in final evalu the api also provid a realli nice answer to this problem attach is some r code to creat a glm entri for this competit as alway happi to hear feedback from other about how this could have been done more eleg anthoni nathaniel is right the data is correct it just a problem with head format will fix this short and reupload the data rob thank for jump in eleni just upload rout length approx csv which has approxim rout length data konstantin just upload r t a error csv the is valid data it avail on the data page final fix the head just to reiter all the data are correct it just the capit in the head that caus troubl as for the inconsist number of delimit also fix my softwar packag stop print delimit when there were no more valu or n as in a row jack the countri of birth issu is now fix pleas download the latest version of the data p v kiran it mean that if your solut is implement use a softwar packag that is not avail to the univers of melbourn it must be possibl to translat your solut into a differ packagelanguag no toward the end of this competit you will be ask to nomin five entri that count toward the final result dielson good pick up denni is correct the date format doesnt matter what is import is that you put the correct data in the correct cell mooma i appreci your frustrat but sensor malfunct are part and parcel of deal with realworld data if we had the data readi at the outset we might have exclud fail sensor and downweight the impact of partial fail sensor when evalu predict konstantin denni is correct it is not safe to assum that there is no error in the control data nathaniel thank for point this out definit worth investig the number of success grant and number of unsuccess grant field dont chang in the test dataset for obvious reason the journal citat also remain constant in the test dataset to prevent particip use the futur to predict the past jose and joseph just spoke to the rta about this the answer is no becaus it might allow futur weather condit to be use to predict the present ahm just got an answer from the rta on this here the respons the answer is mayb rta would request that anyon wish to use the data for further research purpos write to the rta and make their case describ what they wish to do ie the purpos of the research and how they would use the data the rta will consid each applic on it merit let me know if youd like me to pass on the relev email address im reluct to do it in the forum but will offer an introduct to anybodi who ask just elabor a littl the type of solut that cant be implement are those that are encumb by patent or other intellectu properti restrict michelangelo truth is that you can submit ani real number we suggest a number between and becaus of the conveni interpret auc rank your score the higher the score the more confid you are that the instanc is a member of the posit class nathaniel i have look at the problem in some detail and have spoken to the univers of melbourn they are look into it and hope to have an answer for us tomorrow befor they break for christma mani thank to everyon for all your great activ on this fascin problem insight question and comment on the forum good earli result on the leaderboard and interest discuss there have been a lot of question about exact what constitut an accept model for the rta so far my guidanc on this matter has possibl been too fuzzi and i hear a lot of you look for more definit rule therefor we have come up with the follow specif rule regard the allow model input your model can be of ani form you like as long as it take it input onli from the follow paramet time of predict day of week is holiday month of year rout number to be predict the time taken for rout r for datetim t where r is ani rout and t is ani time less than the datetim be predict for as mani rout and datetim as you wish the sensor accuraci measur for ani rout r and datestim t defin as abov the estim rout distanc as provid by kaggl to clarifi the follow are not permit the use of ani data other than those provid by kaggl for this competit and the list of nsw holiday the time taken for ani rout in the futur compar to the predict be made your model can still be train use all data as long as the result model onli use the input list abov furthermor the algorithm must not be encumb by patent or other ip issu and must be fulli document such that the rta can complet replic it without reli on ani black box librari or system the univers has spent the last two day on the problem they suspect it an intern inconsist in their databas the figur are drawn from differ part of their databas well have to wait until the end of the christma break to get a final verdict hi alexand no use full timestamp make it possibl for a model to implicit incorpor extern data and futur data you may also use holiday data extract from the pdf file that you link to in order to get holiday inform for previous year howev we will not be provid a file of this inform direct this is correct anthoni deepak thank for point this out we will ask the univers about this as well unfortun we cant expect an answer until earli next year person i ds refer to all the column that have investig i ds e g column has investig column has investig ignor the comment numer valu that should be as jeremi howard point out earlier in this thread the key point that answer most of these question is that the limit is onli on the function form of the final model more specif xiaoshi lu you can build your model filter aggreg etc use all the datetim inform you like the final function form that you end up with howev should onli use the predictor list abov mooma the input list includ this the time taken for rout r for datetim t where r is ani rout and t is ani time less than the datetim be predict for as mani rout and datetim as you wish so what you ask is specif allow of cours for you to creat your input file which includ for exampl the time taken one hour earlier you will need to use the full datetim howev the result model will not direct use this instead it will onli use the time taken on that rout as allow by the rule alexand groznetski imagin use a veri flexibl model neural net for instanc which train with all datetim info includ in the input paramet it might implicit end up use the rout time later in the day to predict those earlier this is an exampl of how a model could be useless in practic even although it appear high predict on the competit data jose i notic that you are now on the leaderboard it can take a few minut befor you show up anthoni matthew use gpl code is fine the isholiday variabl can be a direct input rather than a variabl that is deriv by refer to a timestamp you contact me direct at anthoni goldbloomkaggl com denni you can use isspringbreak rather than isholiday david it realli neat for info it work in safari but the page video are align a littl strang martin dane is correct the inform in rout length approx csv is in metr so rout is approxim km the in the money indic is base on the public leaderboard onli it doesnt reveal anyth about the final stand martin when i open the file it show and what applic are you use anthoni i believ it refer to grant made when the research was at anoth univers nichola a matlab solut is fine as long you dont includ librari that use patent or undocumentedsecret algorithm apolog will i was on a plane and onli just got your messag will make the adjust this afternoon id also like to congratul the top team and congratul dirk for run an excel competit thank to everybodi who particip and a big thank to dirk for put togeth a realli nice design competit the test label are attach to this post rafael and are fine is also fine as long as the data is deriv entir from the time seri as you say b yang first off congratul again on a fantast perform your frustrat is understand but we cannot enforc rule that dont exist what is common sens to some is not common sens to other as jeremi point out in the rta competit the rule say the win entri has to be a general algorithm that can be implement by the rta an algorithm that involv look up futur answer could not be implement by the rta reginald pleas email your submiss to anthoni goldbloomkaggl com and ill have a look edith thank for the feedback we agre with your comment and we are work on make the term more competitor friend the univers has done an investig and has found that the issu aris from an inconsist in their databas wu wei a rout is made up of sever loop a figur of mean that per cent of the loop in the given rout are give suspect read apolog i didnt clarifi this with mahmoud befor the launch but we have discuss this offlin this competit requir you to choos five entri that count toward the final result to choos five entri visit your submiss page and click the star next to the relev entri to select it if you do not choos ani entri your last five entri will be chosen by default michelangelo the per cent come from the test dataset eu jin lok the sampl is done random for anybodi interest here the actual solut hi greg the answer will be made avail on the forum i can ask whether the data can be use for publish research if you like kind regard anthoni hi greg and suhendar the univers doesnt want the data to be use for ani purpos other than for this competit anthoni hi greg it would be nice if the dataset could be use for other work howev if we dont allow competit host to place restrict on the use of their data then we wouldnt get access to it in the first place will post the solut file now regard anthoni the solut file is attach to this post thank all for particip anthoni entri made befor we fix the leaderboard were score incorrect i have now rescor the relev entri the error was the fault of kaggl and not the competit organ apolog anthoni hi cerin apolog for the error they all stem from the fact that the server hard drive fill up ive clear some space for inform were current rewrit the entir site for the heritag health prize you can expect the next version to be faster and includ mani more featur thank for your patienc anthoni hi cerin ali is right your entri will count toward the final stand anthoni hi all submit from multipl account is most definit against the rule we have done some analysi and found that it happen veri rare howev we are work to put the system in place to identifi and block those who attempt to do it kind regard anthoni the solut is attach thank all for particip anthoni harri thank for the thought post the ijcnn peopl agre with you and have decid not to disqualifi shen as mention abov kaggl will soon have the system in place to detect multipl account in real time so that such issu dont aris anthoni i have sympathi for peopl frustrat in this case the competit host decid that the result should stand so we are facilit their decis chris make a good point about the rule be scatter throughout the site we will be sure to address this in futur competit we will also ensur that they are tight enforc for inform a lot of effort has gone into frame the heritag health prize rule final thank for the feedback it discuss like this that will help us improv kaggl kaggl has receiv legal advic after the controversi surround this competit we have been advis that it set a danger preced for us to ignor our own term and condit notabl claus prevent multipl signup we have therefor act in accord with this claus disqualifi those who clear submit from multipl account thank you all for your patienc on this issu and rest assur that we are work to ensur that it is not a featur of futur competit entrant are welcom to use other data to develop and test their algorithm and entri until utc on april if the data are i freeli avail to all other entrant and i publish or a link provid to the data in the “ extern data” on this forum topic within one week of an entri submiss use the other data entrant may not use ani data other than the data set after utc on april without prior approv also cover by slate and forb and the wall street journal a coupl of week ago and smarter planet the criteria was that somebodi had to make at least one claim in y be elig to make a claim in y outlier have been remov from the dataset as well as those suffer from stigmat diseas just to clarifi when jeremi say we clean it as much as we can we didnt do much to the claim data on purpos we figur it make more sens for you to make your own clean assumpt rather than have us impos them on you not onli are patient who die in y not in the dataset but patient who die in y are also not in the dataset becaus they didnt remain elig to claim for the whole of y apolog this was an error thank for draw our attent to it the miss valu are for those peopl who have been in hospit for more than two week they should be replac with a you can either do this yourself or download the updat dataset for inform member who have in hospit for more than two week have been group for privaci reason they are rare so may otherwis be identifi the implic of this group is that if you expect somebodi to be in hospit for more than two week you should predict day this group should not have a big impact becaus a member who are in hospit for more than two week are rare about one per cent of member b the evalu metric favor algorithm that accur predict fewer day in hospit on the assumpt that these are more prevent dorofino great idea form a team is a realli good way to learn are you affili with the new york r user group for info ive heard rumbl about them set up a team good luck with this anthoni hi rich just spoke to hpn about this for the moment they dont want to provid general guidanc and ask that you make a request through the contact us form your request should detail the topic of your propos research definit worth make it clear that your just look to publish the method that you use to enter the competit anthoni y y y etc refer to differ year we havent reveal which year to help keep the data privat the year are sequenti we are not reveal what year yn refer to nor whether or not they refer to calendar year for data privaci reason apolog for the miss valu it was an error you can either replac the miss valu with or download the updat data set if your interest in the reason for the miss data see hi bacg day in hospit refer to y the second year while the claim refer to y the first year not everyth that has a length of stay count as a hospit in fact you dont have enough detail in the claim tabl to calcul day in hospit the detail has been suppress for privaci reason anthoni hi mbenjam we would have love to releas more detail data but have to be mind of data privaci anthoni have receiv advic from the hpn lawyer im realli sad to say that the answer is no on all account eu jin youv obvious not seen this wgn the intent is not to rule out the public of research ive pass on your messag to hpn and a clarif will be forthcom the lawyer are take a conserv stanc on this issu apolog it realli disappoint to have peopl rule for this reason flsdcom i have a meet with them in minut i will be sure to rais this point in respons to ashasho origin question i have sought a reexamin of the issu the hpn lawyer explain that the reason for the hard line is that they have no way to verifi that resid permit compli with us legisl im realli sorri to say that there not more i can do the accuraci threshold will be announc when we releas the full claim dataset on may i want to reassur everyon that hpn is work hard behind the scene to clarifi the ip issu it is not their intent to prevent peopl from use standard tool nor to discourag anyon from appli their innov idea to this problem for background at monday launch event dr richard merkin the man behind the prize spoke of the long tradit of innov that has result from past prize he spoke of the longitud prize appar newton and galileo had attempt to solv this problem but the winner was a self educ clockmak from yorkshir napoleon food preserv prize won by a confection and result in the invent of can food the orteig prize to fli nonstop from new york to pari won by the unlik charl lindbergh it is his hope that this prize will spur similar innov to solv one of america most vex problem we appreci your patienc while we await clarif kind regard anthoni this is a sampl of the final dataset but the final dataset is not in the terabyt rang to the best of my knowledg this dataset is on the larger side for medic dataset which tend to be quit small this algorithm will not need to oper in a realtim environ and so there is no restrict on execut time rudychev receiv an answer from hpn on this a patient who visit a clinic outsid the network should be captur in this dataset of cours as jeremi keep reiter there is alway a disconnect between realiti and the content of a databas the decis to predict day in hospit was made to make the test dataset richer so we can better sort out good algorithm from bad the logarithm in the evalu metric was chosen to favor model that predict short stay more accur as these are assum to be more readili prevent as for the question of nefari intent i can tell you what i know about dr richard merkin the man behind the prize he is a big philanthropist who devot time and resourc to fund scientif project school and the art in my opinion hpn did not need to put up million to get an amaz algorithm kaggl has found in it own competit that with prize as small as or a chess dvd particip approach the limit of what possibl on a dataset in our communic with hpn we have been told that the million prize is an attempt to draw mass attent to this prize and the issu in general dr merkin want to promot the potenti for medic data mine in lower healthcar cost the prize also serv to introduc a larg number of talent data scientist to medic data final rest assur that hpn are work hard behind the scene to clarifi the ip issu mgomari one issu we have to keep in mind are the tradeoff in releas data for data privaci reason hpn have a granular threshold which theyr not will to breach the data anonym team repres by keleman in the forum are tri to releas c p t code probabl at an aggreg level appar it pretti linebal and releasng day in hospit y might put this in jeopardi i describ the data privaci consider like a waterb you push down on one part of the bed and it creat a bulg somewher els after may youll be abl to use day in hospit y and day in hospit y to predict day in hospit y ogenex even if we releas day in hospit y you wont be abl to do a consist check not all length of stay count as hospit as calcul for this competit and you dont have enough detail in this dataset to work out which count and which dont for those who dont know jphoward was kaggl most success competitor befor join the team his tutori give realli clear explan of the tool and techniqu that made him such a success competitor hi jim that is correct for inform the reason for the misnom is that it was day when we sent it to the anonym team but they had to group the day to ensur the requir level of data privaci anthoni sciolist yes team are requir to publish public ashojae the clarif havent been made yet mkarbowski as jphoward keep point out there often a massiv disconnect between realiti and the content of a transact databas see ejlok humor post for even odder record agre see the updat evalu page we intent decid against clean the data so as not to impos our assumpt on particip domcastro one of kaggl first suggest was to remov the registr fee for info the registr fee wasnt ever to rais money but to tri and deter peopl who werent serious from download this sensit data kaggl point out that anybodi with malevol intent would probabl still pay the modest registr fee so it effect would be to deter peopl who didnt think they had a chanc of win kaggl went on to argu that these peopl may also come from interest background and may be the one most like to appli creativ think to the problem dih includ inpati admiss and emerg room visit as mention previous you dont have enough detail to calcul it from the claim tabl we want the forum to be tight integr into the site e g to be abl to link to forum post from profil and vice versa yaf is the best net forum softwar out there and integr it into kaggl is more troubl than it worth also moserwar is a brilliant programm so it the type of thing he could put togeth in less than a week realworld data is messi well put up a data dictionari soon quotedaveim serious i understand the need for random and anoym the data but unless they have some way to unrandom it afterward ani algorithm we creat will serv no real world applic quot daveim the data is messi not becaus it been peturb but becaus it realworld data anonym focus on general again not peturb the the nineyear old pregnant male actual exist in the raw data for info im told that this is one of the cleaner medic claim dataset around mgomari the differ between and is count as two day overlap were account for so were not doubl count fjn pi doe not have to be an integ blonchar your correct hpn are limit in what it can releas by the need to protect patient privaci frankthedefalco com or the women who have been treat for erectil dysfunct mgomari the answer to both question is yes jesenski you will be abl to use day in hospit y and day in hospit y as an input to day in hospit y i like your think on the use of other data loophol if the answer had been no creativ think cybaea mani thank for a great discoveri after do some dig weve discov that the oddeven observ is an artifact of the clean procedur we have work out a remedi and it will be appli to the dataset that will be releas on may in the meantim it shouldnt make a huge differ to model that are current be develop boegel yes on may we will be issu signific more data day in hospit y csv will be chang then liveflow i may be misunderstand the question but the competit requir particip to use data from y y and y to predict y no some y patient are no longer elig in y we still provid y patient who arent elig in y becaus theyr use to train on no again for privaci reason inform man that is not the intent of the rule the hpn lawyer are work on clarifi this at the moment dougi d everi member list in day in hospit y is elig to claim in y so if they have dih they are abov the same will appli for the member list in day in hospit y and day in hospit y when we releas those file day in hospit is calcul base on the length of stay variabl howev you dont have enough detail to calcul day in hospit from length of stay irwint good pickup thank now fix gschmidt not sure if this answer your question but the geograph spread is limit to the area in which hpn oper southern california i believ as to whether patient chang doctor on may youll have a few year worth of data so will be abl to work this out alexx the hpn lawyer are work on a clarif this will be releas by the time entri can be made on may metaxab the competit was design this way to replic how the model might be use in real life in a real life situat you wouldnt be abl to predict hospit with contemporan claim day in hospit y is deriv from the claim tabl where a hospit stay includ an inpati stay or an emerg visit note you dont have enough inform to calcul day in hospit y from the claim tabl hi drew it will be in place by may when entri are accept anybodi who accept the exist rule will receiv the notif via email anthoni in this dataset miss pay delay either mean unknown or greater than in the may releas the anonym team will topcod pay delay so there will be fewer miss valu and will mean you will get some procedur code inform in the may releas i understand the frustrat but data privaci is a prioriti for hpn for generat featur i recommend s q l lite though my s q l doe the same thing i know jeremi and jeff like cs linq for build model i use r rks we will post a sampl entri with the rest of the data on may ralph h day in hospit count day not night so if day in hospit is then they have not been to the hospit at all if they were in and out of the er then day in hospit would be trezza and rhm y contain data for a period trezza unfortun not the anonym team have identifi this as a data privaci risk hi allan that becaus some member have had claim suppress in releas come soon well make it clear which member this appli to anthoni hi domcastro can i use r yes can i use weka yes can i use excel yes if i organis the data in a novel way and just use a standard process algorithm such as naiv bay is this ok yes you must preserv the order in target csv releas zip doe supersed releas zip unfortun not apolog for ani inconveni darragh it a list of all member in the dataset no chris just heard back from the data anonym team member have been renumb cacross hpn had a granular threshold that they want to remain below some l o ss had to be suppress to achiev this target if there is a blank los and sup l o s is then this is how it was when it came out of the hpn dataset if there is a blank los and sup l o s is then the los has been suppress hope that help mkwan you fill in the team wizard when you make your first entri team merger will be grant at the organ discret yes chris r nice to see you compet in this sampl is random we cant give you an hpn benchmark becaus theyv not tackl this problem befor boegel day in hospit y contain member who made a claim in y and were elig to make a claim in y day in hospit y contain member who made a claim in y and were elig to make a claim in y similar target csv contain member who made a claim in y and were elig to make a claim in y to be elig mean to be an hpn member regardless of whether or not a claim was made therefor the member in day in hospit y are not miss from target csv but rather didnt make a claim in y or werent elig to make a claim in y therefor all member in target csv were elig to make a claim in y so we have an answer for each of these member jesenski by my calcul member appear in day in hospit y and day in hospit y but not day in hospit y perhap you can confirm this figur these member are miss from day in hospit y becaus they didnt make a claim in y despit be elig apolog if we didnt communic this effect in the descript page dan b your right about the select bias but becaus hpn are releas almost no inform on the member themselv there noth to model on for patient without claim pro tester there noth in the raw data that distinguish a death from a patient that leav hpn for anoth provid ssrc map los to dih is imposs not everi los entri correspond with a dih e g hospic stay one reason somebodi may have dih in y but no claim is if they werent elig to claim in y in which case their y claim wouldv been remov georg there are member in the dataset but you are onli test on member that becaus the extra member arent elig to claim in y or didnt claim in y they have onli been provid to help you train your model tom sf hain jeremi is not the author of the rule he is mere tri his best to point peopl to the section that make the rule as competitor friend as possibl given h p ns requir also if you would like to publish your algorithm i strong encourag you to put in a research request use the contact us form is the maximum ive said this befor but i think jeremi tutori is realli excel although it is not focuss on hhp he is hope to get the opportun to do an hhp tutori in the next few month further to will point those who follow the netflix prize will rememb the jump from the simon funk discoveri she will be ad in the next releas darragh i pass your question onto hpn here the repli is there a delay between the schedul of the surgeri and when it take place yes but that is just a matter of schedul not someth forc by the govern it would also of cours depend on how urgent the surgeri is the intent of that provis is to prevent the data be share with those who have not agre to the competit rule jeff was just refer to the measur he would take to ensur the data isnt access to other jose thank for your dilig on this it difficult for us to give specif guidelin again hpn is just tri to prevent the data from be access to those who havent accept the rule jim it be assess against y hospit bernhard your interpret sound about right to me thank dave the data descript has been fix hi guy glad you like this dataset remind me of the rta data which was realli popular on the ip question when no rule are explicit state the kaggl term and condit prevail specif claus by accept an award you agre to grant a licens to the competit host to use ani model use or consult by you in generat your entri in ani way the competit host think fit this licens will be nonexclus unless otherwis specifi anthoni hi bobbi can you clarifi what you mean by this are you ask if they are oblig to share their model if they finish in first place anthoni hi willem to what extend the result have to be ident for exampl small differ in the random number generat may give differ result although they should be similar they do need to be ident you can give your random number generat a seed to make sure the resultl are the same each time in how much time should the result be reproduc my current best result is a mix of mani model each may take minut to hour to generat there is no rule about execut time the algorithm should produc similar result on a new dataset this doesnt sound veri realist i dont think there is ani way to win this competit without optim for this specif dataset result on other dataset may be veri bad with the given optim probabl veri good result can be produc by the same algorithm after some tune but this is a process that requir a lot of knowledg about the use algorithm and a lot of time and patienc not sure i follow whi this is an issu rememb the mileston prize is judg in a portion of the test dataset that particip have not been given ani feedback on perhap im misunderstand the concern hth antthoni regard the requir that solut be ident willem it would be better to have particip spend time on innov rather than reproduc howev it import to have strict rule so that the competit remain as fair as possibl b yang with regard to the compil issu we can address it if the issu aris for exampl we might start by ensur that the same compil is use for verif sali mali it is except to describ the algorithm and not how it is deriv we are seek clarif from hpn on the inconsist that you describ apolog for the delay regard the requir that the algorithm perform similar on a separ dataset this is best answer by explain the rational behind the rule it is there to catch ani cheat or blatant overfit if your not blatant overfit then your like to be on safe ground hi all not ignor this thread just seek clarif from hpn on one issu anthoni john onli the lowest of the five entri count note for the mileston prize onli one can be select anthoni i have check with hpn and a mileston prize winner can choos not to disclos their method but will not be elig for the mileston prize sorri for the delay on this was just clarifi some issu with hpn is it inconsist as sali mali point out in anoth thread to requir document of the win algorithm be public disclos to all competitor given rule entrant represent it seem that this disclosur will encourag other competitor to use aspect of the win predict algorithm which caus violat direct or otherwis of i iii and possibl iv of that rule rule doe not appli to the extent that it prevent a competitor other than a mileston prizewinn from use code publish by a mileston prizewinn in accord with competit rule and b a mileston prizewinn from compet subsequ in the competit use code for which it was award the mileston prize can you clarifi that code librari and softwar specif are not requir to be public disclos to competitor these materi and intellectu properti appear to be referenc separ from predict algorithm and document chris correct point to jeremi respons in an earlier forum post “ onli the paper describ the algorithm will be post public the paper must fulli describ the algorithm if other competitor find that it miss key inform or doesnt behav as advertis then they can appeal the idea of cours is that progress prize winner will fulli share the result theyv use to that point so that all competitor can benefit for the remaind of the comp and so that the overal outcom for health care is improv ” will kaggl or heritag have a moder or appeal process for handl competitor complaint from the win entrant pointofview they would not want to be forc through the review process to allow backdoor answer to code and librari which acceler a competitor integr of the win solut kaggl and the hhp judg panel will moder the appeal process can you comment on the spirit and fair of the public disclosur of the predict algorithm document and it impact on competit in particular if the document truli doe meet the requir of enabl a skill comput scienc practition to reproduc the win result then this place the win team at an unfair disadavantag all competitor will have access to their algorithm and research in addit to the win algorithm this rule is in place to promot collabor those who would prefer not to share can opt out of the prize can you provid more detail clarif on the level of document requir by condit mileston winner the guidelin provid by the rule would cover a rang of detail and descript span from lectur note to detail tutori to whitepap to confer paper etc hope this was adequ dealt with in jeremi respons requot abov let me know if further clarif is need can you comment on the reproduc requir for exampl it is possibl to construct algorithm with stochast element that may not be precis reproduc even use the same random seed is it suffici for these algorithm to reproduc the submiss approxim what if they dont reproduc exact or reproduc at a predict accuraci that is wors than the submiss score possibl wors than other competitor submiss exact reproduc is requir correct if you were per cent sure that somebodi would spend day in hospit in y and per cent sure they would spend day in hospit than you might predict that they spend would day in hospit pham you do not have enough detail in the claim data to reproduc the dih proper youv like reproduc dih from claim data as accur as is possibl sir guessalot thank for the pointer it been ad to our issu tracker i must admit we have higher prioriti issu to tackl but well get there eventu just to keep you all in the loop the plan is to announc the mileston prize winner at o reilli strataconf will let you know the exact date as soon as were told full mileston prize rank will be releas after the announc is made the rule do not prohibit oracl data miner hi all hpn are current look for data scientist heritag provid network the sponsor of the heritag health prize is look to hire data scientist to take it data and analyt depart to the next level if you are interest in healthcar join the largest physician group in california and one of the largest in the unit state and use your data mine skill to make a differ in the provis of health care to individu throughout southern california if interest pleas send an email indic your interest to datascientistheritagem com anthoni provision mileston prize winner will receiv an email over the weekend an announc will be made at strataconf on septemb jason the anonym guy have withheld this inform intent to make the data set more secur sorri correct libraryrandom forestsetwd c usersantgoldbloom dropbox kaggl competit credit scoringtrain read csvcstrain csv r f random foresttrainingctrain serious dlqinyr sampsizecdo trace t r u eimport t r u entreeforest t r u etest read csvcstest csvpred data framepredict r ftestcnamespr serious dlqinyrswrit csvpredfilesampl entri csv alec set the random seed is a good idea domcastro your hypothesi is correct your correct shouldnt includ header congratul team market maker and willem great coverag in the wall street journal here for those interest here the footag from the award ceremoni doe be a member of hpn mean you usual refer to an innetwork provid of say lab test unless obviosuli it is some specialti unavail yes can you be a member of hpn and have govt sponsor insur eg medicar medi cal yes for medicar i can follow up on medi cal if you like have pass these question onto hpn will respond as soon as i get an answer on octob the judg in their sole discret decid whether or not the document is suffici take account of the comment made on this forum if they decid the document is not suffici they can impel the winner to address their concern in the seven day follow octob if the winner are ask to resubmit particip have anoth day from novemb to rais ani addit complaint the judg panel are experienc academ review hi all we are in the process of liais with the judg well report their decis as soon as we have everybodi feedback we have made a slight chang to the term and condit ad no individu or entiti may share solut or code for ani competit or collabor in ani way with ani other individu or entiti that is particip as a separ individu or entiti for the same competit the forego shall not appli to ani public communic such as forum particip or blog post we are also awar that the rule havent been as clear as we might have like from now on befor you download the data for ani new competit you will be remind that you cannot sign up to kaggl from multipl account and therefor you cannot submit from multipl account and privat share code or data is not permit outsid of team share data or code is permiss if made avail to all player such as on the forum weve reach out to sever team about this issu pleas let us know asap if you have multipl account and weve not reach out to you we are awar that the rule havent been as clear as we might have like pleas be remind that you cannot sign up to kaggl from multipl account and therefor you cannot submit from multipl account and privat share code or data is not permit outsid of team share data or code is permiss if made avail to all player such as on the forum weve reach out to sever team about this issu pleas let us know asap if you have multipl account and weve not reach out to you it is a mistak were sorri for it but weve decid not to correct it becaus it might not be fair to some contest if we chang the data midstream shouldnt be too importanton happen to chunk it the same mistak that caus a few chunk to have some miss data within the chunk sound like there a thrive communiti in melb which look to have been the strongest perform citi congrat all thank for the nice wish of cours kaggl wouldnt exist without a brilliant communiti of data scientist who can solv realli challeng problem look forward to see what we can do in donovan weve look into this and it turn out that a bug with our process meant that we hadnt receiv the past few week of queri weve found your email and you will receiv a respons short as will other who slip through the crack apolog to you and other who have not receiv a respons as a result of this error one of my cowork said were realli do well if you think of kaggl everi time you see the facebook logo nice for interest we typic see strong metric on kaggl dure holiday becaus peopl have more discretionari time which at least suggest our communiti isnt too busi with famili anoth possibl explan is that peopl have exhaust their travel budget both time and money on holiday travel and need to wait a littl while befor book more travel clear and entertain nice work whi doe lower bound get mention so much more than upper bound ive play with pca befor but never associ plot or mca glad to see an exampl usag and be abl to add these to my toolkit thank you for the associ plot i assum the width of the box refer to the number of tweet refer that that airlin i assum is the proport of comov explain by the first dimens is that correct is it typic for the first dimens to explain so much of the comov ani thought on how to interpret this dimens small nit you might want to chang res to reason i initi assum res stood for residu and reduc the font size for the x axi label on plot that the most interest plot to me but it hard to read the label becaus they overlap this is a nice notebook suggest to make this easier to follow for those who havent yet look at the data itd be great if you ad a section show a few row or possibl even a few exploratori chartshistogram perhap after the load the data section itd also be nice to see the befor and after you preprocess the data ie befor and after the use textmin to format our data section renam the use textmin to format our data to someth like clean the data great i alway look at the top rate notebook befor look at the data becaus the notebook usual give me a sens for what in the data and what i could do with it love it interest that for everyon other than woodrow wilson the name popular monoton declin over the cours of the presid dwight look like it increas in popular dure ww which make sens one suggest is to add year to the x axi label for each chart to make thing like this easier to spot i tweet this script and somebodi repli ask is there a correspond drop in the name frequenc of the lose presidenti candid right after the elect i was think anoth interest extens would be to answer the question what most influenti in determin babi name trend out of presid and first ladi musician that was for longest on the billboard chart in a given year best actoractress in the oscar basketbal footbal basebal m v ps nobel prize winner name time person of the year if nobodi els tackl this i might tri it this build off a convers i had with my cowork meghan who said itd be interest to see whether presid or royal babi had a bigger impact on babi name from this page this is great im surpris north america is not higher for sugar the sweet of food was one of the first thing i notic when we move to the us from australia although it could be becaus a lot of the sweet come from high fructos corn syrup which is not captur nice done and fun write style one addit conclus is that real data is messi big data borat captur it best in data scienc of time spent prepar data of time spent complain about need for prepar data interest how noisi the veri earli year are i suspect the s data is veri poor qualiti realli nice script interest to see the temperatur uncertainti chart give a nice visual of when the data start becom more reliabl also nice idea to put dt into a variabl import plot to see it relat import obvious would have been more interest if wed provid more data one suggest is to better label your plot there some good stuff here but it stake a while to figur out what each chart is show i actual look at your code to figur it out i suspect this script will be more popular with some label that make it easier to follow sven have you been abl to figur out an interpret of this chart thank itd be help if you label the chart and possibl ad sub label point to your interpret it may not be use for the reason you mention but it look nice would be cool to see the by citi version i assum you didnt use it initi becaus of the size of the data set btw i assum red hot would be help to have a key it awesom realli nice put togeth bluefool i thought you came out realli well is this a work in progress or is there an error the chart are show up blank for me juanchaco this is neat but itd be easier to follow if you ad a descript between chart at the moment im scan the codecom to tri and figur what each chart is show ha id not known about this for other cool as someon who live in san francisco im curious akshay this script would be more interest if you found a neat way to visual temperatur by countri love this chart and the titl is funni just a head up that were still work through the winner solut will need more time to befor announc the final result as offici quick updat we will announc the offici result on wednesday march at am et thank for the thought comment first off as alway we will not make retrospect chang to how we handl past competit includ this one when issu like this come up we use it as an opportun to evalu how we might improv in the futur intern our debat focus on three issu recognit for those who complet stage one but not stage two achiev and how the competit appear on profil th out of look more impress than th out of how point are handl recognit for those who complet stage one but not stage two we need to view the stage one leaderboard as have no weight if it get a weight we incentiv overfit or hand label for stage one achiev and how the competit appear on profil if we did what julian suggest and add stage one particip to the bottom of the stage two leaderboard we undermin our rank by make it veri easi for somebodi to get an impressivelook top achiev by finish th out of with a naiv submiss how point are handl the one chang we will make in futur is the way point are handl we will add a multipli to the number of point for a twostag competit we have not settl on a formula for do this yet but commit to communic it clear in the rule of the next twostag competit these are difficult issu but we think this approach strike the best balanc between compet consider julian respond in the other thread hi all the result on the final leaderboard are now offici congratul to the winner and all involv this is among the hardest and most ambiti competit weve host we couldnt be prouder of the result the competit has receiv some press coverag with a chanc of more to come anthoni this is the photo from the kaggl offic this lunch was one of the highlight of my six year of kaggl not someth i will forget in a hurri minor comment there a typo in the titl prelimnari should be preliminari i onli make this comment becaus it a nice script and i dont want grammar stickler to be put off the typo jeff interest seem like make the system a littl conserv in the handl of new player chang made thank for the feedback cool to get real world sku data but how would i creat a recommend engin with just s k us and ie without custom data i guess ill see when you upload your code thank allen implement most of these chang allen jeff or anyon els how do peopl typic prevent rate from becom stale with trueskil for exampl nonact racer maintain high rank i am plan to make the rank appli over a month roll window but am curious if there were other approach such as the inclus of some kind of time decay this version should now be correct the model doe not systemat make money but it onli use veri basic featur barrier weight and rider hope it a good start and somebodi can take it and build on it hi all just a follow up on the request to not share solut as mention abov this is not a legal oblig but rather a request from the host we tri and avoid request like this becaus it limit the learn that come out of a competit our takeaway from this thread is that if there is a confidenti request we will flag it up front to avoid an unwelcom surpris at the end of a competit anthoni rakhlin one thing were experi with is ask host to write blog post summar the outcom from a competit this wont be time becaus itll happen after theyv spoken to the winner and digest the result unclear what recept we will get from host but it someth were test out foxtrot did you find a bug or a mistak in my code or is it that the return just dont look right given how simpl the featur are hi foxtrot i comment out that line becaus i chang the problem to predict the probabl of victori for each hors rather than posit xgb x g b classifierobjectivebinarylogist fitdftrain dropdftrainwinpositionmarketidaxi so i that not the sourc of leakag im not actual certain there is leakag if you run the code multipl time the return switch from be posit to be negat depend on the traintest split have run it a bunch of time i suspect the expect return is actual negat right but that doesnt happen in this case becaus im predict probabl so the predict are foxtrot nice pickup thank uncom the shuffl the deck line i suspect there more leakag in this analysi becaus the trainingtest split is random and not timebas the way to get around this for this data set is to train on the form tabl and test against the runner tabl or even more use would be to get less anonym data if luke or someon els has it at the everi least race date are valuabl make it possibl to protect against leakag but more general get access to the complet raw data is valuabl everyth one doe to disguis the data destroy inform and limit what a data scientist can do with it some small exampl relat to know the venu might give inform on what surfac the hors are run on a km race at one racecours could be a straight wherea at other cours it could involv a turn this might impact the perform of specif hors and cant be account for when the variabl is venueid final for boost engag with the data set anonym data is less fun to play with than richer raw data i revers moodi bluefool downvot by upvot i dont have databas write privileg chris i built a model to project out my sep and oct result shame there no oct race accord to my model id finish first queue xkcd william nice kernel look like your predict author base on the number of comment subject etc what the think behind predict who the author is i was think it might be interest the predict the number of comment a proxi for how interest the articl is also i was look at your error chart and your comment that it seem like we are better than chanc curious how your measur that i tri to read it off the chart but couldnt see where it came from william fyi that was quick your code is much cleaner put pressur on me to go back and clean up mine this was realli just a quick analysi if i had more time id have actual look at the word be use in the post rather than guessingr on memori e g i probabl shouldv includ rnn i could also have includ packag name tensorflow kera etc in fact anoth version of this might look at the packag that are most common use by winner ps thank for the bug report on the file download well look into it i was wonder whi the data set had download jordan look great now that the data is in csv format i featur it write a first exploratori kernel to show peopl what in the data is typic help for drive drive engag from the communiti sound good i was think itd be fun to appli trueskil to this dataset i use it here id like to do it but probabl wont have time over the next few week unfortun margin revolut featur will novak analysi of their post if there are interest find made on techcrunch data ill send it across to techcrunch theytheirread may be similar interest david i have been abl to creat the rds file libraryggmap mapdata getopenstreetmapbbox c colorbw scale round save r d smapdatafilestpet rds how do i creat the text file also whi arent you use osm file the open street map xml file format rather than use g g map to export to txt or rds ive spent a while tri to import osm file for plot in python but with no success wonder if you also found this difficult ps this is for my gps watch data so the coordin abov are not the chicago coordin anoka thank my bear calcul is incorrect my next step with this notebook is to debug it how would i use np raddegnp arctan anoka i figur out my problem i was feed latitud and longitud into the calcul bear function in the wrong order feed in lat as long and vice versa ad your more eleg bear calcul formula as well thank ryan appreci you rais the concern i want to share kaggl perspect first off intern we are incred excit about the launch of codeon competit it the biggest chang weve made to competit sinc we launch in it increas the rang of competit we can run includ time seri competit such as this one reinforc learn competit and competit on larger data set on this competit specif this is the first step in what we hope will turn into a bigger relationship between kaggl two sigma and the communiti this first competit is aim at test out the concept of codeon competit gaug the communiti interest in financethem competit and get a sens for the type of signal the communiti find on a typic financialmarketsrel data set rakhlin regard your comment about anonym and the use of an api these were primarili driven by kaggl the anonym is in place to discourag peopl from look up the answer the api allow this to be a proper time seri competit we tri to make our competit fair this mean make it difficult for the rare particip who is inclin to circumv the rule ultim our goal is to give our communiti as mani opportun as we can this mean a mix of commerci competit research competit playground competit get start competit and most recent open data set ultim the communiti will select what interest them and kaggl will continu to offer the thing that reson anthoni did you tri the my submiss tab on the left hand side dashboard you could also look at some of the weather data and kernel that other have share hi jacki perhap this dataset is a good start point total agre with frustrat express here neither kaggl dhs nor the communiti want geo restrict in this case it a legal restrict dhs work hard to allow ani intern particip albeit without prize money from kaggl perspect we prefer to host a competit with a geo restrict and make the opportun avail to the communiti than not host we think this is a better albeit imperfect outcom i understand and agre with frustrat around non u s citizensperman resid be inelig for prize money none of kaggl dhs or the communiti want this restrict the reason for the restrict is the america compet act d h ss legal team work hard to find a way to allow intern particip at all albeit without prize money while not violat the act from kaggl perspect we had a choic host this competit with a geo restrict or not host at all this was a tough choic host the competit meant run a twospe competit where some are elig for prize money while other are not do this violat our desir for meritocraci where we offer equal opportun to everi communiti member regardless of educ background and of cours countri if we dont host then we dont expos our communiti to one of the most interest and valuabl dataset that ever been host on kaggl as weve seen over the past five year with the rise of deep neural network the avail of dataset allow us to push forward to scienc of machin learn not expos this dataset to our communiti meant depriv our communiti and the machin learn world more general of a chanc to push forward the scienc with a novel and challeng dataset we decid that it was better to make the dataset avail mani in this forum have said this was a mistak this was a challeng issu i hope this at least give you a window into our think anthoni kitefoil jame those speed are a actual littl slow i will upload my latest data soon but that knot upwind and knot downwind ive improv with train and there are kitefoil who are much quicker than me rounak banik nice dataset can it be updat to be current if you can make it current and updat this notebook i will send it to the ted team we did this with margin revolut and it end up get featur on their blog who know mayb the ted team will be realli interest and can highlight it in some way rounak apolog i miss that youd updat this i will send to ted finger cross they will pick it up kamil and hama chi i actual dont think hama chis expect are too high have demand user is healthi it push us to improv our product were aim to make kaggl kernel into the lead cloudbas workbench for data scienc not just a free comput environ were migrat onto gcp and make some architectur chang that we expect will make a big differ stay tune anthoni we have spent consider time discuss the situat intern here at kaggl and with zillow we thought kaggl was put suffici emphasi on zillow elig criteria by post it on the competit overview page it clear from this thread that this nonstandard elig rule was not suffici public to the communiti as an acknowledg of this we are make the follow adjust we are reinstat the affect team so they will be elig to receiv point and prize money we are ad supplementari prize if need these prize aim to avoid penal team that would have been in the top three team if we didn't reinstat team who aren't in complianc with the elig criteria for exampl a team that in fourth place on the final leaderboard will be award nd place prize money if the nd and rd place team don't meet the elig criteria note the elig criteria for round two remain unchang past below for conveni we will soon be reach out to everyon who place in the top team to verifi their employ or institut affili to ensur complianc with the offici competit rule anybodi who work for a compani that is referenc in the elig criteria will not be accept into round two if you were on a team with one or more member who are inelig you will still be accept to move forward into round two but the teammat who doesn't qualifi will not in the futur we will make a bigger effort to make nonstandard rule clearer post them as a pin forum post for exampl and invit question and clarif but pleas also rememb that the rule are import ultim it is each kaggler respons to read the rule befor choos to particip nobodi want a situat where kaggler are put consider effort into a competit that they are inelig to particip in anthoni elig criteria member of the follow entiti are not elig to particip in either round of the zillow prize contest ani commerci entiti that engag in the sale valuat or analyt of residenti or commerci real estat ani entiti that offer servic in the leas and properti manag space includ vacat rental and ani entiti that monet residenti real estat relat data offic director employe and advisori board member and their immedi famili and member of the same household of sponsor kaggl and each of sponsor and kaggl respect affili subsidiari agent judg and advertis and promot agenc in addit you are not elig to particip in the zillow prize contest if you are a a resid of a countri design as an embargo countri by the unit state treasuri offic of foreign asset control see for addit inform or b are an individu that appear on the unit state treasuri offic of foreign asset control special design nation and block person list see for addit inform bojan total agre that we dont want to incent kaggler to hide person inform in this case nobodi will be remov from the round one leaderboard becaus of person inform they have share and everybodi will have to volunt their affili to be elig for round two that remov ani discrimin base on public share user inform or wire magazin profil competit with elig criteria are quit rare so we havent figur out all the nuanc cant promis we wont make mistak in the futur but we aim to keep improv we spoke about that option in connect with the tsa competit it not clear that it was legal in that case this question is move into a more general discuss of elig criteria if we want to move the convers to a more general discuss of elig criteria i suggest we move it to general the elig criteria still appli to entri to round so onli team that meet the elig criteria will be accept into round have you tri trueskil befor it should be more power becaus from memori it doesnt assum a fix standard deviat so it take into account the certainti about a rate when adjust rate after a game itd be interest to compar the perform of elo vs trueskil there a nice python implement of trueskil love this stori tenac count for a lot well done ryan on monday numerai announc that they were give away their cryptocurr to kaggl user with a rate abov novic and account creat befor march this wasnt done in partnership with kaggl we had no idea it was come follow that announc there has been a massiv increas in the number of login attempt to the kaggl websit these are attempt to break into kaggl account in order to claim the cyptocurr airdrop this is both stress our system and put kaggl account at risk as a result we have remov the abil add as your websit url forc a password reset for anybodi whose websit was set to sent an email to the numerai ceo let him know that this has happen if you find that your password has been reset pleas go through the forgot password flow anthoni im alway happi when the kaggl credenti give our communiti opportun that they wouldnt otherwis have in this case the way offer was structur effect creat a bounti for hack kaggl account and the fact that we had no notic meant that we couldnt think through the implic and prepar were go to look at chang to the kaggl login flow this week numerai has suspend the kaggl airdrop nathanforyou nice featur note these communiti guidelin are replac by revis guidelin avail here the kaggl communiti has a lot of divers with member from over countri and skill level rang from those learn python through to the research who creat deep neural network we have had competit winner with background rang from comput scienc to english literatur howev all our user share a common thread you love work with data as our communiti grow we want to make sure that kaggl continu to be welcom to that end we are introduc guidelin to ensur everyon has the same expect about discours in the forum be patient be friend discuss idea dont make it person threat of ani kind are unaccept lowlevel harass is still harass the kaggl team determin whether content is appropri if you see someth that violat these guidelin you can bring it to our attent use the flag option on messag and topic if you have a serious concern you should report it to supportkaggl com all report will be kept confidenti sad to share that kaggl master vlado tomecek pass away on sunday octob i receiv the note attach abov from dana vlado sister i rememb sit in on his call with draper lab the competit that he won his solut involv a tremend tenac and creativ in our exchang dana mention that data scienc was vlado passion and main focus on behalf of the kaggl team we wish vlado famili well and we will miss his presenc in the communiti justinminsk this is nice fwiw i suggest updat the titl to someth like identifi the best colleg wine the kernel is more fun than the titl suggest faraz has this been address if not send me a privat messag with a link to the comment and ill make sure we take a look asap i'm a longtim competit lurker whos final muster the courag to join a competit 😅 i pick this competit becaus i find the topic interest if i was go to colleg today i'd probabl pick biolog and also as a chanc to tri out googl auto m l vision kaggl is part of googl now which give me exposur to some of the technolog that are generat excit insid googl auto m l is a tool with a lot of buzz i hadn't paid much attent to the autom machin learn tool a m l ts until the kaggl day competit in san francisco this past april two a m l ts googl auto m l and h end up particip in the hackathon and get top perform here the googl ai writeup of their perform while i believ it requir human creativ to do problem setup and domainspecif featur engin kaggl day left me wonder whether human need to be do generic featur engin architectur select pick activ function and set learn rate i plan to share detail of my experi and experi with googl auto m l vision in this thread i'm also open to suggest from other in the communiti for thing i should tri but bear with me if i take sometim to respond like mani other kaggler i have a busi day job first few experi i start with the imag creat by xhlulu kernel thank you xhlulu in that kernel xhlulu convert the data to x rgb jpeg by default googl auto m l random split between train valid and test and the predict output label and confid score for each imag i defin a threshold and it output all label and confid abov that confid threshold i made predict for both site and then pick the label that had a higher confid score across both site below is a tabl of my score note auc is the accuraci metric that googl auto m l vision report haven't put the time into figur out what it mean but i assum it probabl treat each class as binari onoff and then aggreg up train hour resolut number of imag extens auto m l auc public leaderboard score jpeg jpeg jpeg jpeg i then made small modif xhlulu code to creat x rgb pngs to test the impact of higher resolut imag train hour resolut number of imag extens auto m l auc leaderboard score png png png png png is my best score it a result of a poor thought out experi so i'm surpris it my best score i train a model for hour that includ the control imag but after inspect the testset predict i realiz my model was often pick label which are control label that don't appear in the test set i end up filter the testset predict to go to the next most confid label when a label was predict train hour resolut number of imag extens auto m l auc leaderboard score yes png yes png yes png yes png yes png yes png googl auto m l vision allow me to defin my own train valid and test split for my next experi i'm go to start use this featur i'm go to defin my own split to includ the control data in train but not valid or test to allow the model to use the control data for train but to discourag the model from predict label on the competit test set split by experi rather than random experi with googl auto m l vision i'm get pretti decent result consid how naiv my model are i'v found googl auto m l easi in mani way to get a basic model all i need to do is upload the rgb imag to googl cloud storag upload a csv file with a pointer to the gcs bucket and the target label there are a bunch of limit with the product my biggest issu so far have been inabl to add other metadata e g would like to be abl to add metadata on control plate id and posit on plate i can probabl get around this use a postprocess step but it'd be nice to be abl to add this metadata into the singl model there isn't a featur that allow batch predict on a larg number of imag i work around this by hit the predict api time to generat my submiss file the model evalu page is not veri help for debug model perform i had to use a lot of hour of train to get good result you can see that the model keep improv with addit train time this mean i have to wait a long time and spend a lot of money can't chang the loss function base on the forum it seem like the loss function might be an import set for this competit there are some more minor frustrat that i had to workaround which i'm happi to share if other are plan to tri it out and want to learn from some of the friction i encount lkhphuc unlik you need anywher near as mani train hour if your not use auto m l the upsid of auto m l is that you dont have to set ani hyperparamet and still get a decent model one of the drawback is it so comput intens xhlulu i actual regret convert to png it would have been a purer comparison if id also use jpeg i tri run the px convers in a kernel but i didnt have quit enough comput time let me find out how i can share the px dataset with everyon xhlulu unfortun cant share the dataset through the dataset platform thatd allow user to access the dataset without have to accept the competit rule apap im pick the site that has the highest confid score across the two site for my best model lb my predict for site and site agre of the time not sure sorri zaharch sorri for the delay i want to see if auto m l could be run out of kernel turn out it can the hour limit for kernel is not an issu becaus my kernel kick off a hour train job and then stop my next step is to share my infer code which send the test imag to the train model and generat predict label auto m l doesnt give architectur or hyper paramet recommend it just creat an end point that you can send imag to and get back predict label and the model confid in that label ttylacm dont know what hardwar googl auto m l is use under the hood it invis to the user good chanc it use t p us though ratthachat thank look like your a few place ahead of me on the leaderboard watch out i have a few new idea to tri 😏 oop i made a chang that introduc an error in version version work im current commit a version which should work as well ok now also ad the code for do infer use googl auto m l i didnt do my strongest model to avoid mess with the leaderboard i pick a model that perform a littl below to strongest perform kernel to chang model all one need to do is chang the modelid ankitkp giuliasavorgnan and jiangkun improv reliabl of kernel is a big push for the team i share this thread one question that came up are you all use g p us or just c p us jiangkun i dont follow what doe that mean manag to jump to by exploit the structur of the structur of the data mention here zaharch im realli interest in see this as well i am curious to tri t p us sometim and would be interest in summari of your experi also curious if you were abl to realiz a perform speedup when use t p us nice thank for share realli love this writeup particular how you step through the differ thing you tri and how they map to improv in your score veri easi to follow deep there is a mistak in the dataset san francisco is list as squar mile when it actual squar mile also itd be help to have a descript of the data includ where it came from and to have the area column as an integ or float rather than a strong i upload a dataset of doctor and nurs per capita for countri from the oecd im go to add a pointer to the recov case dataset in the pin dataset thread cpmpml point out that have the number of recov case could be help just point out that avail in this dataset alreadi share by sudalairajkumar nice i would havent thought of this dataset in case it help to compar with past pandem sar dataset ebola dataset mer dataset ht sudalairajkumar i found these dataset from one of his tweet i suspect it pretti easi to get on a countrybycountri basi question is whether there a good global sourc btw tri to keep this thread relat clear for actual dataset were use this thread to ask question and discuss idea for dataset nice idea but a better fit for discuss in this thread sasrdw this dataset seem to have a decent amount of what your look for that version is three year old so may be worth updat although at a glanc mani of the measur are updat pretti infrequ pretti sure this is the weather data mani of us have been look for to look at the impact of temperatur and humid on transmiss rate this look like a nice version of that dataset ad it to the data share thread davidbnn post a dataset of global weather condit at i wonder if googl trend data might be interest perhap search for hand wash or hand sanit in a particular citi might correspond with a lower transmiss rate or even just search for covid in a particular citi might correspond with a lower transmiss rate indic peopl are take the virus more serious in that citi hannesmarai im not an epidemiologist but at a glanc this look realli impress i had a quick look and saw a few place where your system seem to be answer the prompt this seem to give an answer to the persist on surfac question this seem to give an answer to the immun respons question are there mani place where you think your system is accur answer the prompt are you in a posit to give a bit more background on what your system is do savannareid i would also be realli interest at a glanc and to my untrain eye hannesmarai system look realli impress jaimeblasco upload one for school closur i assum countri level data is still like use if you can match the file in that dataset i can like updat it on their behalf otherwis i suggest you just creat a new version nice mind also share it in this thread nice to have all public dataset in one thread as i understand it this has been an influenti preprint show the relationship between temperatur and humid and transmiss rate it look at chines citi and find that a degre celsius increas in temperatur lead to a drop in r and a increas in relat humid lead to a drop in r as i understand it r is a a common use measur by epidemiologist known as the effect reproduct number and is the averag number of secondari case per infecti case if r is greater than then an epidem will spread if it less than it will die out my understand is that the estim of r for covid is in the rang of cpmpml nice dataset do you mind ad a pointer here want to have all extern dataset on the same thread the economist did an analysi of tourism flow tofrom china a few week ago could look at the dataset they use as a start point koryto this is a great direct make a lot of sens to start join a lot of these dataset into a tabl absolut davidbnn this is great will save a lot of user a lot of pain and make it easier to explor the impact of weather nice work nightrang nice start to join datacr featur matric is a realli nice way to make the dataset surfac in this thread more use koryto one direct you can take is tri to combin as mani of the dataset mention here into your featur matrix as you can make fit nice glad to see that larger and larger featur matric be built up agre complet with paul guidanc big goal is to product find easi for the medic and health polici communiti to digest this is a nice notebook itd be help if you also link to the paper as well as provid the excerpt this is veri use can you use your approach to look at word like weather temperatur and humid that is shown to have an impact on transmiss rate e g referenc here itd be interest to see what your approach turn up ajrwhit this is veri use i find key phrase most use part of your notebook few request can you print more key than key phrase when your search return more result e g chronic respiratori diseas can you print key phrase for addit you look at risk factor e g hypertens can you use your approach to look at word like weather temperatur and humid that is shown to have an impact on transmiss rate e g referenc here itd be interest to see what your approach turn up get estim for r and r from the literatur would be great one problem with those snippet is it doesnt have the actual number we have been ask by our health polici collabor to put togeth a summari page that surfac the most use find from the kaggl communiti the research on covid is move quick make it hard for virologist and the health polici communiti to stay on top of the latest they are work long day and need inform present in as clear and concis a format as possibl we'v format the summari page to be digest to them the page is current organ into three section find tool and dataset im go to focus on the find and tool section sinc theyr most relev to this challeng find map to the ten task issu by the white hous offic of technolog polici address open question about covid our goal is to help answer as mani open question as possibl and repres the state of knowledg on each of those question find should be focus concis extract quot and number out of paper and also provid a link to the under sourc it help if you can produc find that map as close as possibl to the format of what current present on the summari page some of the most impact work so far have involv simpl method like string match and regular express for those work on tool googl scholar and ai semant search are alreadi matur product if you are build a tool make sure your tool add valu beyond those product if you have contribut that you think we should be highlight on the summari page pleas email me at akaggl com or share on this thread also feel free to ask ani clarifi question on this thread most of the dataset have come from the forecast challeng howev for those interest ill provid some guidanc on what dataset are help it use to share individu dataset with promis signal it even more use if you can join to other relev dataset that virologist and the health polici communiti can analyz and by all mean use ani dataset to tri and produc find if you do make use find pleas document them in a clear written notebook that easi for other to follow we have been ask by our health polici collabor to put togeth a summari page that surfac the most use find from the kaggl communiti virologist and the health polici communiti are work long day and need inform in as clear and concis a format as possibl we'v format this page to be as easili digest to them as possibl the page is current organ into three section find tool and dataset im go to focus on dataset and find sinc they are most relev to this challeng dataset so far the forecast challeng has done a nice job of surfac potenti use dataset the most valuabl contribut have been join those dataset to creat valuabl resourc for test which factor impact transmiss shout out to davidbnn who join each region in the john hopkin univers data to the nearest weather station find these clear written notebook that help show which factor have an impact on transmiss rate call to action creat enrich dataset that would allow research to easili test which factor impact transmiss these dataset should be well document and kept updat write simpl notebook that show the impact of factor on transmiss use whether or not you find a correl if you have contribut that you think we should be highlight on the summari page pleas email me at akaggl com somethingkag this is not a templat but ajrwhit notebook doe a realli nice job address a hand of the subtask it direct address the subtask the informationdata is veri clear present skylord nice are you updat this daili if so ill includ it franck you should absolut be open to look at extern data sourc to answer the question one nice thing about start with the paper is we build up a pictur for what has alreadi been studi and where the gap are no this is not a supervis machin learn competit we are host a supervis covid machin learn competit at ajrwhit realli nice said ken ad to summari page thank nofoosport at a glanc your notebook look realli nice like have some folk with medicalpubl health background start to help out with curat from tomorrow so plan to take a close look with them ken thank for group the notebook by task make them much easier for us to process thank mike will take a look today just ad a challeng for share use covid relat dataset motiv for ad that challeng is that a lot of dataset share in this thread a are realli use b potenti less relev for the forecast challeng we want to creat a specif outlet for all covid relat dataset remanuel pleas add as a kaggl dataset that make it easili access to those write notebook for those interest safegraph have a realli interest human movement dataset it locat data from million of anonym smart phone it current on aw but they can move it to gcp to make it easier to use from a kaggl notebook if there suffici interest you can get access by fill out this form i receiv quit a bit of feedback from the healthcar and health polici communiti about our page summar communiti contribut as well as includ the titl of the paper itd be help if we also share the name of the journal in the summari tabl the name of the journal is a proxi for the the qualiti of the paper a result publish in jama or the lancet carri more weight more challeng is to also classifi the type of evid that a studi is base on there a hierarchi of evid and it help to know what evid was use to draw a particular conclus here are a few refer that explain the hierarchi of evid ajrwhit can you updat this notebook to includ the new data dump also is it easi for you to add the journal name to the articl titl column per this guidanc if so itd be great if you could put includ it if you can id format itd be great if you could put it outsid the link to make it easier to distinguish from the titl persist of coronavirus on inanim surfac and their inactiv with biocid agent the journal of hospit infect mikehoney a realli nice tool for explor the data but doesnt direct give me the answer to specif task without me choos my own filter mlconsult the chloroquin section had most relev articl so ad it to the summari page under effect of drug be develop and tri to treat covid patient mlconsult realli appreci you do this it make our job of curat much easier hannesmarai i have been mean to go back through your system and see how it been perform sinc you made improv are there question that it doe particular well on if so which one also can you explain the differ between the question set origin kaggl vs extend kaggl from savanna reid skylord ad here btw you may want to crosspost to this challeng wow this is impress itd be great if you load it as a kaggl dataset just notic paultimothymooney alreadi has it on kaggl this seem to be an import paper is it in the cord dataset zohrarezgui nice idea to pull down clinic trial data but the dataset has no inform on the therapi be trial though is it possibl to pull more inform about the natur of trial mlconsult nice ad can you pull how larg the differ is between and also what doe relscor mean mlconsult couldnt find a task that this address but includ on the contribut page becaus it come up as an open question recent nofoosport this is great ive start ad your answer to the contribut page a coupl of thing that would be help start order by date i like to present result chronolog on the contribut page also help if you can start break out answer to subtask a littl more for exampl for this subtask there rang of incub period for the diseas in human and how this vari across age and health status and how long individu are contagi even after recoveri itd be help to have paper and excerpt that just answer rang of incub period for the diseas in human how this vari across age and health status how long individu are contagi youll notic im start to break up even the subtask on the contribut page to make them more digest this is feedback were consist get our audienc is veri busi and want thing as scannabl as possibl anoth request it even help if you can break out subtask into sever compon as an exampl take this subtask rang of incub period for the diseas in human and how this vari across age and health status and how long individu are contagi even after recoveri itd be help to have paper and excerpt that just answer rang of incub period for the diseas in human how this vari across age and health status how long individu are contagi im start to break up even the subtask on the contribut page to make them more digest the feedback were consist get is our audienc is veri busi and want thing as scannabl as possibl i just share this feedback on nofoosportss excel notebook but want to make it more broad visibl also a nit but you have group two subtask accident preval of asymptomat shed and transmiss e g particular children and season of transmiss csheesun were typic find result easier to read when contributor are submit one notebook per task we had anoth call today and receiv anoth round of feedback first off were get good feedback that is head in a realli help direct the big issu is still that there limit signal on how strong the under paper is sinc covid is so new and most articl are in preprint journal med rxiv and bio rxiv that doesnt give much signal i think this is consist with what savannareid has been say they are interest in whether we can mine other signal from the dataset i think were go to get more feedback in the next week ill provid it when i get it but an exampl that came up was pull out the number of particip in a studi the more particip the more weight should be given to a studi for eg this didnt come up on the call but i was also wonder about author previous public record might be a use signal would love to add a column that a proxi for public qualiti by the end of the week if anyon can crack the code on that davidmezzetti start a discuss thread on the topic so suggest the convers continu there anoth bit of feedback we receiv is that the health polici and medic communiti are interest in tool that might help them get to answer more quick themselv the dream tool is a questionansw engin that can reliabl answer freeform question but there interest in see other tool as well that might go beyond what thing like googl scholar and other exist search engin can do if anyon build a tool that they think meet this criteria pleas either share it here or email me wow amaz you got this done so quick ultim our goal is to be as use as possibl to health polici decis maker so you are free to search pubm if you think itll improv your result davidmezzetti would it be easi to add the h factor of the last author as i understand it the lastauthor is the correspond author and the credibl of the studi rest on their shoulder that could be anoth help proxi davidmezzetti ignor the hfactor suggest ive just run it past a few peopl and the feedback were get is that it probabl not such a use metric were get feedback that some measur of sampl size would be realli valuabl in conjunct with the level of evid metric the kind of guidanc we were given was for level i either the number of studi or the total number of particip across all studi for level ii the number of particip in the random control trial etc itd be great if you could take a swing at this amaz youv made such quick progress look forward to tomorrow updat a recent paper suggest a potenti link between transmiss and air pollut paper suggest that explain region differ is itali between transmiss rate mlconsult thank for share paultimothymooney own look at tool be develop hes go to take a look at this if he think it promis itll definit need to be stood up as a web app feedback were get is mani of the end user are not technic enough to fork a notebook nice work mlconsult paultimothymooney as an fyi this is now a web app one of the main bit of feedback were get about the result were present is that decis maker would like to better understand the strength of the evid in a studi davidmezzetti has made a realli nice start by includ level of evid i spoke with stelio serghiou and byron wallac byron actual use nlp to assess the qualiti of clinic trial in speak to stelio and a few other im get the feedback that pull out the under studi design might be even more help than level of evid for each studi type there are measur that can help proxi the strength of the evid below is a tabl that attempt to map studi type to measur these are the thing that stelio look for when hes assess the qualiti of a paper itd be great if you can attempt to extract studi type and the correspond measur to indic evid strength this appli to those present task answer in tabl as well as those who are build searchquest answer tool level of evid studi type measur to indic evid strength i meta analysi i heterogen and tau squar heterogen number of studi ii experiment studi random random sampl size loss of follow up length of follow up iii experiment studi non random random sampl size loss of follow up length of follow up iii observ studi prospect cohort sampl size loss of follow up iii observ studi retrospect cohort sampl size sampl method how was the sampl captur iii observ studi case control sampl size select bias for control when control come from a differ popul from case observ crosssect sampl size sampl method how was the sampl captur tempor did the exposur e g covid come befor the outcom e g hospit desir or after undesir iv observ case seri case studi note savannareid point out that this doesnt includ modeldriven result like those use to estim thing like reproduct rate for modeldriven result measur of evid strength are like simpl size some measur of model fit and some inform on where the under data was drawn from aravindmc nice work weve list it on the contribut page as paul said were get a lot of feedback about includ measur of the strength of evid share some of the feedback we are hear here itd be great if you could extract and includ some of that metadata in your search result arturkiulian absolut the overal object is to product someth use to the healthcar and health polici communiti if those thing help you produc more relev result then pleas do use them ajrwhit and davidmezzetti and other look to contribut the the contribut page savannareid hand code the design and sampl size for all the risk factor you can see what it look like here she also reformat the tabl can you tri and pull updat your risk factor notebook for the new data dump and can you tri and pull out design and sampl size also if you can follow this tabl format we can dump it direct onto the page we now have a more autom process for generat the tabl date studi studi link journal sever fatal design sampl hi all we now have a team of medic student help to curat the result that go on the contribut page theyv been review the result of notebook that focus on the transmiss incub and environment stabil task as a start point each student own curat a tabl for each of the subtask the goal for these review is to to come up with a standard format for those subtask just like savannareid has done for risk factor this will give you a target format to produc your result in give feedback to author of notebook that are review on how to make your algorithm more relevantaccurateus the goal is to continu to popul the contribut page which is gain a nice follow attract k uniqu visitor in the last week up from the week befor and when we have a meaning amount of coverag and a solid process for keep the page uptod as the new paper come in with less manual curat than we are current do were go to aim to start publish the updat literatur review with a lead medic journal so that it more visibl in the medic communiti encourag those of you who are interest in contribut notebook in the format help for the contribut page to join the slack channel with the team of medic student here is an invit link im go to attempt to pass on feedback im hear in this thread but join that slack channel give you more direct and interact way to hear from the expert who are review your work final if you want to see the curat process in action and the notebook that are be review you can check out this googl doc pleas let me know if you have a notebook you want review for a particular subtask and i can add it to the queue the queue is maintain on the master sheet tab some consist theme in the feedback so far often algorithm are pull snippet from the abstract when the key number find or result is in the full text of the articl with the summari in the abstract be too high level to be use often algorithm are pull snippet that are a refer to work from anoth paper rather than work that is core to the paper be cite this challeng is get the attent of the health polici and medic communiti who are work night and day to better understand covid in februari covid paper were publish per day in march that increas to by mid april that number is up to it is veri difficult to stay up with the literatur weve been take some of the most promis notebook and work with a larg team of volunt epidemiologist m ds and medic student to turn those result into a regular updat literatur review you can see the work in progress here now that we have a clearer understand of what this need to look like were call on our communiti to more direct feed this effort by output result in a standard format all notebook should output csv file in the format list in the task descript how you can contribut read the instruct within each individu task and make a submiss we also encourag particip that have made submiss and want feedback to join this slack channel it includ the domain expert who are curat the tabl this challeng is get the attent of the health polici and medic communiti who are work night and day to better understand covid we have heard feedback that an a ipow literatur review is a power way to synthes new research i have start a new thread outlin how you can feed into an a ipow literatur review that thread aim to be a clear and direct way you can make contribut that will have impact i am unpin this thread to direct convers there mlconsult look like youv done a realli nice job of pull out sampl column nice one other than that column im not see a close map to the tabl format mention abov am i miss someth oop my bad read too quick dirktheeng weve been chat with kylelo and team about the import of have tabl as paultimothymooney mention theyr work on approach to make the tabl machin readabl do you think you could reliabl take imag of tabl and turn them into panda datafram defer to kylelo but that may be a way to get you tabl sooner anthoni nice updat one chang that would be help is if you can break out studi studi link and journal into three separ column per this guidanc that will result in less manual curat to get the result into the format for the contribut page that was quick updat the target c s vs with addit tabl from the transmiss incub and environment stabil task sapal i just tri it from incognito and it work for me and other have been get in perhap you hit it dure a temporari outag suggest tri again sasrdw thank for start this thread david ani sens for how the imh model would have perform in these competit curious where would it have place made anoth updat to the target c s vs chang to risk factor remov tb chang bmi to overweight ad an extra two column to break out the result column and ad extra data chang to tie clean up the season tabl ad extra data fix issu found via error check wow this is realli nice interest that the kaggl communiti keep get stronger vs ihm i guess there are three possibl explan the kaggl model are get better as more data come in more empir and less theoret than ihm the kaggl model are get better becaus peopl have more time to work on them ihm are better at forecast further out sound like you believ it david also realli curious about week is the trend continu or are we plateau vs ihm assum you left it off becaus we dont have enough data for week yet david funnili enough i was just look at his dashboard i assum theyr all output data in a standard format if hes abl to put them on the same dashboard is it easi for you to benchmark the perform of those other model as well were tri to figur out whether the top kaggl model are like ad valu beyond the standard epidemiolog model were consid launch a longer run competit and combin top perform kaggl model into an i h m estyl dashboard for interest section of this paper discuss the metric use by the epidemiolog model communiti a nudg we still have tabl that need to be claim if you think it someth you can help with pleas see the relev list of task i did some more benchmark i compar the current leader for week and with ihm and lanl model across three loss function mae rmse and rmsle im onli compar the forecast for fatal for us state in week and the kaggl leader clear win on rmsle but perform a lot less well on the other loss function cpmpml regard your comment about the kaggl comp optim differ thing the same is probabl true for ihm and lanl on mae and rmse it look like you basic get good score by get new york and mayb new jersey correct they account for of fatal and get new york right involv predict a data revis nyc revis their fatal measur to includ probabl covid death on april this revis start includ peopl who die but never receiv a covid test but had c o v i dlike symptom this chart come from invers excel notebook that show chart for lanl ihm and two kaggl ensembl on a state by state basi cpmpml the basi for the claim that get new york right is most import come from the observ that new york account for of the total absolut error for the kagglelead and ihm model so that state is drive most of the error i havent had a chanc to take a close look at what happen with your model your number are realli impress across the board unfortun i have to return to my day job now though this was a small weekend project that i did dure my daughter nap david i bet if you ask ihm and lanl if the benchmark is fair theyd also have complaint as well optim for a differ loss function etc nonetheless i was curious to see what would happen if i benchmark across a rang of metric that the model didnt necessarili optim for i was hope it might tell me someth about the robust of result to explain whi this setup lanl onli doe forecast for us state so need to restrict to this subset to includ them and lanl and ihm model updat dont perfect correspond with our competit date made anoth updat to c s vs cover the new data and add a bunch of new column nice present thank for share ive put togeth got a notebook that benchmark the perform of profession epidemiolog model and compar them with a strong kaggl model i was curious to keep track of how the kaggl model compar with the profession epidemiolog model for now im onli track one kaggl model the previous week leader to avoid make a select expost so the week model im use is one of sasrdw and david model it current do realli well although interest it make veri differ forecast from the profession epidemiolog model i have creat a benchmark panel that includ actual and the forecast from differ model at differ point in time here the notebook that generat the benchmark panel encourag other interest to play with it either add one of your model or play with some kaggl ensembl can download submiss file from public notebook mrisdal you are domin enough leaderboard i need to pick up my game maintain this thread to describ known issu if you post an issu it help if you can link to a notebook that is prefix the the titl issu the conveni file have negat daili case and death count in some place the raw file are cumul case and death count the conveni file the diff of the raw file here a link the notebook demonstr the issu with some of my earli attempt to understand it pleas add ani request you have for this dataset in this thread if ani request get a number of upvot indic some amount of broader interest i will attempt to address the request add data in the jhu daili report file for us counti and the global file for global file this includ incid rate and case fatal ratio for us counti file it includ incid rate peopl test peopl hospit mortal rate test rate and hospit rate includ popul by countri in the global metadata file remov provincest from the countri level data to make it more conveni to process yes and also mean you can schedul a notebook to run on a recur basi xhlulu i use g c ps cloud schedul combin with the kaggl api but a few peopl have ask about schedul function so we are consid ad it as a featur if this is interest to you itd be great if you could contribut to the discuss thread that mrisdal start earlier today fvcoppen i want to add addit data sourc over time so if you find good data sourc you think i should add pleas let me know sagnik have you seen this post i describ the issu there let me know if you have suggest for resolv it maithiltandel it look like a nice dataset in the dataset descript itd be help if you explain more about the dataset itself rather than the find for exampl where the dataset come from a link to the raw sourc perhap a link to the notebook you use to transform it look like a great dataset can you add document would love to know more about the dataset where it from what it can be use for etc sansuthi i tweet the dataset do you have a twitter account you want me to link to in a thread on that tweet mohit fyi think link you post is broken i think this is the link you meant to post work if you copi the text version click the link url take you to have an issu with the dataset that you want resolv request for addit metric that should be ad pleas add your suggest to this thread kaviml thank for the love post it realli motiv for us to receiv comment like this surajjha in repli im tri to compet with herbison to be the best among the staff 😂 joke asid thank you also for your nice word to the kaggl communiti today were share that d sculley will be take over as kaggl new leader alongsid other ecosystemfocus machin learn effort ben and i are leav kaggl and googl for our next adventur go back to our startup root kaggl recent pass it year anniversari we start with a lightheart competit aim at predict the vote matrix for the eurovis song contest in back then it would have been hard to imagin that kaggl would play a meaning role in the futur of machin learn and ai there are sever aspect of what kaggl has achiev that we are realli proud of first and most import the impact on peopl live mani have learn machin learn through kaggl of our almost mm user mm have submit to the titan get start competit and almost mm have complet exercis from kaggl cours k colleg cours have run classroom competit with k student submit to those competit and it not just those new to machin learn who use kaggl to learn advanc user get handson experi on lot of differ problem and the opportun to learn from the win solut as grandmast vladimir iglovikov like to say “i think about machin learn competit as about a gym but for a machin learn muscles” kaggl has also provid a credenti to the machin learn world we have given those who are suffici determin anoth way to break into the field alreadi back in facebook was use kaggl to find strong machin learn talent by we were well establish as a great way to land an elit ai job today wellregard ai compani like n vidia and h ai hire team of kaggl grandmast we'r also proud of the role the kaggl communiti has play in highlight what work well in practic machin learn paper per day are publish on arxiv and countless machin learn packag are develop kaggl user explor these techniqu in a competit environ and spread those that work framework like kera and x g boost took off in the kaggl communiti along with use preprocess and data augment librari like albument for comput vision mani techniqu have spread through kaggl includ u net for segment denois autoencod and adversari valid and kaggl has help prove out new applic for machin learn includ medic imag and autom essay grade and final we'r proud of the fact that while we start with machin learn competit we'v launch other servic notebook have increas the way our user can share learn and our cours have made kaggl more approach to new user and there no machin learn without data and we'r proud of our collect of over k public dataset which make kaggl one of the world largest repositori of public dataset of cours none of this would have been possibl without both the communiti and the kaggl team past and present over the year we have had the privileg of meet so mani passion and energet communiti member with so mani inspir stori and the opportun to work with a talent and motiv team we'r grate to all of you and while we are proud of what kaggl has achiev so far we'r also veri excit to see what the d and the team accomplish in the year ahead d has been oper on the cut edg of machin learn for the past year work on veri larg scale machin learn system insid googl do foundat research on topic like m l op and lead larg research team d also has a long histori with kaggl start with the semisupervis machin learn challeng he launch in which as it happen was won by some familiar face we'r excit what the combin of d ’s background and histori with kaggl will bring to the futur for those who want to get to know d better he is go to be answer the most upvot question over on this post anthoni and ben\""
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "forum_data_agg['clean_messages'][1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.model_selection import cross_validate\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import ExtraTreesClassifier\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "\n",
    "kfolds = StratifiedKFold(n_splits=5, shuffle=True, random_state=1)\n",
    "\n",
    "scoring = {'acc': 'accuracy',\n",
    "           'neg_log_loss': 'neg_log_loss',\n",
    "           'f1_micro': 'f1_micro'}"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ExtraTreesClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "etc = ExtraTreesClassifier(n_estimators = 20, max_depth=4, n_jobs = -1)\n",
    "tfidf = TfidfVectorizer(ngram_range=(1, 1), stop_words='english')\n",
    "tsvd = TruncatedSVD(n_components=10)\n",
    "model = Pipeline([('tfidf1', tfidf), ('tsvd1', tsvd), ('etc', etc)])\n",
    "\n",
    "kfolds = StratifiedKFold(n_splits=5, shuffle=True, random_state=1)\n",
    "\n",
    "np.random.seed(1)\n",
    "\n",
    "results = cross_validate(model, train_data['clean_posts'], train_data['type'], cv=kfolds, \n",
    "                          scoring=scoring, n_jobs=-1)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CV Accuracy: 0.3882 (+/- 0.0128)\n",
      "CV F1: 0.3882 (+/- 0.0128)\n",
      "CV Logloss: 2.0592 (+/- 0.0199)\n"
     ]
    }
   ],
   "source": [
    "print(\"CV Accuracy: {:0.4f} (+/- {:0.4f})\".format(np.mean(results['test_acc']),\n",
    "                                                  np.std(results['test_acc'])))\n",
    "\n",
    "print(\"CV F1: {:0.4f} (+/- {:0.4f})\".format(np.mean(results['test_f1_micro']),\n",
    "                                            np.std(results['test_f1_micro'])))\n",
    "\n",
    "print(\"CV Logloss: {:0.4f} (+/- {:0.4f})\".format(np.mean(-1*results['test_neg_log_loss']),\n",
    "                                                 np.std(-1*results['test_neg_log_loss'])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Naive Bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(1)\n",
    "\n",
    "tfidf2 = CountVectorizer(ngram_range=(1, 1), \n",
    "                         stop_words='english',\n",
    "                         lowercase = True, \n",
    "                         max_features = 5000)\n",
    "\n",
    "model_nb = Pipeline([('tfidf1', tfidf2), ('nb', MultinomialNB())])\n",
    "\n",
    "results_nb = cross_validate(model_nb, train_data['clean_posts'], train_data['type'], cv=kfolds, \n",
    "                          scoring=scoring, n_jobs=-1)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CV Accuracy: 0.5614 (+/- 0.0094)\n",
      "CV F1: 0.5614 (+/- 0.0094)\n",
      "CV Logloss: 6.2611 (+/- 0.3302)\n"
     ]
    }
   ],
   "source": [
    "print(\"CV Accuracy: {:0.4f} (+/- {:0.4f})\".format(np.mean(results_nb['test_acc']),\n",
    "                                                  np.std(results_nb['test_acc'])))\n",
    "\n",
    "print(\"CV F1: {:0.4f} (+/- {:0.4f})\".format(np.mean(results_nb['test_f1_micro']),\n",
    "                                            np.std(results_nb['test_f1_micro'])))\n",
    "\n",
    "print(\"CV Logloss: {:0.4f} (+/- {:0.4f})\".format(np.mean(-1*results_nb['test_neg_log_loss']),\n",
    "                                                 np.std(-1*results_nb['test_neg_log_loss'])))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(1)\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "tfidf2 = CountVectorizer(ngram_range=(1, 1), stop_words='english', lowercase = True, max_features = 5000)\n",
    "\n",
    "model_lr = Pipeline([('tfidf1', tfidf2), ('lr', LogisticRegression(class_weight=\"balanced\", C=0.005))])\n",
    "\n",
    "results_lr = cross_validate(model_lr, train_data['clean_posts'], train_data['type'], cv=kfolds, \n",
    "                          scoring=scoring, n_jobs=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CV Accuracy: 0.6550 (+/- 0.0126)\n",
      "CV F1: 0.6550 (+/- 0.0126)\n",
      "CV Logloss: 1.2901 (+/- 0.0203)\n"
     ]
    }
   ],
   "source": [
    "print(\"CV Accuracy: {:0.4f} (+/- {:0.4f})\".format(np.mean(results_lr['test_acc']),\n",
    "                                                  np.std(results_lr['test_acc'])))\n",
    "\n",
    "print(\"CV F1: {:0.4f} (+/- {:0.4f})\".format(np.mean(results_lr['test_f1_micro']),\n",
    "                                            np.std(results_lr['test_f1_micro'])))\n",
    "\n",
    "print(\"CV Logloss: {:0.4f} (+/- {:0.4f})\".format(np.mean(-1*results_lr['test_neg_log_loss']),\n",
    "                                                 np.std(-1*results_lr['test_neg_log_loss'])))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\eleed\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.9_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python39\\site-packages\\sklearn\\linear_model\\_logistic.py:444: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 864x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "model_lr.fit(train_data['clean_posts'], train_data['type'])\n",
    "pred_all = model_lr.predict(forum_data_agg['clean_messages'])\n",
    "\n",
    "cnt_all = np.unique(pred_all, return_counts=True)\n",
    "\n",
    "pred_df = pd.DataFrame({'personality': cnt_all[0], 'count': cnt_all[1]},\n",
    "                      columns=['personality', 'count'], index=None)\n",
    "\n",
    "pred_df.sort_values('count', ascending=False, inplace=True)\n",
    "\n",
    "plt.figure(figsize=(12,4))\n",
    "sns.barplot(x=pred_df['personality'], y=pred_df['count'], alpha=0.8)\n",
    "plt.ylabel('Number of Occurrences', fontsize=12)\n",
    "plt.xlabel('Personality', fontsize=12)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_df['percent'] = pred_df['count']/pred_df['count'].sum()\n",
    "pred_df['description'] = pred_df['personality'].apply(lambda x: ' '.join([mbti[l] for l in list(x)]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>personality</th>\n",
       "      <th>count</th>\n",
       "      <th>percent</th>\n",
       "      <th>description</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>INTJ</td>\n",
       "      <td>90742</td>\n",
       "      <td>0.294819</td>\n",
       "      <td>Introversion Intuition Thinking Judging</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>ESFP</td>\n",
       "      <td>65321</td>\n",
       "      <td>0.212227</td>\n",
       "      <td>Extroversion Sensing Feeling Perceiving</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>ISFP</td>\n",
       "      <td>40884</td>\n",
       "      <td>0.132831</td>\n",
       "      <td>Introversion Sensing Feeling Perceiving</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>ISFJ</td>\n",
       "      <td>27949</td>\n",
       "      <td>0.090806</td>\n",
       "      <td>Introversion Sensing Feeling Judging</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>ENTJ</td>\n",
       "      <td>24879</td>\n",
       "      <td>0.080831</td>\n",
       "      <td>Extroversion Intuition Thinking Judging</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>ISTP</td>\n",
       "      <td>20638</td>\n",
       "      <td>0.067052</td>\n",
       "      <td>Introversion Sensing Thinking Perceiving</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>ISTJ</td>\n",
       "      <td>9637</td>\n",
       "      <td>0.031310</td>\n",
       "      <td>Introversion Sensing Thinking Judging</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>INTP</td>\n",
       "      <td>6561</td>\n",
       "      <td>0.021317</td>\n",
       "      <td>Introversion Intuition Thinking Perceiving</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>ESTP</td>\n",
       "      <td>5461</td>\n",
       "      <td>0.017743</td>\n",
       "      <td>Extroversion Sensing Thinking Perceiving</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ENTP</td>\n",
       "      <td>5105</td>\n",
       "      <td>0.016586</td>\n",
       "      <td>Extroversion Intuition Thinking Perceiving</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ENFJ</td>\n",
       "      <td>3303</td>\n",
       "      <td>0.010731</td>\n",
       "      <td>Extroversion Intuition Feeling Judging</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ESFJ</td>\n",
       "      <td>3001</td>\n",
       "      <td>0.009750</td>\n",
       "      <td>Extroversion Sensing Feeling Judging</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>ESTJ</td>\n",
       "      <td>1562</td>\n",
       "      <td>0.005075</td>\n",
       "      <td>Extroversion Sensing Thinking Judging</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ENFP</td>\n",
       "      <td>1340</td>\n",
       "      <td>0.004354</td>\n",
       "      <td>Extroversion Intuition Feeling Perceiving</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>INFP</td>\n",
       "      <td>1018</td>\n",
       "      <td>0.003307</td>\n",
       "      <td>Introversion Intuition Feeling Perceiving</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>INFJ</td>\n",
       "      <td>388</td>\n",
       "      <td>0.001261</td>\n",
       "      <td>Introversion Intuition Feeling Judging</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   personality  count   percent                                 description\n",
       "10        INTJ  90742  0.294819     Introversion Intuition Thinking Judging\n",
       "5         ESFP  65321  0.212227     Extroversion Sensing Feeling Perceiving\n",
       "13        ISFP  40884  0.132831     Introversion Sensing Feeling Perceiving\n",
       "12        ISFJ  27949  0.090806        Introversion Sensing Feeling Judging\n",
       "2         ENTJ  24879  0.080831     Extroversion Intuition Thinking Judging\n",
       "15        ISTP  20638  0.067052    Introversion Sensing Thinking Perceiving\n",
       "14        ISTJ   9637  0.031310       Introversion Sensing Thinking Judging\n",
       "11        INTP   6561  0.021317  Introversion Intuition Thinking Perceiving\n",
       "7         ESTP   5461  0.017743    Extroversion Sensing Thinking Perceiving\n",
       "3         ENTP   5105  0.016586  Extroversion Intuition Thinking Perceiving\n",
       "0         ENFJ   3303  0.010731      Extroversion Intuition Feeling Judging\n",
       "4         ESFJ   3001  0.009750        Extroversion Sensing Feeling Judging\n",
       "6         ESTJ   1562  0.005075       Extroversion Sensing Thinking Judging\n",
       "1         ENFP   1340  0.004354   Extroversion Intuition Feeling Perceiving\n",
       "9         INFP   1018  0.003307   Introversion Intuition Feeling Perceiving\n",
       "8         INFJ    388  0.001261      Introversion Intuition Feeling Judging"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.plotly.v1+json": {
       "config": {
        "linkText": "Export to plot.ly",
        "plotlyServerURL": "https://plot.ly",
        "showLink": false
       },
       "data": [
        {
         "labels": [
          "Introversion Intuition Thinking Judging",
          "Extroversion Sensing Feeling Perceiving",
          "Introversion Sensing Feeling Perceiving",
          "Introversion Sensing Feeling Judging",
          "Extroversion Intuition Thinking Judging",
          "Introversion Sensing Thinking Perceiving",
          "Introversion Sensing Thinking Judging",
          "Introversion Intuition Thinking Perceiving",
          "Extroversion Sensing Thinking Perceiving",
          "Extroversion Intuition Thinking Perceiving",
          "Extroversion Intuition Feeling Judging",
          "Extroversion Sensing Feeling Judging",
          "Extroversion Sensing Thinking Judging",
          "Extroversion Intuition Feeling Perceiving",
          "Introversion Intuition Feeling Perceiving",
          "Introversion Intuition Feeling Judging"
         ],
         "type": "pie",
         "values": [
          29.481885317538964,
          21.222655780421,
          13.28312577772435,
          9.080571430427987,
          8.083134874865573,
          6.705242877425769,
          3.1310410703436444,
          2.131655127376222,
          1.774267436458093,
          1.658603783760953,
          1.0731377664568909,
          0.9750186004048228,
          0.5074905211037432,
          0.43536318711844807,
          0.3307460630496866,
          0.12606038552384913
         ]
        }
       ],
       "layout": {
        "template": {
         "data": {
          "bar": [
           {
            "error_x": {
             "color": "#2a3f5f"
            },
            "error_y": {
             "color": "#2a3f5f"
            },
            "marker": {
             "line": {
              "color": "#E5ECF6",
              "width": 0.5
             },
             "pattern": {
              "fillmode": "overlay",
              "size": 10,
              "solidity": 0.2
             }
            },
            "type": "bar"
           }
          ],
          "barpolar": [
           {
            "marker": {
             "line": {
              "color": "#E5ECF6",
              "width": 0.5
             },
             "pattern": {
              "fillmode": "overlay",
              "size": 10,
              "solidity": 0.2
             }
            },
            "type": "barpolar"
           }
          ],
          "carpet": [
           {
            "aaxis": {
             "endlinecolor": "#2a3f5f",
             "gridcolor": "white",
             "linecolor": "white",
             "minorgridcolor": "white",
             "startlinecolor": "#2a3f5f"
            },
            "baxis": {
             "endlinecolor": "#2a3f5f",
             "gridcolor": "white",
             "linecolor": "white",
             "minorgridcolor": "white",
             "startlinecolor": "#2a3f5f"
            },
            "type": "carpet"
           }
          ],
          "choropleth": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "type": "choropleth"
           }
          ],
          "contour": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "contour"
           }
          ],
          "contourcarpet": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "type": "contourcarpet"
           }
          ],
          "heatmap": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "heatmap"
           }
          ],
          "heatmapgl": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "heatmapgl"
           }
          ],
          "histogram": [
           {
            "marker": {
             "pattern": {
              "fillmode": "overlay",
              "size": 10,
              "solidity": 0.2
             }
            },
            "type": "histogram"
           }
          ],
          "histogram2d": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "histogram2d"
           }
          ],
          "histogram2dcontour": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "histogram2dcontour"
           }
          ],
          "mesh3d": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "type": "mesh3d"
           }
          ],
          "parcoords": [
           {
            "line": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "parcoords"
           }
          ],
          "pie": [
           {
            "automargin": true,
            "type": "pie"
           }
          ],
          "scatter": [
           {
            "fillpattern": {
             "fillmode": "overlay",
             "size": 10,
             "solidity": 0.2
            },
            "type": "scatter"
           }
          ],
          "scatter3d": [
           {
            "line": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatter3d"
           }
          ],
          "scattercarpet": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattercarpet"
           }
          ],
          "scattergeo": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattergeo"
           }
          ],
          "scattergl": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattergl"
           }
          ],
          "scattermapbox": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattermapbox"
           }
          ],
          "scatterpolar": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatterpolar"
           }
          ],
          "scatterpolargl": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatterpolargl"
           }
          ],
          "scatterternary": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatterternary"
           }
          ],
          "surface": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "surface"
           }
          ],
          "table": [
           {
            "cells": {
             "fill": {
              "color": "#EBF0F8"
             },
             "line": {
              "color": "white"
             }
            },
            "header": {
             "fill": {
              "color": "#C8D4E3"
             },
             "line": {
              "color": "white"
             }
            },
            "type": "table"
           }
          ]
         },
         "layout": {
          "annotationdefaults": {
           "arrowcolor": "#2a3f5f",
           "arrowhead": 0,
           "arrowwidth": 1
          },
          "autotypenumbers": "strict",
          "coloraxis": {
           "colorbar": {
            "outlinewidth": 0,
            "ticks": ""
           }
          },
          "colorscale": {
           "diverging": [
            [
             0,
             "#8e0152"
            ],
            [
             0.1,
             "#c51b7d"
            ],
            [
             0.2,
             "#de77ae"
            ],
            [
             0.3,
             "#f1b6da"
            ],
            [
             0.4,
             "#fde0ef"
            ],
            [
             0.5,
             "#f7f7f7"
            ],
            [
             0.6,
             "#e6f5d0"
            ],
            [
             0.7,
             "#b8e186"
            ],
            [
             0.8,
             "#7fbc41"
            ],
            [
             0.9,
             "#4d9221"
            ],
            [
             1,
             "#276419"
            ]
           ],
           "sequential": [
            [
             0,
             "#0d0887"
            ],
            [
             0.1111111111111111,
             "#46039f"
            ],
            [
             0.2222222222222222,
             "#7201a8"
            ],
            [
             0.3333333333333333,
             "#9c179e"
            ],
            [
             0.4444444444444444,
             "#bd3786"
            ],
            [
             0.5555555555555556,
             "#d8576b"
            ],
            [
             0.6666666666666666,
             "#ed7953"
            ],
            [
             0.7777777777777778,
             "#fb9f3a"
            ],
            [
             0.8888888888888888,
             "#fdca26"
            ],
            [
             1,
             "#f0f921"
            ]
           ],
           "sequentialminus": [
            [
             0,
             "#0d0887"
            ],
            [
             0.1111111111111111,
             "#46039f"
            ],
            [
             0.2222222222222222,
             "#7201a8"
            ],
            [
             0.3333333333333333,
             "#9c179e"
            ],
            [
             0.4444444444444444,
             "#bd3786"
            ],
            [
             0.5555555555555556,
             "#d8576b"
            ],
            [
             0.6666666666666666,
             "#ed7953"
            ],
            [
             0.7777777777777778,
             "#fb9f3a"
            ],
            [
             0.8888888888888888,
             "#fdca26"
            ],
            [
             1,
             "#f0f921"
            ]
           ]
          },
          "colorway": [
           "#636efa",
           "#EF553B",
           "#00cc96",
           "#ab63fa",
           "#FFA15A",
           "#19d3f3",
           "#FF6692",
           "#B6E880",
           "#FF97FF",
           "#FECB52"
          ],
          "font": {
           "color": "#2a3f5f"
          },
          "geo": {
           "bgcolor": "white",
           "lakecolor": "white",
           "landcolor": "#E5ECF6",
           "showlakes": true,
           "showland": true,
           "subunitcolor": "white"
          },
          "hoverlabel": {
           "align": "left"
          },
          "hovermode": "closest",
          "mapbox": {
           "style": "light"
          },
          "paper_bgcolor": "white",
          "plot_bgcolor": "#E5ECF6",
          "polar": {
           "angularaxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           },
           "bgcolor": "#E5ECF6",
           "radialaxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           }
          },
          "scene": {
           "xaxis": {
            "backgroundcolor": "#E5ECF6",
            "gridcolor": "white",
            "gridwidth": 2,
            "linecolor": "white",
            "showbackground": true,
            "ticks": "",
            "zerolinecolor": "white"
           },
           "yaxis": {
            "backgroundcolor": "#E5ECF6",
            "gridcolor": "white",
            "gridwidth": 2,
            "linecolor": "white",
            "showbackground": true,
            "ticks": "",
            "zerolinecolor": "white"
           },
           "zaxis": {
            "backgroundcolor": "#E5ECF6",
            "gridcolor": "white",
            "gridwidth": 2,
            "linecolor": "white",
            "showbackground": true,
            "ticks": "",
            "zerolinecolor": "white"
           }
          },
          "shapedefaults": {
           "line": {
            "color": "#2a3f5f"
           }
          },
          "ternary": {
           "aaxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           },
           "baxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           },
           "bgcolor": "#E5ECF6",
           "caxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           }
          },
          "title": {
           "x": 0.05
          },
          "xaxis": {
           "automargin": true,
           "gridcolor": "white",
           "linecolor": "white",
           "ticks": "",
           "title": {
            "standoff": 15
           },
           "zerolinecolor": "white",
           "zerolinewidth": 2
          },
          "yaxis": {
           "automargin": true,
           "gridcolor": "white",
           "linecolor": "white",
           "ticks": "",
           "title": {
            "standoff": 15
           },
           "zerolinecolor": "white",
           "zerolinewidth": 2
          }
         }
        },
        "title": {
         "text": "Kaggle Personality Distribution"
        }
       }
      }
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import plotly.graph_objs as go\n",
    "import plotly.offline as py\n",
    "\n",
    "\n",
    "labels = pred_df['description']\n",
    "sizes = pred_df['percent']*100\n",
    " \n",
    "trace = go.Pie(labels=labels, values=sizes)\n",
    "layout = go.Layout(title='Kaggle Personality Distribution')\n",
    " \n",
    "data = [trace]\n",
    "fig = go.Figure(data=data, layout=layout)\n",
    "py.iplot(fig)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Creating pickle file and testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "pickle.dump(model_lr, open(\"model-lr.pkl\", \"wb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = forum_data_agg['clean_messages']\n",
    "\n",
    "model = pickle.load(open('model-lr.pkl', 'rb'))\n",
    "prediction = model.predict(query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'INTJ'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display(prediction)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Creating mock data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "forum_data_agg['clean_messages'].to_csv('example.csv')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
